% !TEX TS-program = pdflatex
% !TEX encoding = UTF-8 Unicode

\documentclass{scrartcl} % use larger type; default would be 10pt

\usepackage[utf8]{inputenc} % set input encoding (not needed with XeLaTeX)

%%% PAGE DIMENSIONS
\usepackage{geometry} % to change the page dimensions
\geometry{a4paper} % or letterpaper (US) or a5paper or....

\usepackage{graphicx} % support the \includegraphics command and options

 %\usepackage[parfill]{parskip} % Activate to begin paragraphs with an empty line rather than an indent

%%% PACKAGES
\usepackage{booktabs} % for much better looking tables
\usepackage{array} % for better arrays (eg matrices) in maths
\usepackage{paralist} % very flexible & customisable lists (eg. enumerate/itemize, etc.)
\usepackage{verbatim} % adds environment for commenting out blocks of text & for better verbatim
\usepackage{subfig} % make it possible to include more than one captioned figure/table in a single float
\usepackage{color}
\usepackage{ulem}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{tabularx}
\usepackage[numbers]{natbib} % for citeauthor


\title{Outline}
\author{Alex Baier \\ abaier@uni-koblenz.de}


\begin{document}
\maketitle

\section{Introduction}
Motivation. Related work. Solution. Evaluation.

\section{Foundations}

\subsection{Wikidata}
What is Wikidata? Collaborative, people-driven, etc..
Open World Assumption.\\
What is an entity? What are statements? References? Qualifiers?\\
Informal explanation of taxonomy in the use case of Wikidata. \\
Show problems in Wikidata's taxonomy.\\
\citeauthor{Galarraga2016} \cite{Galarraga2016}

\subsection{Taxonomy}
\begin{itemize}
\item Ontology\\
	\citeauthor{Cimiano2009} \cite{Cimiano2009}\\
	\citeauthor{Galarraga2016} \cite{Galarraga2016}
\item Taxonomy\\
	\citeauthor{Cimiano2009} \cite{Cimiano2009}\\
	\citeauthor{Galarraga2016} \cite{Galarraga2016}
\item Unconnected taxonomy as in the case of Wikidata
\item Root class
\item Unlinked class
\item Problem statement\\
	challenge: potentially very high number of target classes
\end{itemize}

\subsection{Similarity}
\begin{itemize}
\item semantic similarity e.g. distributional, feature-based \\
	\citeauthor{Lin1998} \cite{Lin1998} \\
	\citeauthor{Rodriguez2003} \cite{Rodriguez2003}
\item geometrical similarity e.g. distance based, cosine
\end{itemize}

\subsection{Similarity-based classification}
kNN. Maybe SVM, which is not appropriate for the task at hand, because the number of target classes is too high.
\citeauthor{Chen2009} \cite{Chen2009}\\
\citeauthor{Zhang2015} \cite{Zhang2015}

\subsection{Text processing}
\begin{itemize}
\item N-Gram \\
	What is an N-Gram? how can it be used?
	\citeauthor{Jurafsky2014} \cite{Jurafsky2014}
\item Skip-Gram \\
	What is a Skip-Gram? Benefits in comparison to N-Grams.
	\citeauthor{Guthrie2006} \cite{Guthrie2006}
\item Counting-based word representations \\
	Short explanation is sufficient.
	\citeauthor{Levy2015} \cite{Levy2015}
\item Predictive word representations \\
	Shortly mention models by \citeauthor{Mikolov2013}. Maybe mention GloVe too.
	\citeauthor{Levy2015} \cite{Levy2015}
\end{itemize}

\section{Analysis of the Wikidata taxonomy}
How can classes and unlinked classes be recognized in Wikidata?
Analyze whole taxonomy. Identify ''relevant'' unlinked classes. Repeat analysis for these classes.
Summarize important observations.

\section{Ontology learning}
General concepts. Tasks in ontology learning. Classification of considered problem in the task of ontology learning.
Solutions for similar problems. \\
\citeauthor{Cimiano2009} \cite{Cimiano2009}\\
\citeauthor{Wong2012} \cite{Wong2012}\\
\citeauthor{dAmato16} \cite{dAmato16}\\
\citeauthor{Petrucci16} \cite{Petrucci16}\\
\citeauthor{Fu2014} \cite{Fu2014}

\section{Neural networks}
Notion of neural networks will be introduced.\\
Show a schematic for every model.\\
Purpose of specific NN. How can it be used for the task at hand? What are problems with the NN?

\subsection{Recursive neural networks for graph representation}
\citeauthor{Scarselli2009} \cite{Scarselli2009}
\subsection{Deep neural networks for graph representation}
\citeauthor{Cao2016} \cite{Cao2016} \\
\citeauthor{Raghu2016} \cite{Raghu2016}
\subsection{Continuous Bag-of-Words}
\citeauthor{Mikolov2013} \cite{Mikolov2013}
\subsection{Skip-gram with negative sampling}
\citeauthor{Mikolov2013} \cite{Mikolov2013}\\
\citeauthor{Levy2015} \cite{Levy2015}\\
\citeauthor{Goldberg14} \cite{Goldberg14}

\subsection{Comparison}
Make a decision, which NN will be used for solving the task. Skip-gram with negative sampling.

\section{Algorithm}
Do I need to explain the actual implementation in detail? It is really not that interesting.

\subsection{Challenges}
Identify and summarize the challenges of the task.\\
High number of target classes. Big amount of possibly usable data, what is relevant, what is not?
\subsection{Baseline}
\begin{itemize}
\item Architecture\\
	Using word embeddings of classes in kNN\\
	How does this solve the problem definition?
	How are the challenges addressed?
\item Hyper parameters\\
	configuration of the model, explain decision\\
	\citeauthor{Levy2015} \cite{Levy2015}
\item Training data\\
	mapping relevant Wikidata entities to a textual representation
\end{itemize}

\subsection{Variation 1}
supplement training data with Wikipedia, what are possible advantages? More data, Wikipedia contains ''soft'' 
information? which cannot be represented in Wikidata. what are disadvantages? Wikipedia contains ''noise'' unlike 
Wikidata, training will take much longer. How to combine embeddings of Wikidata and Wikipedia? just appending,
other options?

\section{Evaluation}

\subsection{Method}
\citeauthor{Dellschaft2006} \cite{Dellschaft2006}

\subsection{Generation of gold standard}
pick number (tbd) of random linked classes. the distribution of instances, subclasses, properties per class should
similar to the repeated analysis of the observed unlinked classes. remove the subclass properties of the chosen classes.
also generate a new set of training data, which reflects the changes, and train the model on this modified data.

\subsection{Results}
compare baseline algorithm and variation(s). do the results match with my expectations? if not why could
this be? mention training and execution time of the algorithms. 

\section{Conclusion}
what was learned? can the developed solution be used in practical application? what about future work?

\bibliographystyle{plainnat}
\bibliography{/home/alex/PycharmProjects/thesis/thesis/bibliography}


\end{document}

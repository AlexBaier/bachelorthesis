% !TEX TS-program = pdflatex
% !TEX encoding = UTF-8 Unicode

\documentclass{scrartcl} % use larger type; default would be 10pt

\usepackage[utf8]{inputenc} % set input encoding (not needed with XeLaTeX)

%%% PAGE DIMENSIONS
\usepackage{geometry} % to change the page dimensions
\geometry{a4paper} % or letterpaper (US) or a5paper or....

\usepackage{graphicx} % support the \includegraphics command and options

 %\usepackage[parfill]{parskip} % Activate to begin paragraphs with an empty line rather than an indent

%%% PACKAGES
\usepackage{booktabs} % for much better looking tables
\usepackage{array} % for better arrays (eg matrices) in maths
\usepackage{paralist} % very flexible & customisable lists (eg. enumerate/itemize, etc.)
\usepackage{verbatim} % adds environment for commenting out blocks of text & for better verbatim
\usepackage{subfig} % make it possible to include more than one captioned figure/table in a single float
\usepackage{color}
\usepackage{ulem}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{tabularx}
\usepackage[numbers]{natbib} % for citeauthor


\title{Outline}
\author{Alex Baier \\ abaier@uni-koblenz.de}


\begin{document}
\maketitle

\section{Introduction}
\begin{itemize}
\item Motivation
\item Related work
\item Novelty/Challenges
\begin{itemize}
\item application of neural word embeddings in OL
\item large number of classes
\end{itemize}
\item Solution
\item Results
\item Overview
\end{itemize}

\section{Foundations}

\subsection{Wikidata}
\begin{itemize}
\item What is Wikidata?
\item Items
\item Statements
\item Difference between class and instance
\end{itemize}

\subsection{Taxonomy}
\begin{itemize}
\item Notion of ontology and taxonomy
\item Definition of taxonomy
\item Unlinked vs. root classes
\end{itemize}

\subsection{Similarity measures}
\begin{itemize}
\item Information-theoretical notion of similarity (commonalities, differences)
\item Vector similarity: distance-based, cosine
\item Semantic similarity
\end{itemize}

\subsection{Problem statement}
\begin{itemize}
\item Definition of problem
\item Challenges
\end{itemize}

\subsection{k-nearest-neighbors classification}
\begin{itemize}
\item What is kNN?
\item How is it suited to the problem?
\item Formal description for weighted single-label kNN
\item Multi-label kNN
\item How can it be applied to the problem? Make Wikidata items comparable via similarity measure.
\end{itemize}

\section{Neural networks}
Answer question, how neural networks are suited to represent Wikidata items as vectors.
This will allow it to apply similarity measures and therefore solve the problem.

\subsection{Feedforward neural network with backpropagation using gradient descent}
\begin{itemize}
\item Introduce neural network on simple model
\item Use simple classification or regression problem as example
\end{itemize}

\subsection{Deep neural networks for graph representation}

\subsection{Word2Vec}
\begin{itemize}
\item N-grams, Skip-grams
\item CBOW
\item Skip-gram
\item about hyperparameters
\end{itemize}

\subsection{RDF2Vec}

\subsection{Comparison}

\section{Ontology learning}
What is ontology learning and why is it interesting?

\subsection{Process and architecture for ontology learning}

\subsection{Approaches for learning taxonomic relations}

\subsection{Ontology learning using neural networks}

\section{Analysis of the Wikidata taxonomy}

\subsection{Root taxonomy}

\subsection{Unlinked classes}

\subsection{Labeled, instantiated, unlinked classes}
\textbf{Find an acronym for this set}
\begin{itemize}
\item Reasons for reducing the input set.
\item Compare results to unlinked classes
\end{itemize}

\section{Combined algorithm}

\subsection{Baseline}
Skip-gram using Wikidata statement triples as input sentences + ml-kNN.

\begin{itemize}
\item Model of algorithm
\item How are the challenges solved?
\item Choice of hyperparameters
\end{itemize}

\subsection{Variation: RDF2Vec}
Skip-gram using graph walk to create input sentences from Wikidata + ml-kNN.
\begin{itemize}
\item What is added/changed?
\item Model of algorithm
\item Possible benefits/deficits
\end{itemize}

\subsection{Variation: Wikipedia}
Skip-gram using Wikidata statement triples as input sentences.
Skip-gram using Wikipedia as input.
Combine word embeddings of both NNs to one embedding and apply ml-kNN.
\begin{itemize}
\item What is added/changed?
\item Model of algorithm
\item Possible benefits/deficits
\end{itemize}

\section{Evaluation}

\subsection{Method}

\subsection{Generation of gold standard}
pick number (tbd) of random linked classes. the distribution of instances, subclasses, properties per class should
similar to the repeated analysis of the observed unlinked classes. remove the subclass properties of the chosen classes.
also generate a new set of training data, which reflects the changes, and train the model on this modified data.

\subsection{Results}
compare baseline algorithm and variation(s). do the results match with my expectations? if not why could
this be? mention training and execution time of the algorithms. 

\section{Conclusion}
what was learned? can the developed solution be used in practical application? what about future work?

\end{document}

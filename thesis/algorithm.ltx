As motivated in the previous chapters, a hybrid algorithm using neural word embeddings is implemented to solve the defined problem.
In this chapter, the required components for the algorithm are presented.
For each component, possible implementations are proposed and implemented.
Additionally, only relevant classes, as described in Section~\ref{section:relevant classes}, are considered in this chapter.
Therefore training of the algorithm uses only of relevant classes and their corresponding instances.

\subsection{Components}
The hybrid algorithm exploits neural word embeddings to solve the classification task,
which is motivated by the power of word embeddings in similarity tasks \cite{Mikolov2013} and in taxonomy construction \cite{Fu2014}.

For the task of computing word embeddings, the SGNS model (Section~\ref{section:word2vec}) will be used, which has shown to generate effective word embeddings relatively fast 
\cite{Mikolov2013} \cite{Levy2015}.
It has been proven through theoretic analysis as well as experiments that SGNS creates very effective
word embeddings, if its hyperparameters are chosen well \cite{Mikolov2013} \cite{Levy2015}.

Based on \fullcite{Levy2015}, \fullcite{Mikolov2013} and \fullcite{Mikolov2013a} 
the following hyperparameters are used:
\begin{itemize}
\item Embedding size: $300$
\item Subsampling frequency: $10^{-5}$
\item Context window: $2$
\item Negative samples: $15$
\end{itemize}
The original paper by \fullcite{Mikolov2013} uses context windows of size $2$.
Bigger context windows show slight improvements in the quality of word embeddings \cite{Levy2015}.
However triple sentences (see Section~\ref{section:triple sentences}) only have a maximum context of $2$, e.g. $w_1 \: w_2 \: w_3$ the maximum distance
between words in the same sentence is $2$. Therefore triple sentences would not benefit from bigger
context sizes. To preserve comparability between variations of the algorithm, the same context size is also used
for all types of SequenceGen components.
The implementation of SGNS in the gensim library by \fullcite{Rehurek2010} is used.

Two additional components can be identified for the hybrid classification algorithm using SGNS.
The data flow and IO of the components is visualized in Figure~\ref{fig:algorithm data flow}.

\begin{figure}
\centering
\includegraphics[width=\textwidth]{images/algorithm/hybrid_algorithm_components.pdf}
\caption{Data flow between components}
\label{fig:algorithm data flow}
\end{figure}

\begin{enumerate}
\item \textbf{SequenceGen}. 
SGNS requires a linear sequence of words as input (see Section~\ref{section:word2vec}).
The SequenceGen component transforms any possible kind of data into a set of sentences, which can be used for training the SGNS.
Possible data sets include Wikidata and Wikipedia.
Wikidata's highly interlinked structure is more alike to an RDF graph \cite{Erxleben2014}.
Using Wikidata therefore requires an approach, which maps the graph to linear sequences.
If the encyclopedic text provided by Wikipedia is used, a different challenge has to be solved.
Since the gold standard consists of pairs of subclass-superclass pairs, which were acquired from Wikidata \cite{WikidataDump},
a mapping from words to Wikidata IDs would have to be implemented.

\item \textbf{Classification}.
The Classification component is trained on a set of gold standard subclass-superclass pairs.
It uses the word embeddings generated by SGNS to make a classification decision.
The word embeddings produced by SGNS group similar words close to each other and preserve linguistic regularities.
These properties can be exploited by different classifiers. 
kNN can exploit the first characteristic using a similarity or distance measure.
The second characteristic can be exploited by computing the vector offset or learning a linear projection representing the subclass-of relationship between two classes \cite{Fu2014}.
The classification component should implement a single-label multiclass classification.
\end{enumerate}

\subsection{SequenceGen}\label{section:sequencegen}

\subsubsection{Triple sentences}\label{section:triple sentences}
Triple sentences uses Wikidata as input data set.
Statements in Wikidata can be represented as triples of $(\mathit{source}, \mathit{property}, \mathit{target})$,
where $\mathit{source}$ is a Wikidata item ID, $\mathit{property}$ is a Wikidata property ID, and $\mathit{target}$ is either a Wikidata item ID or a literal.
These triples represents simple 3-word sentences, which are sufficient as training input for Word2Vec.
Appereances of literals are removed from the generated sentences to reduce the amount of noise in the data.
In comparison to natural text, the triple sentence contain a very low amount of noise, which may improve the quality of embeddings in comparison to using for example Wikipedia as data set.

Figure~\ref{fig:triple sentences} shows an example mapping of Wikidata to triple sentences.
Edges between entities, e.g. $(Q42, P31, Q5)$, are expressed as triple sentences, e.g. ''Q$42$ P$31$ Q$5$'',
while edges between an entity and a literal, e.g. $(Q42, P1559, \textit{''Douglas Adams''})$, ignore the literal
and are therefore double sentences, e.g. ''Q$42$ P$1559$''.
\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{images/algorithm/triple_sentences.pdf}
\caption{Generation of triple sentences from Wikidata}
\label{fig:triple sentences}
\end{figure}

\subsubsection{Graph walk sentences}
A graph walk, developed by \fullcite{Ristoski2016} and explained in Section~\ref{section:graph embeddings},
can be used for generating longer sentences instead of triple sentences. 
The longer sequences are able to better capture contextual information than triples.
This is achieved by creating walks of a depth $d$, starting from each class.
Evaluation of RDF2Vec shows that  Skip-gram and CBOW both produce better word embeddings using graph walk \cite{Ristoski2016}.
Therefore it is expected that graph walk will also improve the performance of the hybrid algorithm.
Computing all walks for each vertex is however not feasible, since the number of walks for each vertex is potentially exponential \cite{Ristoski2016}.
The exponential runtime can be counteracted by computing a limited number of random walks instead of computing all walks \cite{Perozzi2014}.
Additionally, the starting vertices for each walk will be sampled from the set of classes rather than the set of all items.
The quality of word embeddings in regards to classes is a major concern, while the word embeddings for other items are only optional for the task
of classification. The proposed measure will ensure that each class occurs at least once in the graph walk sequences and
the frequency of classes in the generated set will be higher than by random sampling from all items.

The set of random walk sequences will be combined with the set of triple sentences, which are used in the baseline, to constitute a new training data set for the SGNS.
The following parameters are used for the graph walk :
\begin{itemize}
\item Depth of walks: $4$
\item Maximum number of walks per vertice: $10$
\end{itemize}
\fullcite{Ristoski2016} applies $200$ maximum walks per vertice for the Wikidata dataset. 
Because of time and hardware constraints only $10$ walks per vertice are computed,
which should allow a "good" coverage of classes in the generated sequences.

Figure~\ref{fig:graph walk sentences} shows an excerpt of the Wikidata graph. $2$ random walks of depth $4$ are generated beginning at the node \textit{Q42}.
The walks by the Arabic and Roman numerals. The walk indicated by Arabic numerals has a length of $3$ and ends at the node \textit{Q223557}.
If no further successors can be found from a given node, the walk ends prematurely.
The walks indicated by roman numerals has a length of $4$ and ends at node \textit{Q215627}.
The Wikidata graph contains cycles, as is shown between nodes \textit{Q5} and \textit{Q8205328}. The random walks can therefore also display cycles
and therefore visit nodes multiple times.
The output of graph walk sentences with $2$ maximum walks and depth $4$ with source node \textit{Q42} (based on Figure~\ref{fig:graph walk sentences} is the following:
\begin{itemize}
\item "Q42 P31 Q5 P1542 Q8205328 P279 Q223557" ($1, 2, 3, 4$)
\item ""Q42 P31 Q5 P1542 Q8205328 P170 Q5 P279 Q215627" (\textit{\rom{1}, \rom{2}, \rom{3}, \rom{4}, \rom{5}})
\end{itemize}

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{images/algorithm/graph_walk_sentences.pdf}
\caption{Two random graph walks in Wikidata with depth $4$.}
\label{fig:graph walk sentences}
\end{figure}

\subsection{Classification}

\subsubsection{k-nearest-neighbors}
Similar classes are grouped close to each other in the embedding vector space.
Intuitively a kNN classifier (see Section \ref{section:knn}) should be able to use this property to great effect.

Distance-based kNN using the euclidean norm is implemented as classification component.
The kNN implementation provided by the scikit-learn library \cite{scikit-learn} is used.

\subsubsection{Vector offset}\label{section:vector offset}

\begin{figure}
\begin{mdframed}
\centering
\includegraphics[width=\textwidth]{images/algorithm/2d_subclass_offsets.png}
\caption{Subclass-of offsets of $240,789$ subclass-superclass pairs.}
\medskip
$240,789$ subclass-superclass pairs were retrieved from the set of relevant classes (Section~\ref{section:relevant classes}).
For each pair, the offset $\vec{\textit{superclass}} - \vec{\textit{subclass}} \in \mathbb{R}^{300}$ was computed 
using triple sentence word embeddings (Section~\ref{section:triple sentences}).
The offsets were clustered into $15$ clusters using KMeans clustering.
PCA was applied on the clustered offsets for plotting.
The legend indicates the color of each cluster $k \in [0, 14]$.
\label{fig:2d offsets}
\end{mdframed}
\end{figure}

A possible approach for classification is to exploit the linear regularities encoded in the word embeddings.
As shown in Section~\ref{section:word2vec}, it is possible to answer semantic questions by applying algebraic operations on word embeddings \cite{Mikolov2013}.
Given a subclass-superclass pair with word embeddings $\vec{x}, \vec{y} \in \mathbb{R}^d$ and the word embedding $\vec{u} \in \mathbb{R}^d$ of an unknown class,
the superclass with word embedding $\vec{p} \in \mathbb{R}^d$ of $\vec{u}$ could be calculated, as follows 
\begin{align*}
\vec{p} = \vec{y} - \vec{x} + \vec{u}
\end{align*}
where $d \in \mathbb{N}$ is the embedding size.
The subclass-of relationship is represented by the offset $\vec{y} - \vec{x}$. 
Using the subclass offsets would simplify the difficult task of classifying an unknown class in a big taxonomy
to a vector addition, which represents a translation.
This is however only possible, if the subclass-of offset is similar for all existing subclass-superclass pairs \cite{Fu2014}.

For the use case of Wikidata, it can be shown that the offsets do not fulfill this criteria. 
It can rather be seen that different clusters of similar subclass-of offsets exist. 
Using the word embeddings created by SGNS trained on triple sentences,
the subclass-of offsets for $240789$ subclass-superclass pairs were computed.
The subclass-superclass pairs were extracted from the set of relevant classes (Section~\ref{section:relevant classes}).
Using Principal Component Analysis (PCA), the 300-dimensional vectors were reduced to  2-dimensional vectors,
which are shown in Figure~\ref{fig:2d offsets}.

The subclass offsets with dimension $d=300$ were clustered into $K=15$ clusters using KMeans.
The clusters $1$, $7$ and $13$, which attribute to $71.5\%$ of all offsets contain a variation subclass-of relation types,
which is most likely the case because $15$ clusters are not sufficient to separate all different types of subclass-of relations.
Therefore algorithms like piecewise linear projection \cite{Fu2014}, which exploit the similarity of specific types of subclass-of relations,
need to use more clusters $K$ . 
The other clusters show very specific types of offsets.
Table~\ref{table:offset cluster} shows the list of $15$ clusters with their percentage of all offsets, their topic and a subclass-superclass pair in the cluster.

Clusters, which describe similar topics are close together, e.g. clusters $2$ and $10$ are related to proteins, clusters $6$, $9$ and $11$ are related to vehicles.
An error in filtering for relevant classes occurred (Section~\ref{section:relevant classes}), since two clusters in the set are related to proteins, which is $11.9\%$
of all relevant subclass-superclass relations.
Because the percentage of protein-related classes is relatively low, further filtering of the relevant dataset is not pursued.
It can be seen, that the clusters $2$, $4$ and $10$ have very different orientations in respect to the origin $(0,0)$ than the other clusters.
Therefore directly using vector offsets would not be applicable, as offsets can not represent the complex subclass-of relation.
\fullcite{Fu2014} reached a similar conclusion and proposes the use of linear projections as means to represent the subclass-of relation.
Additionally, it can be concluded that $15$ clusters is too low for usage in piecewise linear projection, since $3$ clusters, representing $71.5\%$ of all offsets, exist,
which do not have definite topics. Therefore cluster sizes $k$ of at least $30$ should be considered as parameter for piecewise linear projection.
In the following section, the approach by \citeauthor{Fu2014}, which is described in Section~\ref{section:ol nn},
is adjusted for usage in the hybrid algorithm.

\begin{table}
\begin{tabularx}{\textwidth}{l | l | l | X}
$k$ & perc. & topic & example \\
\hline
$0$ & $1\%$    & wine (mostly Italian wines)                         & \textit{Greek wine} $\subclassof{}$ \textit{wine}, \textit{Alghero frizzante rosato} $\subclassof{}$ \textit{wine} \\
$2$ & $8.3\%$ & protein families                                           & \textit{G kinase-anchoring protein 1} $\subclassof{}$ \textit{protein family} \\
$3$ & $3.3\%$ & Spanish mayors (Alcalde)                           & \textit{mayor of Granollers} $\subclassof{}$ \textit{Alcalde} \\
$4$ & $6.5\%$ & badminton championships                            & \textit{Jamaican International Badminton Championships} $\subclassof{}$ \textit{badminton championship} \\
$5$ & $0.6\%$ & food                                                            & \textit{Sour cream doughnut} $\subclassof{}$ \textit{food} \\
$6$ & $1.6\%$ & locomotives					& \textit{GWR 103 President} $\subclassof{}$ \textit{tender locomotive}, \textit{GE C31-8} $\subclassof{}$ \textit{diesel-electric locomotive} \\
$8$ & $0.8\%$ & tennis tournaments 				& \textit{ECC Antwerp} $\subclassof{}$ \textit{tennis tournament} \\
$9$ & $1.5\%$ & aircrafts						& \textit{Martin KF-1} $\subclassof{}$ \textit{aircraft} \\
$10$ & $3.6\%$ & protein domains                                        & \textit{ISX02-like transposase domain} $\subclassof{}$ \textit{protein domain} \\
$11$ & $0.4\%$ & military aircrafts                                        & \textit{Heinkel He 50} $\subclassof{}$ \textit{military aircraft} \\
$12$ & $1\%$  & diseases                                                      & \textit{cervical squamotransitional carcinoma} $\subclassof{}$ \textit{disease} \\
$14$ & $0.5\%$ & ambassadors                                            & \textit{Ambassador of the Republic of China to the United States} $\subclassof{}$ \textit{ambassador}
\end{tabularx}
\caption{Analysis of $15$ subclass-of clusters, which are plotted in Figure~\ref{fig:2d offsets}}
\label{table:offset cluster}
\end{table}

\subsubsection{Linear projection}\label{section:linear projection}
Following the idea of using vector offsets to find the correct superclass $\vec{p} \in \mathbb{R}^d$ 
for a given unknown $\vec{u} \in \mathbb{R}^d$,
the translation represented by the subclass offset can be extended by adding a linear projection matrix $\Phi \in \mathbb{R}^{d \times d}$.
This follows the idea of \fullcite{Fu2014}, who uses linear projection without translation
to find the superclasses for a given unknown.
The superclass $\vec{p}$ for an unknown $\vec{u}$ is accordingly computed:
\begin{align}
&\vec{p} = \Phi \vec{u} + \vec{v} = \begin{bmatrix}\Phi & \vec{v}\end{bmatrix} \begin{bmatrix}\vec{u} \\ 1\end{bmatrix}
\end{align}
where $\vec{v} \in \mathbb{R}^d$ is a translation vector.
Combining the translation and projection into a single matrix $T = \begin{bmatrix}\Phi & \vec{v}\end{bmatrix}$
enables the use of \fullcite{Fu2014}'s approach for finding $T^* \in \mathbb{R}^{d \times d}$,
which is the best fit for the given subclass-superclass pairs.
$T^*$ can be computed by minimizing the following mean squared error \cite{Fu2014}:
\begin{align}
&T^* = \argmin_T \frac{1}{N} \sum_{(\vec{x}, \vec{y})} \norm{T \vec{x} - \vec{y}}^2 
\end{align}
where $N$ is the number of $(\vec{x}, \vec{y})$ subclass-superclass pairs in the training data \cite{Fu2014}.

\fullcite{Fu2014} proposes an improvement to the linear projection.
As shown in Figure~\ref{fig:2d offsets}, the subclass offsets appear in clusters,
therefore it is intuitive to assume that training a matrix $T_k \in \mathbb{R}^{d \times d}$ for different clusters $k$
increases the accuracy of the method \cite{Fu2014}.
However, the approach by \fullcite{Fu2014} is not applicable for the given problem.
In \citeauthor{Fu2014}'s stated problem, a an unknown class can have zero, one, or multiple superclasses, but in the thesis' problem an unknown class is always assigned
one superclass.
Additionally, the input for the thesis' classification is a single unknown class, therefore it cannot be decided in which subclass-of offset cluster the unknown class belongs.
Instead of using subclass-of clusters to train the piecewise projections, the subclass-superclass pairs $(\vec{x}, \vec{y})$ are clustered by their input vector $\vec{x}$.
This leads to the following training objective for piecewise linear projections \cite{Fu2014}:
\begin{align}
&T_k^* = \argmin_{T_k} \frac{1}{N_k} \sum_{(\vec{x}, \vec{y})\in C_k} \norm{T_k \vec{x} - \vec{y}}
\end{align}
where $N_k \in \mathbb{N}$ is the amount of word pairs in the $k^\mathit{th}$ cluster $C_k = \{ (\vec{x_i}, \vec{y_i}) |  \vec{x_i} \textit{ in cluster } k, i \in [1, N] \}$ \cite{Fu2014}.
Both training objectives describe multivariate linear regression tasks.
\fullcite{Fu2014} uses stochastic gradient descent (SGD) to solve the objective.
SciPy's SGD regressor \cite{Jones2001} is used to train the linear projection matrices.

The projection for a given unknown $\vec{u}$ using piecewise linear projection can be computed as follows:
\begin{align}
&\vec{p} = T_k^* \begin{bmatrix}\vec{u} \\ 1 \end{bmatrix}
\end{align}
where $\vec{u}$ in cluster $k$.
The appropiate superclass is subsequently the closest class embedding to the computed projection.

The thesis' evaluation will show whether linear projection is superior to the distance-based kNN in the given task,
and to what degree it benefits from word embeddings using graph walk sentences instead of triple sentences.

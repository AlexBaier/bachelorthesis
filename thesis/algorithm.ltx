As motivated in the previous chapters, a hybrid algorithm using neural word embeddings is implemented to solve the defined problem.
In this chapter, the required components for the algorithm are presented.
For each component, possible implementations are proposed and implemented.
Additionally, only relevant classes, as described in Section~\ref{section:relevant classes}, are considered in this chapter.
Therefore training of the algorithm uses only of relevant classes and their corresponding instances.

\subsection{Components}\label{section:components}
The hybrid algorithm exploits neural word embeddings to solve the classification task,
which is motivated by the power of word embeddings in similarity tasks \cite{Mikolov2013} and in taxonomy construction \cite{Fu2014}.

For the task of computing word embeddings, the SGNS model (Section~\ref{section:word2vec}) will be used, which has shown to generate effective word embeddings relatively fast 
\cite{Mikolov2013} \cite{Levy2015}.
It has been proven through theoretic analysis as well as experiments that SGNS creates very effective
word embeddings, if its hyperparameters are chosen well \cite{Mikolov2013} \cite{Levy2015}.

Based on \fullcite{Levy2015}, \fullcite{Mikolov2013} \cite{Mikolov2013a}  and \fullcite{Baroni2014}
the following hyperparameters are used:
\begin{itemize}
\item Embedding size: $300$
\item Subsampling frequency: $10^{-5}$
\item Context window: $2$
\item Negative samples: $15$
\end{itemize}
The original paper by \fullcite{Mikolov2013} uses context windows of size $2$.
Bigger context windows show slight improvements in the quality of word embeddings \cite{Levy2015}.
However triple sentences (see Section~\ref{section:triple sentences}) only have a maximum context of $2$, e.g. $w_1 \: w_2 \: w_3$ the maximum distance
between words in the same sentence is $2$. Therefore triple sentences would not benefit from bigger
context sizes. To preserve comparability between variations of the algorithm, the same context size is also used
for all types of SequenceGen components.
The implementation of SGNS in the gensim library by \fullcite{Rehurek2010} is used.

Two additional components can be identified for the hybrid classification algorithm using SGNS.
The data flow and IO of the components is visualized in Figure~\ref{fig:algorithm data flow}.

\begin{figure}
\centering
\includegraphics[width=\textwidth]{images/algorithm/hybrid_algorithm_components.pdf}
\caption{Data flow between components}
\label{fig:algorithm data flow}
\end{figure}

\begin{enumerate}
\item \textbf{SequenceGen}. 
SGNS requires a linear sequence of words as input (see Section~\ref{section:word2vec}).
The SequenceGen component transforms any possible kind of data into a set of sentences, which can be used for training the SGNS.
Possible data sets include Wikidata and Wikipedia.
Wikidata's highly interlinked structure is more alike to an RDF graph \cite{Erxleben2014}.
Using Wikidata therefore requires an approach, which maps the graph to linear sequences.
If the encyclopedic text provided by Wikipedia is used, a different challenge has to be solved.
Since the gold standard consists of pairs of subclass-superclass pairs, which were acquired from Wikidata \cite{WikidataDump},
a mapping from words to Wikidata IDs would have to be implemented.

\item \textbf{Classification}.
The Classification component is trained on a set of gold standard subclass-superclass pairs.
It uses the word embeddings generated by SGNS to make a classification decision.
The word embeddings produced by SGNS group similar words close to each other and preserve linguistic regularities.
These properties can be exploited by different classifiers. 
kNN can exploit the first characteristic using a similarity or distance measure.
The second characteristic can be exploited by computing the vector offset or learning a linear projection representing the subclass-of relationship between two classes \cite{Fu2014}.
The classification component should implement a single-label multiclass classification.
\end{enumerate}

\subsection{SequenceGen}\label{section:sequencegen}
SequenceGen is the first interchangeable component in the proposed hybrid algorithm.
SGNS is trained on a set of word sequences (or sentences).
Purpose of the SequenceGen component is the generation of sentences, where the words represent
Wikidata IDs. In this work, the input for SequenceGen is a Wikidata JSON dump \cite{WikidataDump},
as it directly uses Wikidata IDs and is structured data, therefore easy to automatically process.
It is possible to use other knowledge bases as input, if a mapping to Wikidata can be found.
For example, an incomplete mapping between Wikidata and DBpedia exists in DBpedia
via the \textit{owl:equivalentProperty} and \textit{owl:equivalentClass} properties \textbf{TODO: reference}.

\subsubsection{Triple sentences}\label{section:triple sentences}
Triple sentences uses Wikidata as input data set.
Statements in Wikidata can be represented as triples of $(\mathit{source}, \mathit{property}, \mathit{target})$,
where $\mathit{source}$ is a Wikidata item ID, $\mathit{property}$ is a Wikidata property ID, and $\mathit{target}$ is either a Wikidata item ID or a literal.
These triples represents simple 3-word sentences, which are sufficient as training input for Word2Vec.
Appereances of literals are removed from the generated sentences to reduce the amount of noise in the data.
In comparison to natural text, the triple sentence contain a very low amount of noise, which may improve the quality of embeddings in comparison to using for example Wikipedia as data set.

Figure~\ref{fig:triple sentences} shows an example mapping of Wikidata to triple sentences.
Edges between entities, e.g. $(Q42, P31, Q5)$, are expressed as triple sentences, e.g. ''Q$42$ P$31$ Q$5$'',
while edges between an entity and a literal, e.g. $(Q42, P1559, \textit{''Douglas Adams''})$, ignore the literal
and are therefore double sentences, e.g. ''Q$42$ P$1559$''.
\begin{figure}[H]
\centering
\includegraphics[width=0.6\textwidth]{images/algorithm/triple_sentences.pdf}
\caption{Generation of triple sentences from Wikidata}
\label{fig:triple sentences}
\end{figure}

\subsubsection{Graph walk sentences}
A graph walk, developed by \fullcite{Ristoski2016} and explained in Section~\ref{section:graph embeddings},
can be used for generating longer sentences instead of triple sentences. 
The longer sequences are able to better capture contextual information than triples.
This is achieved by creating walks of a depth $d$, starting from each class.
Evaluation of RDF2Vec shows that  Skip-gram and CBOW both produce better word embeddings using graph walk \cite{Ristoski2016}.
Therefore it is expected that graph walk will also improve the performance of the hybrid algorithm.
Computing all walks for each vertex is however not feasible, since the number of walks for each vertex is potentially exponential \cite{Ristoski2016}.
The exponential runtime can be counteracted by computing a limited number of random walks instead of computing all walks \cite{Perozzi2014}.
Additionally, the starting vertices for each walk will be sampled from the set of classes rather than the set of all items.
The quality of word embeddings in regards to classes is a major concern, while the word embeddings for other items are only optional for the task
of classification. The proposed measure will ensure that each class occurs at least once in the graph walk sequences and
the frequency of classes in the generated set will be higher than by random sampling from all items.

The set of random walk sequences will be combined with the set of triple sentences, which are used in the baseline, to constitute a new training data set for the SGNS.
The following parameters are used for the graph walk :
\begin{itemize}
\item Depth of walks: $4$
\item Maximum number of walks per vertice: $200$
\end{itemize}
These parameters are given by \fullcite{Ristoski2016} and have shown good results in their work.

Figure~\ref{fig:graph walk sentences} shows an excerpt of the Wikidata graph. $2$ random walks of depth $4$ are generated beginning at the node \textit{Q42}.
The walks by the Arabic and Roman numerals. The walk indicated by Arabic numerals has a length of $3$ and ends at the node \textit{Q223557}.
If no further successors can be found from a given node, the walk ends prematurely.
The walks indicated by roman numerals has a length of $4$ and ends at node \textit{Q215627}.
The Wikidata graph contains cycles, as is shown between nodes \textit{Q5} and \textit{Q8205328}. The random walks can therefore also display cycles
and therefore visit nodes multiple times.
The output of graph walk sentences with $2$ maximum walks and depth $4$ with source node \textit{Q42} (based on Figure~\ref{fig:graph walk sentences} is the following:
\begin{itemize}
\item "Q42 P31 Q5 P1542 Q8205328 P279 Q223557" ($1, 2, 3, 4$)
\item ""Q42 P31 Q5 P1542 Q8205328 P170 Q5 P279 Q215627" (\textit{\rom{1}, \rom{2}, \rom{3}, \rom{4}, \rom{5}})
\end{itemize}

\begin{figure}[H]
\centering
\includegraphics[width=0.6\textwidth]{images/algorithm/graph_walk_sentences.pdf}
\caption{Two random graph walks in Wikidata with depth $4$.}
\label{fig:graph walk sentences}
\end{figure}

Multiple test runs with graph walk-based classification have shown that embeddings trained on
graph walks produced worse results than the trivial baseline algorithm described in 
Section~\ref{section:evaluation method}.
This stands in contradiction to intuition and experiments by \fullcite{Ristoski2016}.
The following is a list of possible issues, which may be responsible for the unexpected bad results:
\begin{itemize}
\item Erroneous implementation of random graph walk. The most likely reason at this time is errors in
the implementation of the random graph walk, which lead to the generation of ''bad'', duplicate or otherwise
suboptimal walks.
\item Common words are dis-proportionally overrepresented. Random graph walks are more likely to
capture common words like properties such as \textit{instance of (P31)}. Classes are relatively rare words,
and will therefore occur even less often in the graph walk sentences than in triple sentences,
because they are captured less often by the graph walks.
Consequently the generated embeddings for classes are less expressive and contain less semantic information.
\end{itemize}
Due to time constraints, the graph walk sentence component is dropped in the following evaluation.

\subsection{Classification}
The purpose of the Classification component is the prediction of a superclass for a given
orphan class, as defined in the problem statement (see Section~\ref{section:problem statement}).
The classification component is trained of subclass-superclass pairs, which consist of their IDs and their respective
word embeddings generated by the SGNS component.
Generally, a classifier assigns an unknown object a label from a given finite list of labels.
The unknown object is the word embedding of the orphan class and the output is
the Wikidata ID of a superclass.
The kNN algorithm, which is described in Section~\ref{section:knn}, is an example for
a classification algorithm.
Instead of predicting the label directly, another approach would be predicting the word embedding of
a superclass. Then the label of the closest existing superclass embedding is chosen as output
of the classifier.
Predicting continuous values instead of discrete values is accomplished by regression methods.
Two of these indirect regression approaches, linear projection (see Section~\ref{section:linear projection})
and non-linear regression via neural networks (see Section~\ref{section:nonlinear regression}), are
described in this section.

\subsubsection{k-nearest-neighbors}
Similar classes are grouped close to each other in the embedding vector space.
Intuitively a kNN classifier (see Section \ref{section:knn}) should be able to use this property to great effect.

Distance-based kNN using the euclidean norm is implemented as classification component.
The kNN implementation provided by the scikit-learn library \cite{scikit-learn} is used.

\subsubsection{Vector offset}\label{section:vector offset}

A possible approach for classification is to exploit the linear regularities encoded in the word embeddings.
As shown in Section~\ref{section:word2vec}, it is possible to answer semantic questions by applying algebraic operations on word embeddings \cite{Mikolov2013}.
Given a subclass-superclass pair with word embeddings $\vec{x}, \vec{y} \in \mathbb{R}^d$ and the word embedding $\vec{u} \in \mathbb{R}^d$ of an unknown class,
the superclass with word embedding $\vec{p} \in \mathbb{R}^d$ of $\vec{u}$ could be calculated, as follows 
\begin{align*}
\vec{p} = \vec{y} - \vec{x} + \vec{u}
\end{align*}
where $d \in \mathbb{N}$ is the embedding size.
The subclass-of relationship is represented by the offset $\vec{y} - \vec{x}$. 
Using the subclass offsets would simplify the difficult task of classifying an unknown class in a big taxonomy
to a vector addition, which represents a translation.
This is however only possible, if the subclass-of offset is similar for all existing subclass-superclass pairs \cite{Fu2014}.

For the use case of Wikidata, it can be shown that the offsets do not fulfill this criteria. 
It can rather be seen that different clusters of similar subclass-of offsets exist. 
Using the word embeddings created by SGNS trained on triple sentences,
the subclass-of offsets for $204,158$ subclass-superclass pairs were computed.
The subclass-superclass pairs were extracted from the set of relevant classes (Section~\ref{section:relevant classes}).
The offsets were clustered using KMeans with $K=15$ clusters.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{images/algorithm/2d_subclass_offsets.png}
\caption{Subclass-of offsets of $240,789$ subclass-superclass pairs. Each cluster is identified by a specific color.
The offsets are clustered into $15$ clusters using KMeans clustering.}

\label{fig:2d offsets}
\end{figure}

Using Principal Component Analysis (PCA), the 300-dimensional vectors were reduced to  2-dimensional vectors,
which are shown in Figure~\ref{fig:2d offsets}.
$204,158$ subclass-superclass pairs were retrieved from the set of relevant classes (Section~\ref{section:relevant classes}).
For each pair, the offset $\vec{\textit{superclass}} - \vec{\textit{subclass}} \in \mathbb{R}^{300}$ was computed 
using triple sentence word embeddings (Section~\ref{section:triple sentences}).
The offsets were clustered into $15$ clusters using KMeans clustering.
PCA was applied on the clustered offsets for plotting.
The legend indicates the color of each cluster $k \in [0, 14]$.
Table~\ref{table:offset cluster} shows the corresponding percentage of offsets in each cluster,
as well as the type of offsets in the cluster.

The clusters $1$ ($9.9\%$), $6$ ($23.27\%$)  and $13$ ($45.06\%$), 
which attribute to $78.23\%$ of all offsets, contain a variation  of different subclass-of relation types,
which is most likely the case because $15$ clusters are not sufficient to separate all different types of subclass-of relations.
Therefore algorithms like piecewise linear projection \cite{Fu2014}, which exploit the similarity of specific types of subclass-of relations, need to use more clusters $K$ . 
The other clusters show very specific types of offsets.
It can be seen in Figure~\ref{fig:2d offsets}, that the clusters $2$, $7$ and $10$ have very different orientations
in respect to the origin $(0,0)$ than the other clusters.
Therefore directly using vector offsets would not be applicable, as offsets can not represent the complex subclass-of relation.

\fullcite{Fu2014} reached a similar conclusion and proposes the use of linear projections as means to represent the subclass-of relation.
Additionally, it can be concluded that $15$ clusters is too low for usage in piecewise linear projection, since $3$ clusters, representing $78.23\%$ of all offsets, exist,
which do not have specific topics. 
Therefore higher cluster sizes $K$ should be considered as parameter for piecewise linear projection.
In the following section, the approach by \citeauthor{Fu2014}, which is described in Section~\ref{section:ol nn},
is adjusted for usage in the hybrid algorithm.

\begin{table}
\begin{tabularx}{\textwidth}{l | l | l | X}
$k$ & perc. & topic & example \\
\hline
$2$ & $0.42\%$ & cheeses & \textit{Rigotte de Sainte-Colombe} $\subclassof{}$  \textit{cow's-milk cheese} \\
$3$ & $7.61\%$ & badminton tournaments & \textit{1987 Swiss Badminton Championships} $\subclassof{}$ \textit{badminton tournament} \\
$4$ & $0.32\%$ & chemicals & \textit{Actinomycin} $\subclassof{}$ \textit{chemical compound} \\
$5$ & $1.18\%$ & diseases & \textit{African swine fever} $\subclassof{}$ \textit{disease} \\
$7$ & $3.16\%$ & locomotives, military aircrafts & \textit{GWR 103 President} $\subclassof{}$ \textit{tender locomotive} \textit{Heinkel He 50} $\subclassof{}$ \textit{military aircraft}  \\
$8$ & $3.94\%$ & Alcalde (Spanish mayors) & \textit{mayor of Granollers} $\subclassof{}$ \textit{Alcalde} \\
$9$ & $0.41\%$ & automobiles & \textit{Acura TLX} $\subclassof{}$ \textit{automobile} \\
$10$ & $0.67\%$ & food & \textit{Sour cream doughnut} $\subclassof{}$ \textit{food} \\
$11$ & $1.8\%$ & aircrafts & \textit{Bell XH-15} $\subclassof{}$ \textit{aircraft} \\
$12$ & $1.22\%$ & wine (mostly Italian wines)  & \textit{Greek wine} $\subclassof{}$ \textit{wine}, 
	\textit{Alghero frizzante rosato} $\subclassof{}$ \textit{wine} \\
$14$ & $0.42\%$ & RNA & \textit{Small nucleolar RNA SNORD86} $\subclassof{}$  \textit{RNA} \\
$15$ & $0.65\%$ & ambassadors & \textit{Ambassador of the Republic of China to the United States}
\end{tabularx}
\caption{Topics of subclass-of offset clusters (K=15); $\text{total} = 204,158$.}
\label{table:offset cluster}
\end{table}

\subsubsection{Linear projection}\label{section:linear projection}
Following the idea of using vector offsets to find the correct superclass $\vec{p} \in \mathbb{R}^d$ 
for a given unknown $\vec{u} \in \mathbb{R}^d$,
the translation represented by the subclass offset can be extended by adding a linear projection matrix $\Phi \in \mathbb{R}^{d \times d}$.
This follows the idea of \fullcite{Fu2014}, who uses linear projection without translation
to find the superclasses for a given unknown.
The superclass $\vec{p}$ for an unknown $\vec{u}$ is accordingly computed:
\begin{align}
&\vec{p} = \Phi \vec{u} + \vec{v} = \begin{bmatrix}\Phi & \vec{v}\end{bmatrix} \begin{bmatrix}\vec{u} \\ 1\end{bmatrix}
\end{align}
where $\vec{v} \in \mathbb{R}^d$ is a translation vector.
Combining the translation and projection into a single matrix $T = \begin{bmatrix}\Phi & \vec{v}\end{bmatrix}$
enables the use of \fullcite{Fu2014}'s approach for finding $T^* \in \mathbb{R}^{d \times d}$,
which is the best fit for the given subclass-superclass pairs.
$T^*$ can be computed by minimizing the following mean squared error \cite{Fu2014}:
\begin{align}
&T^* = \argmin_T \frac{1}{N} \sum_{(\vec{x}, \vec{y})} \norm{T \vec{x} - \vec{y}}^2 
\end{align}
where $N$ is the number of $(\vec{x}, \vec{y})$ subclass-superclass pairs in the training data \cite{Fu2014}.

\fullcite{Fu2014} proposes an improvement to the linear projection.
As shown in Figure~\ref{fig:2d offsets}, the subclass offsets appear in clusters,
therefore it is intuitive to assume that training a matrix $T_k \in \mathbb{R}^{d \times d}$ for different clusters $k$
increases the accuracy of the method \cite{Fu2014}.
However, the approach by \fullcite{Fu2014} is not applicable for the given problem.
In \citeauthor{Fu2014}'s stated problem, a an unknown class can have zero, one, or multiple superclasses, but in the thesis' problem an unknown class is always assigned
one superclass.
Additionally, the input for the thesis' classification is a single unknown class, therefore it cannot be decided in which subclass-of offset cluster the unknown class belongs.
Instead of using subclass-of clusters to train the piecewise projections, the subclass-superclass pairs $(\vec{x}, \vec{y})$ are clustered by their input vector $\vec{x}$.
This leads to the following training objective for piecewise linear projections \cite{Fu2014}:
\begin{align}
&T_k^* = \argmin_{T_k} \frac{1}{N_k} \sum_{(\vec{x}, \vec{y})\in C_k} \norm{T_k \vec{x} - \vec{y}}
\end{align}
where $N_k \in \mathbb{N}$ is the amount of word pairs in the $k^\mathit{th}$ cluster $C_k = \{ (\vec{x_i}, \vec{y_i}) |  \vec{x_i} \textit{ in cluster } k, i \in [1, N] \}$ \cite{Fu2014}.
Both training objectives describe multivariate linear regression tasks.
\fullcite{Fu2014} uses stochastic gradient descent (SGD) to solve the objective.
SciPy's SGD regressor \cite{Jones2001} is used to train the linear projection matrices.

The projection for a given unknown $\vec{u}$ using piecewise linear projection can be computed as follows:
\begin{align}
&\vec{p} = T_k^* \begin{bmatrix}\vec{u} \\ 1 \end{bmatrix}
\end{align}
where $\vec{u}$ in cluster $k$.
The appropiate superclass is subsequently the closest class embedding to the computed projection.

The thesis' evaluation will show whether linear projection is superior to the distance-based kNN in the given task,
and to what degree it benefits from word embeddings using graph walk sentences instead of triple sentences.

\subsubsection{Non-linear regression via neural nets}\label{section:nonlinear regression}
The linear projection approach uses linear regression to predict a superclass embedding from a
given unknown embedding. The complexity of the subclass-of relation is simplified
by clustering the embeddings and computing a linear regression for each cluster.
However, it is possible that a linear regression is not sufficient for capturing
all correlations between the input and output embeddings.
\fullcite{Leshno1993} states that a neural network with a non-polynomial activation function
is able to approximate any function. Therefore it is possible to use a neural network to
implement a non-linear regression, which is able to better model the subclass-of relation.
The expressiveness of such a network is dependent on the number of neurons and hidden layers
of the network \cite{Hornik1991}. Increasing the number of layers and neurons can theoretically
maximize the expressiveness of a deep neural network \cite{Raghu2017}, but also increases
the memory usage and runtime, as well as the required amount of training data necessary to train the model
\cite{Raghu2017}. In the following a multi-network model for non-linear multi-variate, multi-target regression
is proposed.

The rectifier is well-suited as an activation function for non-linear regression, as it has multiple favorable
characteristics in comparison to the classically used sigmoid and tanh functions \cite{Maas2013} \cite{Glorot2011}. 
The rectifier is defined as a maximum function $f(x)=\max(0,x)$, which either returns the input or zero.
\fullcite{Glorot2011} states that this function allows the network to learn sparse representation
of inputs, since only a subset of neurons are activated for each input, because the rectifier returns $0$
for all negative inputs. The remaining active neurons describe a linear function, which prevents gradient
vanishing \cite{Glorot2011} and makes computations and backpropagation cheaper \cite{Glorot2011} \cite{Maas2013}.

Neurons using rectifier as activation are called Rectified Linear Units (ReLU).
The output of a densely connected layer of ReLU neurons is computed, as follows:
\begin{align*}
& ReLU(\vec{x}) = \max(W\vec{x} + b, 0) = \begin{cases} W\vec{x} + \vec{b} \text{ for } W\vec{x} + b > 0 \\ 0 \text{ else }\end{cases}
\end{align*}
where $\vec{x}$ is the input,  the output, $W$ the input edge weights and $\vec{b}$ the corresponding bias.

Regression tasks involve predicting a single target value from a given input vector.
The thesis' tasks however requires the prediction of a $d$-dimensional vector from a given $d$-dimensional vector,
where $d$ is the dimension of the word embeddings.
The recommended configuration, as stated in Section~\ref{section:components}, sets $d$ as $300$.
Training $300$ networks to predict each of the single targets to a suitable accuracy requires either
a high memory capacity to train $300$ networks in parallel or a long training time to train the networks
sequentially. A compromise can be reached by training a smaller number of networks to predict
multiple targets at once and then combining the results into the final $d$ dimensional output.

The multi-network model (see Figure~\ref{fig:multinet}) consists of $N$ hidden deep neural networks
with $H$ ReLU layers (including input and output layer). Each hidden network predicts $o = \frac{d}{N}$
targets of the complete target vector. The output of each hidden network is concatenated,
which results in the model's $d$ dimensional output.

\begin{figure}
\centering
\includegraphics[width=0.4\textwidth]{images/algorithm/concat_nn_model.png}
\caption{Multi-network regression model}
\label{fig:multinet}
\end{figure}

The model is defined by the following parameters:
\begin{itemize}
\item $N \in \mathbb{N}$: number of hidden networks
\item $H \in \mathbb{N}$: number of layers per hidden network
\item $d \in \mathbb{N}$: dimension of  word embeddings (input and output dimensions of full model)
\item $h \in \mathbb{N}$: dimension of hidden layers of hidden networks (number of neurons in each hidden layer)
\item $o = \frac{d}{N} \in \mathbb{N}$: output dimension of hidden networks
\end{itemize}

The trainable variables are the weights and biases of the fully connected layers of the hidden networks
and the output layer, which are defined as follows:
\begin{itemize}
\item $W_{i,1} \in \mathbb{R}^{h \times d}, i \in \{1,\cdots,N\}$: input weights of hidden networks, maps word embeddings to hidden layer, with corresponding biases $\vec{b}_{i,1} \in \mathbb{R}^h$.
\item $W_{i,j} \in \mathbb{R}^{h \times h}, i \in \{1,\cdots,N\}, j \in \{2,\cdots,H-1\}$: weights between hidden layers
with corresponding biases $\vec{b}_{i,j} \in \mathbb{R}^h$.
\item $W_{i,H} \in \mathbb{R}^{o \times h}, i \in \{1,\cdots,N\}$: output weights for hidden networks
with corresponding biases $\vec{b}_{i,H} \in \mathbb{R}^o$.
\item $W_{con} \in \mathbb{R}^{d \times d}$: concatenation output layer with corresponding bias
$\vec{b}_{con} \in \mathbb{R}^d$.
\end{itemize}

Let $\vec{u} \in \mathbb{R}^d$ be the input word embedding and $\vec{s} \in \mathbb{R}^d$ the predicted
superclass embedding.
$\vec{s}$ is computed from $\vec{u}$, using $ReLU$ as activation, as follows:
\begin{align*}
&\vec{v}_{i,1} = \max(0, W_{i,1} \cdot \vec{u} + \vec{b}_{i,1})\\
&\vec{v}_{i,j} = \max(0, W_{i,j} \cdot \vec{v}_{i,j-1} + \vec{b}_{i,j}) \text{ for } j \in \{2, \cdots, H-1\}\\
&\vec{v}_{i,H} = \max(0, W_{i,H} \cdot \vec{v}_{i,H-1} + \vec{b}_{i,H})\\
&\vec{s} = W_{con} \cdot \begin{bmatrix}\vec{v}_{1,H} & \cdots & \vec{v}_{N,H}\end{bmatrix} + \vec{b}_{con}
\end{align*}
with $i \in \{1, \cdots, N\}$.

The proposed network has a higher expressiveness than the previously described linear projection approach.
The expressiveness of the model can be experimentally optimized by adjusting the different hyperparameters
like depth and width of the hidden networks \cite{Raghu2017} \cite{Hornik1991}.
Additionally the network uses rectifier as activation function, which is non-polynomial, and therefore,
according to \fullcite{Leshno1993}, able to model an arbitrary function.
In comparison, the linear projection approach can only model a linear function and has limited
reconfigurability with only one hyperparameter, the number of clusters.
Therefore it is predicted that the non-linear neural regression model will perform better than linear projection.
As motivated in the previous chapters, a hybrid algorithm using neural word embeddings is implemented to solve the defined problem.
In this chapter, the required components for the algorithm are presented.
For each component, possible implementations are proposed and implemented.

\subsection{Components}
The hybrid algorithm exploits neural word embeddings to solve the classification task,
which is motivated by the power of word embeddings in similarity tasks \cite{Mikolov2013} and in taxonomy construction \cite{Fu2014}.

For the task of computing word embeddings, the SGNS model (Section~\ref{section:word2vec}) will be used, which has shown to generate effective word embeddings relatively fast 
\cite{Mikolov2013} \cite{Levy2015}.
It has been proven through theoretic analysis as well as experiments that SGNS creates very effective
word embeddings, if its hyperparameters are chosen well \cite{Mikolov2013} \cite{Levy2015}.

Based on \fullcite{Levy2015}, \fullcite{Mikolov2013} and \fullcite{Mikolov2013a} 
the following hyperparameters are used:
\begin{itemize}
\item Embedding size: $300$
\item Subsampling frequency: $10^{-5}$
\item Context window: $2$
\item Negative samples: $15$
\end{itemize}
The original paper by \fullcite{Mikolov2013} uses context windows of size $2$.
Bigger context windows show slight improvements in the quality of word embeddings \cite{Levy2015}.
However triple sentences (see Section~\ref{section:triple sentences}) only have a maximum context of $2$, e.g. $w_1 \: w_2 \: w_3$ the maximum distance
between words in the same sentence is $2$. Therefore triple sentences would not benefit from bigger
context sizes. To preserve comparability between variations of the algorithm, the same context size is also used
for all types of SequenceGen components.
The implementation of SGNS in the gensim library by \fullcite{Rehurek2010} is used.

Two additional components can be identified for the hybrid classification algorithm using SGNS.
The data flow and IO of the components is visualized in Figure~\ref{fig:algorithm data flow}.

\begin{figure}[]
\centering
\includegraphics[width=\textwidth]{images/algorithm/hybrid_algorithm_components.pdf}
\caption{Data flow between components}
\label{fig:algorithm data flow}
\end{figure}

\begin{enumerate}
\item \textbf{SequenceGen}. 
SGNS requires a linear sequence of words as input (see Section~\ref{section:word2vec}).
The SequenceGen component transforms any possible kind of data into a set of sentences, which can be used for training the SGNS.
Possible data sets include Wikidata and Wikipedia.
Wikidata's highly interlinked structure is more alike to an RDF graph \cite{Erxleben2014}.
Using Wikidata therefore requires an approach, which maps the graph to linear sequences.
If the encyclopedic text provided by Wikipedia is used, a different challenge has to be solved.
Since the gold standard consists of pairs of subclass-superclass pairs, which were acquired from Wikidata \cite{WikidataDump},
a mapping from words to Wikidata IDs would have to be implemented.

\item \textbf{Classification}.
The Classification component is trained on a set of gold standard subclass-superclass pairs.
It uses the word embeddings generated by SGNS to make a classification decision.
The word embeddings produced by SGNS group similar words close to each other and preserve linguistic regularities.
These properties can be exploited by different classifiers. 
kNN can exploit the first characteristic using a similarity or distance measure.
The second characteristic can be exploited by computing the vector offset or learning a linear projection representing the subclass-of relationship between two classes \cite{Fu2014}.
The classification component should implement a single-label multiclass classification.
\end{enumerate}

\subsection{SequenceGen}\label{section:sequencegen}

\subsubsection{Triple sentences}\label{section:triple sentences}
Triple sentences uses Wikidata as input data set.
Statements in Wikidata can be represented as triples of $(\mathit{source}, \mathit{property}, \mathit{target})$,
where $\mathit{source}$ is a Wikidata item ID, $\mathit{property}$ is a Wikidata property ID, and $\mathit{target}$ is either a Wikidata item ID or a literal.
These triples represents simple 3-word sentences, which are sufficient as training input for Word2Vec.
Appereances of literals are removed from the generated sentences to reduce the amount of noise in the data.
In comparison to natural text, the triple sentence contain a very low amount of noise, which may improve the quality of embeddings in comparison to using for example
Wikipedia as data set.

\subsubsection{Graph walk sentences}
A graph walk, developed by \fullcite{Ristoski2016} and explained in Section~\ref{section:rdf2vec}
can be used for generating longer sentences instead of triple sentences. 
The longer sequences are able to better capture contextual information than triples.
This is achieved by creating walks of a depth $d$, starting from each class.
Evaluation of RDF2Vec shows that  Skip-gram and CBOW both produce better word embeddings using graph walk \cite{Ristoski2016}.
Therefore it is expected that graph walk will also improve the performance of the hybrid algorithm.
Computing all walks for each vertex is however not feasible, since the number of walks for each vertex is potentially exponential \cite{Ristoski2016}.
The exponential runtime can be counteracted by computing a limited number of random walks instead of computing all walks \cite{Perozzi2014}.
Additionally, the starting vertices for each walk will be sampled from the set of classes rather than the set of all items.
The quality of word embeddings in regards to classes is a major concern, while the word embeddings for other items are only optional for the task
of classification. The proposed measure will ensure that each class occurs at least once in the graph walk sequences and
the frequency of classes in the generated set will be higher than by random sampling from all items.

The set of random walk sequences will be combined with the set of triple sentences, which are used in the baseline, to constitute a new training data set
for the SGNS.

The following parameters are used for the graph walk :
\begin{itemize}
\item Depth of walks: $4$
\item Maximum number of walks per vertice: $10$
\end{itemize}
\fullcite{Ristoski2016} applies $200$ maximum walks per vertice for the Wikidata dataset. 
Because of time and hardware constraints only $10$ walks per vertice are computed,
which should allow a "good" coverage of classes in the generated sequences.

\subsection{Classification}

\subsubsection{k-nearest-neighbors}
Similar classes are grouped close to each other in the embedding vector space.
Intuitively a kNN classifier (see Section \ref{section:knn}) should be able to use this property to great effect.

Distance-based kNN using the euclidean norm is implemented as classification component.
The kNN implementation provided by the scikit-learn library \cite{scikit-learn} is used.

\subsubsection{Vector offset}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{images/algorithm/2d_subclass_offsets.png}
\caption{Subclass-of offsets of $800000$ randomly chosen subclass-superclass pairs}
\label{fig:2d offsets}
\end{figure}

A possible approach for classification is to exploit the linear regularities encoded in the word embeddings.
As shown in Section~\ref{section:word2vec}, it is possible to answer semantic questions by applying algebraic operations on word embeddings \cite{Mikolov2013}.
Given a subclass-superclass pair with word embeddings $\vec{c}, \vec{p}$ and the word embedding $\vec{o}$ of an orphan class,
the superclass with word embedding $\vec{r}$ of $\vec{o}$ could be calculated, as follows 
\begin{align*}
\vec{r} = \vec{p} - \vec{c} + \vec{o}
\end{align*}
The subclass-of relationship is represented by the offset $\vec{p} - \vec{c}$. 
The classification task could be simplified to the task of adding such an offset to a given word embedding.
This is however only possible, if the subclass-of offset is similar for all existing subclass-superclass pairs \cite{Fu2014}.

For the use case of Wikidata, it can be shown that the offsets do not fulfill this criteria. It can rather be seen that different clusters of
similar subclass-of offsets exist. 
Using the word embeddings created by SGNS trained on triple sentences.
the subclass-of offsets for $800000$ randomly chosen subclass-superclass pairs were computed.
Using Principal Component Analysis (PCA), the 300-dimensional vectors were reduced to  2-dimensional vectors,
which are shown in Figure~\ref{fig:2d offsets}.

It can be seen, that clusters of offsets exist, which have completely different orientations in respect to the origin $(0,0)$.
Therefore directly using vector offsets would not be applicable, as offsets can not represent the complex subclass-of relation.
\fullcite{Fu2014} reached a similar conclusion and proposes the use of linear projections as means to represent the subclass-of relation.
In the following section, the approach by \citeauthor{Fu2014} is described and implemented as a variation of the classification component.

\subsubsection{Linear projection}
It is assumed that a class $\vec{x}$ can be projected to its superclass $\vec{y}$ using a matrix $\Phi$, so that $\vec{y} = \Phi \vec{x}$ applies.
\fullcite{Fu2014} propose two variants of linear projection for classification. A uniform linear projection, which trains a single matrix $\Phi$ on all subclass-superclass pairs,
and a piecewise linear projection, which trains $k$ matrices $\Phi_k$ for $k$ clusters of subclass-superclass offsets.

Uniform linear projection tries to minimize the mean squared error of the projected $\Phi \vec{x}$ and the actual result $\vec{y}$, as follows
\begin{align}
&\Phi^* = \min_\Phi \frac{1}{N} \sum_{(\vec{x}, \vec{y})} \norm{\Phi \vec{x} - \vec{y}}^2 
\end{align}
where $N$ is the number of $(\vec{x}, \vec{y})$ subclass-superclass pairs in the training data \cite{Fu2014}.

Piecewise linear projection learns a projection matrix $\Phi_k$ for each identified cluster in the data set.
For this to work, training data needs to be selected in such a way that each cluster is represented in the training data.
Otherwise the training objective is the same to the uniform linear projection:
\begin{align}
&\Phi^* = \min_{\Phi_k} \frac{1}{N_k} \sum_{(\vec{x}, \vec{y})\in C_k} \norm{\Phi_k \vec{x} - \vec{y}}^2 
\end{align}
where $N_k$ is the amount of word pairs in the $k^\mathit{th}$ cluster $C_k$ \cite{Fu2014}.

Both training objectives describe multivariate linear regression tasks.
\fullcite{Fu2014} uses stochastic gradient descent (SGD) to solve the objective.
In the thesis' implementation SciPy's SGD regressor \cite{Jones2001} is used to train the linear projection matrices.
The thesis' evaluation will show whether linear projection is superior to the KRI-kNN in the given task,
and to what degree it benefits from word embeddings using graph walk sentences instead of triple sentences.

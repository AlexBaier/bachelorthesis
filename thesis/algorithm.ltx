\subsection{Components}
The hybrid algorithm will use a Word2Vec model to generate word embeddings, which
then can be used in a classification task.
This leads to the identification of three main components. These main components
will be implemented by a baseline algorithm and for each variation an improvement is proposed,
which in general replaces one of the components.
The data flow and input/output between the components is visualized in Figure~\ref{fig:algorithm data flow}

\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{images/algorithm/combined_algorithm_components.pdf}
\caption{Data flow between components}
\label{fig:algorithm data flow}
\end{figure}

\begin{enumerate}
\item \textbf{Wikidata2Sequence}. 
For the task of generating word embeddings the Skip-gram model with negative sampling will be used.
As mentioned in Section~\ref{section:word2vec}, both Word2Vec models require a linear sequence of words as input.
Wikidata's highly interlinked structure is more alike to an RDF graph \cite{Erxleben2014}. 
Therefore the first required component for the algorithm is a mapping from Wikidata to word sequences.
The output of this method is required to be at least one sequence of words, which is called sentence.
Therefore multiple sentences represent a text. Each word in the sequence has to be either a Wikidata item
or property ID, or the data type for a literal value (e.g. replace all strings with \textit{string}).

\item \textbf{Word2Vec}. The Skip-gram model with negative sampling (SGNS)
will be used in every variation of the algorithm.
It has been proven through theoretic analysis as well as experiments that SGNS creates very effective
word embeddings, if its hyperparameters are chosen well. \textbf{TODO: citations}

\item \textbf{Classification}. The final component of the algorithm is a classification method.
The word embeddings produced by SGNS group similar words close to each other and
preserve linguistic regularities. 
kNN can exploit the first characteristic using a similarity or distance measure.
The second characteristic can be exploited by computing the vector offset
or learning a linear projection representing the subclass-of relationship between two classes
\cite{Fu2014}.
For the purpose of evaluation each classification component should have the same output,
this will allow a comparison between the different variations of the algorithm.
Therefore each classification method will output the Wikidata ID (e.g. Q35120) of the most likely superclass.
\end{enumerate}

Wikidata2Sequence and Word2Vec are executed once to compute the word embeddings of
the corresponding Wikidata dump.
The classification method will then be trained on the word embeddings and can
be applied repeatedly to unlinked classes.

\subsection{Baseline}\label{section:baseline}
The baseline algorithm uses a very basic Wikidata2Sequence component and simple weighted kNN 
for classification. If not otherwise mentioned, the variations will use the same hyperparameters and
components as the baseline algorithm. 
\begin{enumerate}
\item \textbf{Wikidata2Sequence}. Trivial sentences will be generated from statements.
Each statement can be transformed to a triple $(\mathit{itemid}, \mathit{pid}, \mathit{value})$,
where value is either another item ID or a data type.
\textbf{Insert example.}

\item \textbf{Word2Vec}. As mentioned above a Skip-gram model with negative sampling will be used.
Based on \fullcite{Levy2015}, \fullcite{Mikolov2013} and \fullcite{Mikolov2013a} 
the following hyperparameters have been proven to be effective in
the task of creating word embeddings and are therefore used:
\begin{itemize}
\item Embedding size: $300$
\item Subsampling frequency: $10^{-5}$
\item Context window: $2$
\item Negative samples: $15$
\end{itemize}
The original paper by \fullcite{Mikolov2013} uses context windows of size $2$.
Bigger context windows show slight improvements in the quality of word embeddings \fullcite{Levy2015}.
However triple sentences only have a maximum context of $2$, e.g. $w_1 \: w_2 \: w_3$ the maximum distance
between words in the same sentence is $2$. Therefore triple sentences would not benefit from bigger
context sizes. To preserve comparability between variations of the algorithm, the same context size is also used
for graph walk sentences, if not mentioned otherwise. 

\item \textbf{Classification}. KRI-kNN by \fullcite{Chen2009} is implemented (see Section~\ref{section:knn}).
As similarity measure the cosine similarity will be used, which finds precedence in other work about 
word embeddings like \fullcite{Mikolov2013}.
\end{enumerate}

\subsection{Variation: Graph walk}
A graph walk, developed by \fullcite{Ristoski2016} and explained in Section~\ref{section:rdf2vec}
can be used for generating longer sentences instead of triple sentences. 
The longer sequences are able to better capture contextual information than three word sentences.
This is achieved by creating walks of a depth $d$, starting from each class.
Evaluation of RDF2Vec shows that  Skip-gram and CBOW both produce better word embeddings this way 
\cite{Ristoski2016}.
Therefore it is expected that graph walk will also improve the performance of the hybrid algorithm.
Computing all walks for each vertex is however not feasible, since the number of walks for each vertex is potentially exponential \cite{Ristoski2016}.
The exponential runtime can be counteracted by computing a limited number of random walks instead of computing all walks \cite{Perozzi2014}.
Additionally, the starting vertices for each walk will be sampled the set of classes rather than the set of all items.
The quality of word embeddings in regards to classes is a major concern, while the word embeddings for other items are only optional for the task
of classification. The proposed measure will ensure that each class occurs at least once in the graph walk sequences and
the frequency of classes in the generated set will be higher than by random sampling from all items.

The set of random walk sequences will be combined with the set of triple sentences, which are used in the baseline, to constitute a new training data set
for the SGNS.

The following parameters are used for the graph walk based on \fullcite{Ristoski2016}:
\begin{itemize}
\item Depth of walks: $4$
\item Maximum number of walks per vertice: $100$
\end{itemize}

\subsection{Variation: Vector offset}
A possible approach for classification is to exploit the linear regularities encoded in the word embeddings.
As shown in Section~\ref{section:word2vec}, it is possible to answer semantic questions by applying algebraic operations on word embeddings \cite{Mikolov2013}.
Given a subclass-superclass pair with word embeddings $\vec{c}, \vec{p}$ and the word embedding $\vec{o}$ of an orphan class,
the superclass with word embedding $\vec{r}$ of $\vec{o}$ could be calculated, as follows 
\begin{align*}
\vec{r} = \vec{p} - \vec{c} + \vec{o}
\end{align*}
The subclass-of relationship is represented by the offset $\vec{p} - \vec{c}$. 
The classification task could be simplified to the task of adding such an offset to a given word embedding.
This is however only possible, if the subclass-of offset is similar for all existing subclass-superclass pairs \cite{Fu2014}.

For the use case of Wikidata, it can be shown that the offsets do not fulfill this criteria. It can rather be seen that different clusters of
similar subclass-of offsets exist. 
Using the word embeddings created by the baseline SGNS (Section~\ref{section:baseline})
the subclass-of offsets for $800000$ randomly chosen subclass-superclass pairs were computed.
Using Principal Component Analysis (PCA), the 300-dimensional vectors were reduced to  2-dimensional vectors,
which are shown in Figure~\ref{fig:2d offsets}.

\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{images/algorithm/2d_subclass_offsets.png}
\caption{Subclass-of offsets of $800000$ randomly chosen subclass-superclass pairs}
\label{fig:2d offsets}
\end{figure}

It can be seen, that clusters of offsets exist, which have completely different orientations in respect to the origin $(0,0)$.
Therefore directly using vector offsets would not be applicable, as offsets can not represent the complex subclass-of relation.
\fullcite{Fu2014} reached a similar conclusion and proposes the use of linear projections as means to represent the subclass-of relation.
In the following section, the approach by \citeauthor{Fu2014} is described and implemented as a variation of the classification component.

\subsection{Variation: Linear projection}

\subsection{Variation: Wikipedia}

\subsection{Implementation details}
\textbf{Is this section necessary?}

% Word embeddings main paper
@article{Mikolov2013,
  author    = {Tomas Mikolov and
               Kai Chen and
               Greg Corrado and
               Jeffrey Dean},
  title     = {Efficient Estimation of Word Representations in Vector Space},
  journal   = {CoRR},
  volume    = {abs/1301.3781},
  year      = {2013},
  url       = {\url{http://arxiv.org/abs/1301.3781}},
  timestamp = {Thu, 07 May 2015 20:02:01 +0200},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/abs-1301-3781},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}
% Hyper-parameters for different word embeddings
@article{Levy2015,
	author = {Levy, Omer  and Goldberg, Yoav  and Dagan, Ido },
	title = {Improving Distributional Similarity with Lessons Learned from Word Embeddings},
	journal = {Transactions of the Association for Computational Linguistics},
	volume = {3},
	year = {2015},
	keywords = {},
	abstract = {Recent trends suggest that neural-network-inspired word embedding models outperform traditional count-based distributional models on word similarity and analogy detection tasks. We reveal that much of the performance gains of word embeddings are due to certain system design choices and hyperparameter optimizations, rather than the embedding algorithms themselves. Furthermore, we show that these modifications can be transferred to traditional distributional models, yielding similar gains. In contrast to prior reports, we observe mostly local or insignificant performance differences between the methods, with no global advantage to any single approach over the others.},
	issn = {2307-387X},
	url = {\url{https://transacl.org/ojs/index.php/tacl/article/view/570}},
	pages = {211--225}
}
% explanation for skip-gram model
@article{Goldberg14,
  author    = {Yoav Goldberg and
               Omer Levy},
  title     = {word2vec Explained: deriving Mikolov et al.'s negative-sampling word-embedding
               method},
  journal   = {CoRR},
  volume    = {abs/1402.3722},
  year      = {2014},
  url       = {\url{http://arxiv.org/abs/1402.3722}},
  timestamp = {Wed, 05 Mar 2014 14:43:44 +0100},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/GoldbergL14},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}
% more on Skip-gram modeling
@inproceedings{Guthrie2006,
  address = {Genoa, Italy},
  author = {Guthrie, David and Allison, Ben and Liu, W. and Guthrie, Louise and Wilks, Yorick},
  booktitle = {Proceedings of the Fifth international Conference on Language Resources and Evaluation (LREC-2006)},
  interhash = {7bed3d3ef712b3ddbe0e3ec5795135b2},
  intrahash = {da23a4932aeebf077c236f9bf65d0298},
  title = {A Closer Look at Skip-gram Modelling},
  year = 2006
}
% deep neural network for graph representation
@inproceedings{Cao2016,
  added-at = {2016-04-21T00:00:00.000+0200},
  author = {Cao, Shaosheng and Lu, Wei and Xu, Qiongkai},
  biburl = {\url{http://www.bibsonomy.org/bibtex/2fe0553a0f56deb89f08675922c74c693/dblp}},
  booktitle = {AAAI},
  editor = {Schuurmans, Dale and Wellman, Michael P.},
  ee = {\url{http://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/view/12423}},
  interhash = {8d446898df77367778ed6deca96e68b8},
  intrahash = {fe0553a0f56deb89f08675922c74c693},
  keywords = {dblp},
  pages = {1145-1152},
  publisher = {AAAI Press},
  timestamp = {2016-04-22T11:37:42.000+0200},
  title = {Deep Neural Networks for Learning Graph Representations.},
  url = {http://dblp.uni-trier.de/db/conf/aaai/aaai2016.html\#CaoLX16},
  year = 2016
}
% deep neural networks
@ARTICLE{Raghu2016,
   author = {{Raghu}, M. and {Poole}, B. and {Kleinberg}, J. and {Ganguli}, S. and 
	{Sohl-Dickstein}, J.},
    title = "{On the expressive power of deep neural networks}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1606.05336},
 primaryClass = "stat.ML",
 keywords = {Statistics - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Learning},
     year = 2016,
    month = jun,
   adsurl = {\url{http://adsabs.harvard.edu/abs/2016arXiv160605336R}},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}
% recursive neural network
@article{Scarselli2009,
author = {Scarselli, F. and Gori, M. and {Ah Chung Tsoi} and Hagenbuchner, M. and Monfardini, G.},
doi = {10.1109/TNN.2008.2005605},
issn = {1045-9227},
journal = {IEEE Transactions on Neural Networks},
month = {jan},
number = {1},
pages = {61--80},
title = {{The Graph Neural Network Model}},
url = {\url{http://ieeexplore.ieee.org/document/4700287/}},
volume = {20},
year = {2009}
}
% ontology learning from text
@article{Wong2012,
 author = {Wong, Wilson and Liu, Wei and Bennamoun, Mohammed},
 title = {Ontology Learning from Text: A Look Back and into the Future},
 journal = {ACM Comput. Surv.},
 issue_date = {August 2012},
 volume = {44},
 number = {4},
 month = sep,
 year = {2012},
 issn = {0360-0300},
 pages = {20:1--20:36},
 articleno = {20},
 numpages = {36},
 url = {\url{http://doi.acm.org/10.1145/2333112.2333115}},
 doi = {10.1145/2333112.2333115},
 acmid = {2333115},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Ontology learning, application of ontologies, concept discovery, semantic relation acquisition, term recognition},
}
% intro to ontology learning
@incollection{Cimiano2009,
  abstract = {Ontology learning techniques serve the purpose of supporting an ontology
engineer in the task of creating and maintaining an ontology. In this chapter, we present a comprehensive and concise introduction to the field of ontology learning.
We present a generic architecture for ontology learning systems and discuss its main components. In addition, we introduce the main problems and challenges addressed in the field and give an overview of the most important methods applied. We conclude with a brief discussion of advanced issues which pose interesting challenges to the state-of-the-art.},
  added-at = {2011-01-17T10:16:48.000+0100},
  author = {Cimiano, P. and Mädche, A. and Staab, S. and Völker, J.},
  biburl = {https://www.bibsonomy.org/bibtex/2f9f8bb0af1a8a514c270f83237313ac7/hotho},
  booktitle = {Handbook on Ontologies},
  description = {DOI 10.1007/978-3-540-92673-3},
  edition = {2nd revised edition},
  editor = {Staab, S. and Studer, R.},
  interhash = {5387f28040285a086ab706bc33e7d7af},
  intrahash = {f9f8bb0af1a8a514c270f83237313ac7},
  keywords = {learning ontology},
  pages = {245--267},
  publisher = {Springer},
  series = {International Handbooks on Information Systems},
  timestamp = {2011-01-17T10:16:49.000+0100},
  title = {Ontology Learning},
  url = {http://www.uni-koblenz.de/~staab/Research/Publications/2009/handbookEdition2/ontology-learning-handbook2.pdf},
  year = 2009
}
% ontology learning related work
@inproceedings{dAmato16,
  added-at = {2016-06-06T00:00:00.000+0200},
  author = {d'Amato, Claudia and Staab, Steffen and Tettamanzi, Andrea G. B. and Minh, Tran Duc and Gandon, Fabien L.},
  biburl = {https://www.bibsonomy.org/bibtex/2383bd3c8b3d0a42649eaa145d4ada382/dblp},
  booktitle = {SAC},
  editor = {Ossowski, Sascha},
  ee = {http://doi.acm.org/10.1145/2851613.2851842},
  interhash = {107b5a597194a2814ca5fa1916089260},
  intrahash = {383bd3c8b3d0a42649eaa145d4ada382},
  isbn = {978-1-4503-3739-7},
  keywords = {dblp},
  pages = {333-338},
  publisher = {ACM},
  timestamp = {\url{2016-06-07T11:38:58.000+0200}},
  title = {Ontology enrichment by discovering multi-relational association rules from ontological knowledge bases.},
  url = {http://dblp.uni-trier.de/db/conf/sac/sac2016.html\#dAmatoSTMG16},
  year = 2016
}
% ontology learning and population from text
@book{Cimiano2006,
 author = {Cimiano, Philipp},
 title = {Ontology Learning and Population from Text: Algorithms, Evaluation and Applications},
 year = {2006},
 isbn = {0387306323},
 publisher = {Springer-Verlag New York, Inc.},
 address = {Secaucus, NJ, USA},
} 
% rule learning ontology
@phdthesis{Galarraga2016,
author = {Gal{\'{a}}rraga, Luis},
school = {Telecom ParisTech},
title = {{Rule Mining in Knowledge Bases}},
year = {2016}
}
% neural network for ontology learning
@article{Petrucci16,
  author    = {Giulio Petrucci and
               Chiara Ghidini and
               Marco Rospocher},
  title     = {Using Recurrent Neural Network for Learning Expressive Ontologies},
  journal   = {CoRR},
  volume    = {abs/1607.04110},
  year      = {2016},
  url       = {\url{http://arxiv.org/abs/1607.04110}},
  timestamp = {Tue, 02 Aug 2016 12:59:27 +0200},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/PetrucciGR16},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}
% ontology learning survey
@article{Hazman2011,
abstract = {The problem that ontology learning deals with is the knowledge acquisition bottleneck, that is to say the difficulty to actually model the knowledge relevant to the domain of interest. Ontologies are the vehicle by which we can model and share the knowledge among various applications in a specific domain. So many research developed several ontology learning approaches and systems. In this paper, we present a survey for the different approaches in ontology learning from semi-structured and unstructured date General Terms Ontology learning approaches.},
author = {Hazman, Maryam and El-Beltagy, Samhaa R and Rafea, Ahmed},
journal = {International Journal of Computer Applications},
keywords = {Ontology learning,Ontology learning evaluation,knowledge discovery},
number = {9},
pages = {975--8887},
title = {{A Survey of Ontology Learning Approaches}},
volume = {22},
year = {2011}
}

% ontology mapping (older paper), useful as reference for problem statement
@inproceedings{Doan2002,
 author = {Doan, AnHai and Madhavan, Jayant and Domingos, Pedro and Halevy, Alon},
 title = {Learning to Map Between Ontologies on the Semantic Web},
 booktitle = {Proceedings of the 11th International Conference on World Wide Web},
 series = {WWW '02},
 year = {2002},
 isbn = {1-58113-449-5},
 location = {Honolulu, Hawaii, USA},
 pages = {662--673},
 numpages = {12},
 url = {\url{http://doi.acm.org/10.1145/511446.511532}},
 doi = {10.1145/511446.511532},
 acmid = {511532},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {machine learning, ontology mapping, relaxation labeling, semantic web},
}

% hierarchy learning via word embeddings
@article{Fu2014,
abstract = {Semantic hierarchy construction aims to build structures of concepts linked by hypernym–hyponym (“is-a”) relations. A major challenge for this task is the automatic discovery of such relations. This paper proposes a novel and effec- tive method for the construction of se- mantic hierarchies based on word em- beddings, which can be used to mea- sure the semantic relationship between words. We identify whether a candidate word pair has hypernym–hyponym rela- tion by using the word-embedding-based semantic projections between words and their hypernyms. Our result, an F-score of 73.74{\%}, outperforms the state-of-the- art methods on a manually labeled test dataset. Moreover, combining our method with a previous manually-built hierarchy extension method can further improve F- score to 80.29{\%}. 1},
author = {Fu, Ruiji and Guo, Jiang and Qin, Bing and Che, Wanxiang and Wang, Haifeng and Liu, Ting},
isbn = {9781937284725},
journal = {Acl},
pages = {1199--1209},
pmid = {1626155},
title = {{Learning Semantic Hierarchies via Word Embeddings}},
year = {2014}
}

% similarity (information theory)
@inproceedings{Lin1998,
 author = {Lin, Dekang},
 title = {An Information-Theoretic Definition of Similarity},
 booktitle = {Proceedings of the Fifteenth International Conference on Machine Learning},
 series = {ICML '98},
 year = {1998},
 isbn = {1-55860-556-8},
 pages = {296--304},
 numpages = {9},
 url = {\url{http://dl.acm.org/citation.cfm?id=645527.657297}},
 acmid = {657297},
 publisher = {Morgan Kaufmann Publishers Inc.},
 address = {San Francisco, CA, USA},
}
% vector similarity
@phdthesis{Weber2000,
abstract = {For a large variety of multimedia applications, similarity search provides an effective tool to find relevant information. A common implementation of similarity search transforms the objects into high-dimensional feature vectors. Searching for the most similar object to a given reference object then corresponds to finding the nearest neighbor (NN) in the feature space. Although a large number of algorithms provide efficient solutions for the NN-search problem in low-dimensional spaces, none of the proposed methods provides satisfactory response times for high-dimensional spaces as required for similarity retrieval in multimedia collections. A first contribution of this work is to formally show that all hierarchical organization finally degenerate to traversing the entire structure to find the nearest neighbor. We further show practically that a brute-force scan through the vector data outperforms such organizations already for low feature space dimensionalities in the order of 5. For NN-searches in high-dimensional spaces, we typically observe that a linear method performs faster than hierarchical approaches by more than a factor of 10. As a second contribution, we propose a new indexing method optimized for high-dimensional spaces. The so-called vector approximation file (VA-File) neither clusters data points nor maintains them in a hierarchical structure. Rather, its approach is to approximate the vectors with a quantization scheme and to use mainly these small approximations for the search. Experiments confirm the superiority over hierarchical structures for highdimensional data (i.e. more than 10 dimensions). Furthermore, we provide in-depth analysis of the method and investigate the influence of various parameters on the search efficiency. However, the method is still linear in the number of vectors and in the number of dimensions. As a consequence, it may not allow interactive-time retrieval for large collections if the dimensionality of the feature space is very large ({\textgreater} 100). Therefore, our third contribution is to extend the VA-File method towards faster evaluation schemes. The first approach trades result quality for response time, i.e. we allow for a relatively small error in the result in trade of reduced execution costs. The second approach deploys parallel resources to speedup query evaluation. We describe an evaluation scheme for a parallel version of the VA-File on a cluster of workstations. A further and important contribution of this work relates to quality aspects of NN-search. Recently, it was argued that high-dimensionality prohibits meaningful applications of NN-search. Our investigations, however, revealed that NN-search indeed makes sense, both from a theoretical point of view, and a practical point of view.},
author = {Weber, Roger},
pages = {210},
school = {SWISS FEDERAL INSTITUTE OF TECHNOLOGY ZURICH},
title = {{Similarity Search in High-Dimensional Vector Spaces}},
year = {2000}
}

% ontology mapping, similarity
@article{Rodriguez2003,
 author = {Rodr\'{\i}guez, M. Andrea and Egenhofer, Max J.},
 title = {Determining Semantic Similarity Among Entity Classes from Different Ontologies},
 journal = {IEEE Trans. on Knowl. and Data Eng.},
 issue_date = {February 2003},
 volume = {15},
 number = {2},
 month = feb,
 year = {2003},
 issn = {1041-4347},
 pages = {442--456},
 numpages = {15},
 url = {\url{http://dx.doi.org/10.1109/TKDE.2003.1185844}},
 doi = {10.1109/TKDE.2003.1185844},
 acmid = {642948},
 publisher = {IEEE Educational Activities Department},
 address = {Piscataway, NJ, USA},
 keywords = {Semantic similarity, ontology integration, information integration, semantic interoperability, semantic matching.},
} 
% multi-label kNN
@inproceedings{Zhang2005,
    author = {Zhang, Min-Ling and Zhou, Zhi-Hua},
    citeulike-article-id = {1164920},
    citeulike-linkout-0 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=1547385},
    journal = {IEEE International Conference on Granular Computing},
    organization = {The IEEE Computational Intelligence Society},
    pages = {718--721 Vol. 2},
    posted-at = {2011-03-21 06:26:51},
    priority = {2},
    title = {{A k-Nearest Neighbor Based Algorithm for Multi-label Classification}},
    url = {\url{http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=1547385}},
    volume = {2},
    year = {2005}
}
% similarity-based classification
@article{Chen2009,
 author = {Chen, Yihua and Garcia, Eric K. and Gupta, Maya R. and Rahimi, Ali and Cazzanti, Luca},
 title = {Similarity-based Classification: Concepts and Algorithms},
 journal = {J. Mach. Learn. Res.},
 issue_date = {12/1/2009},
 volume = {10},
 month = jun,
 year = {2009},
 issn = {1532-4435},
 pages = {747--776},
 numpages = {30},
 url = {\url{http://dl.acm.org/citation.cfm?id=1577069.1577096}},
 acmid = {1577096},
 publisher = {JMLR.org},
} 
% evaluation
@inproceedings{Dellschaft2006,
 author = {Dellschaft, Klaas and Staab, Steffen},
 title = {On How to Perform a Gold Standard Based Evaluation of Ontology Learning},
 booktitle = {Proceedings of the 5th International Conference on The Semantic Web},
 series = {ISWC'06},
 year = {2006},
 isbn = {3-540-49029-9, 978-3-540-49029-6},
 location = {Athens, GA},
 pages = {228--241},
 numpages = {14},
 url = {\url{http://dx.doi.org/10.1007/11926078_17}},
 doi = {10.1007/11926078_17},
 acmid = {2127063},
 publisher = {Springer-Verlag},
 address = {Berlin, Heidelberg},
}
% about n-grams
@incollection{Jurafsky2014,
author = {Jurafsky, Daniel and Martin, James H.},
booktitle = {Speech and Language Processing},
chapter = {N-Grams},
title = {{N-Grams}},
year = {2014}
}
% wikidata subclass of documentation
@misc{WikidataP279,
title={Property talk:P279},
howpublished={\url{https://www.wikidata.org/wiki/Property_talk:P279}}
note={Accessed: 2017-01-29}
}
% wikidata entity talk
@misc{WikidataQ35120,
title={Talk:Q35120},
howpublished={\url{https://www.wikidata.org/wiki/Talk:Q35120}},
note={Accessed: 2017-01-29}
}
% wikidata project chat about subclass tree
@misc{WikidataSubclassTree,
title={Wikidata project chat: Top of the subclass tree},
howpublished={\url{https://www.wikidata.org/w/index.php?title=Wikidata:Project_chat&oldid=91040961#Top_of_the_subclass_tree}},
note={Accessed: 2017-01-30}
}
% wikidata new item
@misc{WikidataNewItem,
title={Wikidata: Create a new item},
howpublished={\url{https://www.wikidata.org/wiki/Special:NewItem}},
note={Accessed: 2017-02-05}
}
% neural network introduction
@book{Nielsen2017,
  author={Nielsen, Michael},
  year={2017},
  title={Neural Networks and Deep Learning},
  url={http://neuralnetworksanddeeplearning.com/}
}
% ""
@book{Kriesel2005,
  author={Kriesel, David},
  year={2005},
  title={A Brief Introduction to Neural Networks},
  url={http://www.dkriesel.com/_media/science/neuronalenetze-en-zeta2-2col-dkrieselcom.pdf},
}
% examples for nns
@article{Kalchbrenner2014,
  author    = {Nal Kalchbrenner and
               Edward Grefenstette and
               Phil Blunsom},
  title     = {A Convolutional Neural Network for Modelling Sentences},
  journal   = {CoRR},
  volume    = {abs/1404.2188},
  year      = {2014},
  url       = {http://arxiv.org/abs/1404.2188},
  timestamp = {Thu, 01 May 2014 14:56:20 +0200},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/KalchbrennerGB14},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}
@inproceedings{Arisoy2012,
 author = {Arisoy, Ebru and Sainath, Tara N. and Kingsbury, Brian and Ramabhadran, Bhuvana},
 title = {Deep Neural Network Language Models},
 booktitle = {Proceedings of the NAACL-HLT 2012 Workshop: Will We Ever Really Replace the N-gram Model? On the Future of Language Modeling for HLT},
 series = {WLM '12},
 year = {2012},
 location = {Montreal, Canada},
 pages = {20--28},
 numpages = {9},
 url = {http://dl.acm.org/citation.cfm?id=2390940.2390943},
 acmid = {2390943},
 publisher = {Association for Computational Linguistics},
 address = {Stroudsburg, PA, USA},
}
@article{Gregor2015,
  author    = {Karol Gregor and
               Ivo Danihelka and
               Alex Graves and
               Daan Wierstra},
  title     = {{DRAW:} {A} Recurrent Neural Network For Image Generation},
  journal   = {CoRR},
  volume    = {abs/1502.04623},
  year      = {2015},
  url       = {http://arxiv.org/abs/1502.04623},
  timestamp = {Mon, 02 Mar 2015 14:17:34 +0100},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/GregorDGW15},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}
% Word embeddings main paper
@article{Mikolov2013,
  author    = {Tomas Mikolov and
               Kai Chen and
               Greg Corrado and
               Jeffrey Dean},
  title     = {Efficient Estimation of Word Representations in Vector Space},
  journal   = {CoRR},
  volume    = {abs/1301.3781},
  year      = {2013},
  url       = {http://arxiv.org/abs/1301.3781},
  timestamp = {Thu, 07 May 2015 20:02:01 +0200},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/abs-1301-3781},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}
% Hyper-parameters for different word embeddings
@article{Levy2015,
	author = {Levy, Omer  and Goldberg, Yoav  and Dagan, Ido },
	title = {Improving Distributional Similarity with Lessons Learned from Word Embeddings},
	journal = {Transactions of the Association for Computational Linguistics},
	volume = {3},
	year = {2015},
	keywords = {},
	abstract = {Recent trends suggest that neural-network-inspired word embedding models outperform traditional count-based distributional models on word similarity and analogy detection tasks. We reveal that much of the performance gains of word embeddings are due to certain system design choices and hyperparameter optimizations, rather than the embedding algorithms themselves. Furthermore, we show that these modifications can be transferred to traditional distributional models, yielding similar gains. In contrast to prior reports, we observe mostly local or insignificant performance differences between the methods, with no global advantage to any single approach over the others.},
	issn = {2307-387X},
	url = {https://transacl.org/ojs/index.php/tacl/article/view/570},
	pages = {211--225}
}
@inproceedings{Levy2014a,
  address = {Baltimore, Maryland},
  author = {Levy, Omer and Goldberg, Yoav},
  biburl = {https://www.bibsonomy.org/bibtex/2c08ef42c3320976b65a9833180815f0e/gchrupala},
  booktitle = {Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  interhash = {ec04086ee882f28c71d67260e100f2e5},
  intrahash = {c08ef42c3320976b65a9833180815f0e},
  keywords = {imported},
  month = {June},
  pages = {302--308},
  publisher = {Association for Computational Linguistics},
  timestamp = {2014-12-13T13:16:07.000+0100},
  title = {Dependency-Based Word Embeddings},
  url = {http://www.aclweb.org/anthology/P14-2050},
  year = 2014
}
@inproceedings{Levy2014,
 author = {Levy, Omer and Goldberg, Yoav},
 title = {Neural Word Embedding As Implicit Matrix Factorization},
 booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems},
 series = {NIPS'14},
 year = {2014},
 location = {Montreal, Canada},
 pages = {2177--2185},
 numpages = {9},
 url = {http://dl.acm.org/citation.cfm?id=2969033.2969070},
 acmid = {2969070},
 publisher = {MIT Press},
 address = {Cambridge, MA, USA},
} 
% skip-gram for phrases
@article{Mikolov2013a,
  author    = {Tomas Mikolov and
               Ilya Sutskever and
               Kai Chen and
               Greg Corrado and
               Jeffrey Dean},
  title     = {Distributed Representations of Words and Phrases and their Compositionality},
  journal   = {CoRR},
  volume    = {abs/1310.4546},
  year      = {2013},
  url       = {http://arxiv.org/abs/1310.4546},
  timestamp = {Thu, 07 May 2015 20:02:01 +0200},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/MikolovSCCD13},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}
% explanation for skip-gram model
@article{Goldberg2014,
  author    = {Yoav Goldberg and
               Omer Levy},
  title     = {word2vec Explained: deriving Mikolov et al.'s negative-sampling word-embedding
               method},
  journal   = {CoRR},
  volume    = {abs/1402.3722},
  year      = {2014},
  url       = {http://arxiv.org/abs/1402.3722},
  timestamp = {Wed, 05 Mar 2014 14:43:44 +0100},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/GoldbergL14},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}
@article{Rong2014,
  author    = {Xin Rong},
  title     = {word2vec Parameter Learning Explained},
  journal   = {CoRR},
  volume    = {abs/1411.2738},
  year      = {2014},
  url       = {http://arxiv.org/abs/1411.2738},
  timestamp = {Mon, 01 Dec 2014 14:32:13 +0100},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/Rong14},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}
% more on Skip-gram modeling
@inproceedings{Guthrie2006,
  address = {Genoa, Italy},
  author = {Guthrie, David and Allison, Ben and Liu, W. and Guthrie, Louise and Wilks, Yorick},
  booktitle = {Proceedings of the Fifth international Conference on Language Resources and Evaluation (LREC-2006)},
  interhash = {7bed3d3ef712b3ddbe0e3ec5795135b2},
  intrahash = {da23a4932aeebf077c236f9bf65d0298},
  title = {A Closer Look at Skip-gram Modelling},
  year = 2006
}
% some results on CBOW
@article{Baroni2014,
  added-at = {2016-01-28T11:05:54.000+0100},
  author = {Baroni, Marco and Dinu, Georgiana},
  biburl = {https://www.bibsonomy.org/bibtex/2d0acaa560edb2aed852b2321856aaf08/ans},
  description = {Don't count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors},
  interhash = {727478fd816465de4a943dac5d92268b},
  intrahash = {d0acaa560edb2aed852b2321856aaf08},
  journal = {52nd Annual Meeting of the Association for Computational Linguistics, ACL 2014 - Proceedings of the Conference},
  keywords = {representation semantic vector},
  pages = {238-247},
  timestamp = {2016-01-28T11:05:54.000+0100},
  title = {Don't count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors},
  volume = 1,
  year = 2014
}
% rdf2vec
@Inbook{Ristoski2016,
author="Ristoski, Petar
and Paulheim, Heiko",
editor="Groth, Paul
and Simperl, Elena
and Gray, Alasdair
and Sabou, Marta
and Kr{\"o}tzsch, Markus
and Lecue, Freddy
and Fl{\"o}ck, Fabian
and Gil, Yolanda",
title="RDF2Vec: RDF Graph Embeddings for Data Mining",
bookTitle="The Semantic Web -- ISWC 2016: 15th International Semantic Web Conference, Kobe, Japan, October 17--21, 2016, Proceedings, Part I",
year="2016",
publisher="Springer International Publishing",
address="Cham",
pages="498--514",
isbn="978-3-319-46523-4",
doi="10.1007/978-3-319-46523-4_30",
url="http://dx.doi.org/10.1007/978-3-319-46523-4_30"
}
% deep walk
@article{Perozzi2014,
  author    = {Bryan Perozzi and
               Rami Al{-}Rfou and
               Steven Skiena},
  title     = {DeepWalk: Online Learning of Social Representations},
  journal   = {CoRR},
  volume    = {abs/1403.6652},
  year      = {2014},
  url       = {http://arxiv.org/abs/1403.6652},
  timestamp = {Tue, 01 Apr 2014 11:56:46 +0200},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/PerozziAS14},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}
% deep neural network for graph representation
@inproceedings{Cao2016,
  added-at = {2016-04-21T00:00:00.000+0200},
  author = {Cao, Shaosheng and Lu, Wei and Xu, Qiongkai},
  biburl = {http://www.bibsonomy.org/bibtex/2fe0553a0f56deb89f08675922c74c693/dblp},
  booktitle = {AAAI},
  editor = {Schuurmans, Dale and Wellman, Michael P.},
  ee = {http://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/view/12423},
  interhash = {8d446898df77367778ed6deca96e68b8},
  intrahash = {fe0553a0f56deb89f08675922c74c693},
  keywords = {dblp},
  pages = {1145-1152},
  publisher = {AAAI Press},
  timestamp = {2016-04-22T11:37:42.000+0200},
  title = {Deep Neural Networks for Learning Graph Representations.},
  url = {http://dblp.uni-trier.de/db/conf/aaai/aaai2016.html\#CaoLX16},
  year = 2016
}
% expressivity of deep neural networks
@ARTICLE{Raghu2016,
   author = {{Raghu}, M. and {Poole}, B. and {Kleinberg}, J. and {Ganguli}, S. and 
	{Sohl-Dickstein}, J.},
    title = "{On the expressive power of deep neural networks}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1606.05336},
 primaryClass = "stat.ML",
 keywords = {Statistics - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Learning},
     year = 2016,
    month = jun,
   adsurl = {http://adsabs.harvard.edu/abs/2016arXiv160605336R},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}
% recursive neural network TODO: probably not needed
@article{Scarselli2009,
author = {Scarselli, F. and Gori, M. and {Ah Chung Tsoi} and Hagenbuchner, M. and Monfardini, G.},
doi = {10.1109/TNN.2008.2005605},
issn = {1045-9227},
journal = {IEEE Transactions on Neural Networks},
month = {jan},
number = {1},
pages = {61--80},
title = {The Graph Neural Network Model},
url = {http://ieeexplore.ieee.org/document/4700287/},
volume = {20},
year = {2009}
}
% ontology learning from text
@article{Wong2012,
 author = {Wong, Wilson and Liu, Wei and Bennamoun, Mohammed},
 title = {Ontology Learning from Text: A Look Back and into the Future},
 journal = {ACM Comput. Surv.},
 issue_date = {August 2012},
 volume = {44},
 number = {4},
 month = sep,
 year = {2012},
 issn = {0360-0300},
 pages = {20:1--20:36},
 articleno = {20},
 numpages = {36},
 url = {http://doi.acm.org/10.1145/2333112.2333115},
 doi = {10.1145/2333112.2333115},
 acmid = {2333115},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Ontology learning, application of ontologies, concept discovery, semantic relation acquisition, term recognition},
}
% intro to ontology learning
@incollection{Cimiano2009,
  abstract = {Ontology learning techniques serve the purpose of supporting an ontology
engineer in the task of creating and maintaining an ontology. In this chapter, we present a comprehensive and concise introduction to the field of ontology learning.
We present a generic architecture for ontology learning systems and discuss its main components. In addition, we introduce the main problems and challenges addressed in the field and give an overview of the most important methods applied. We conclude with a brief discussion of advanced issues which pose interesting challenges to the state-of-the-art.},
  added-at = {2011-01-17T10:16:48.000+0100},
  author = {Cimiano, P. and Mädche, A. and Staab, S. and Völker, J.},
  biburl = {https://www.bibsonomy.org/bibtex/2f9f8bb0af1a8a514c270f83237313ac7/hotho},
  booktitle = {Handbook on Ontologies},
  description = {DOI 10.1007/978-3-540-92673-3},
  edition = {2nd revised edition},
  editor = {Staab, S. and Studer, R.},
  interhash = {5387f28040285a086ab706bc33e7d7af},
  intrahash = {f9f8bb0af1a8a514c270f83237313ac7},
  keywords = {learning ontology},
  pages = {245--267},
  publisher = {Springer},
  series = {International Handbooks on Information Systems},
  timestamp = {2011-01-17T10:16:49.000+0100},
  title = {Ontology Learning},
  url = {http://www.uni-koblenz.de/~staab/Research/Publications/2009/handbookEdition2/ontology-learning-handbook2.pdf},
  year = 2009
}
% ontology learning related work
@inproceedings{dAmato16,
  added-at = {2016-06-06T00:00:00.000+0200},
  author = {d'Amato, Claudia and Staab, Steffen and Tettamanzi, Andrea G. B. and Minh, Tran Duc and Gandon, Fabien L.},
  biburl = {https://www.bibsonomy.org/bibtex/2383bd3c8b3d0a42649eaa145d4ada382/dblp},
  booktitle = {SAC},
  editor = {Ossowski, Sascha},
  ee = {http://doi.acm.org/10.1145/2851613.2851842},
  interhash = {107b5a597194a2814ca5fa1916089260},
  intrahash = {383bd3c8b3d0a42649eaa145d4ada382},
  isbn = {978-1-4503-3739-7},
  keywords = {dblp},
  pages = {333-338},
  publisher = {ACM},
  timestamp = {2016-06-07T11:38:58.000+0200},
  title = {Ontology enrichment by discovering multi-relational association rules from ontological knowledge bases.},
  url = {http://dblp.uni-trier.de/db/conf/sac/sac2016.html\#dAmatoSTMG16},
  year = 2016
}
% ontology learning and population from text
@book{Cimiano2006,
 author = {Cimiano, Philipp},
 title = {Ontology Learning and Population from Text: Algorithms, Evaluation and Applications},
 year = {2006},
 isbn = {0387306323},
 publisher = {Springer-Verlag New York, Inc.},
 address = {Secaucus, NJ, USA},
} 
% ontology learning for the semantic web
@article{Maedche2001,
 author = {Maedche, Alexander and Staab, Steffen},
 title = {Ontology Learning for the Semantic Web},
 journal = {IEEE Intelligent Systems},
 issue_date = {March 2001},
 volume = {16},
 number = {2},
 month = mar,
 year = {2001},
 issn = {1541-1672},
 pages = {72--79},
 numpages = {8},
 url = {http://dx.doi.org/10.1109/5254.920602},
 doi = {10.1109/5254.920602},
 acmid = {630627},
 publisher = {IEEE Educational Activities Department},
 address = {Piscataway, NJ, USA},
}
% taxonomy learning - classification based approach
@Inproceedings{Pekar2002,
	booktitle = {Proceedings of the 19th Conference on Computational Linguistics, COLING-2002, August 24 - September 1, 2002 , Taipei, Taiwan, 2002},
	title = {Taxonomy Learning - Factoring the structure of a taxonomy into a semantic classification decision},
	year = {2002},
	type = {Inproceedings},
	author = {Viktor Pekar and Steffen Staab},
}
% rule learning ontology
@phdthesis{Galarraga2016,
author = {Gal{\'{a}}rraga, Luis},
school = {Telecom ParisTech},
title = {{Rule Mining in Knowledge Bases}},
year = {2016}
}
%  
@article{Cimiano2005,
abstract = {We present a novel approach to the automatic acquisition of taxonomies or concept hierarchies from a text corpus. The approach is based on Formal Concept Analysis (FCA), a method mainly used for the analysis of data, i.e. for investigating and processing explicitly given information. We follow Harris' distributional hypothesis and model the context of a certain term as a vector repre-senting syntactic dependencies which are automatically acquired from the text corpus with a lin-guistic parser. On the basis of this context information, FCA produces a lattice that we convert into a special kind of partial order constituting a concept hierarchy. The approach is evaluated by com-paring the resulting concept hierarchies with hand-crafted taxonomies for two domains: tourism and finance. We also directly compare our approach with hierarchical agglomerative clustering as well as with Bi-Section-KMeans as an instance of a divisive clustering algorithm. Furthermore, we investigate the impact of using different measures weighting the contribution of each attribute as well as of applying a particular smoothing technique to cope with data sparseness.},
archivePrefix = {arXiv},
arxivId = {1109.2140},
author = {Cimiano, Philipp and Hotho, Andreas and Staab, Steffen},
doi = {10.1.1.60.228},
eprint = {1109.2140},
issn = {10769757},
journal = {Journal of Artificial Intelligence Research},
pages = {305--339},
title = {{Learning Concept Hierarchies from Text Corpora using Formal Concept Analysis}},
volume = {24},
year = {2005}
}

% neural network for ontology learning
@article{Petrucci16,
  author    = {Giulio Petrucci and
               Chiara Ghidini and
               Marco Rospocher},
  title     = {Using Recurrent Neural Network for Learning Expressive Ontologies},
  journal   = {CoRR},
  volume    = {abs/1607.04110},
  year      = {2016},
  url       = {http://arxiv.org/abs/1607.04110},
  timestamp = {Tue, 02 Aug 2016 12:59:27 +0200},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/PetrucciGR16},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}
% ontology learning survey
@article{Hazman2011,
abstract = {The problem that ontology learning deals with is the knowledge acquisition bottleneck, that is to say the difficulty to actually model the knowledge relevant to the domain of interest. Ontologies are the vehicle by which we can model and share the knowledge among various applications in a specific domain. So many research developed several ontology learning approaches and systems. In this paper, we present a survey for the different approaches in ontology learning from semi-structured and unstructured date General Terms Ontology learning approaches.},
author = {Hazman, Maryam and El-Beltagy, Samhaa R and Rafea, Ahmed},
journal = {International Journal of Computer Applications},
keywords = {Ontology learning,Ontology learning evaluation,knowledge discovery},
number = {9},
pages = {975--8887},
title = {{A Survey of Ontology Learning Approaches}},
volume = {22},
year = {2011}
}
% ontology mapping (older paper), useful as reference for problem statement
@inproceedings{Doan2002,
 author = {Doan, AnHai and Madhavan, Jayant and Domingos, Pedro and Halevy, Alon},
 title = {Learning to Map Between Ontologies on the Semantic Web},
 booktitle = {Proceedings of the 11th International Conference on World Wide Web},
 series = {WWW '02},
 year = {2002},
 isbn = {1-58113-449-5},
 location = {Honolulu, Hawaii, USA},
 pages = {662--673},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/511446.511532},
 doi = {10.1145/511446.511532},
 acmid = {511532},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {machine learning, ontology mapping, relaxation labeling, semantic web},
}

% hierarchy learning via word embeddings
@article{Fu2014,
abstract = {Semantic hierarchy construction aims to build structures of concepts linked by hypernym–hyponym (“is-a”) relations. A major challenge for this task is the automatic discovery of such relations. This paper proposes a novel and effec- tive method for the construction of se- mantic hierarchies based on word em- beddings, which can be used to mea- sure the semantic relationship between words. We identify whether a candidate word pair has hypernym–hyponym rela- tion by using the word-embedding-based semantic projections between words and their hypernyms. Our result, an F-score of 73.74{\%}, outperforms the state-of-the- art methods on a manually labeled test dataset. Moreover, combining our method with a previous manually-built hierarchy extension method can further improve F- score to 80.29{\%}. 1},
author = {Fu, Ruiji and Guo, Jiang and Qin, Bing and Che, Wanxiang and Wang, Haifeng and Liu, Ting},
isbn = {9781937284725},
journal = {Acl},
pages = {1199--1209},
pmid = {1626155},
title = {{Learning Semantic Hierarchies via Word Embeddings}},
year = {2014}
}
% wikidata mapping to rdf
@inproceedings{Erxleben2014,
abstract = {Wikidata is the central data management platform of Wikipedia. By the efforts of thousands of volunteers, the project has produced a large, open knowledge base with many interesting applications. The data is highly interlinked and connected to many other datasets, but it is also very rich, complex, and not available in RDF. To address this issue, we introduce new RDF exports that connect Wikidata to the Linked Data Web. We explain the data model of Wikidata and discuss its encoding in RDF. Moreover, we introduce several partial exports that provide more selective or simplified views on the data. This includes a class hierarchy and several other types of ontological axioms that we extract from the site. All datasets we discuss here are freely available online and updated regularly.},
author = {Erxleben, Fredo and G{\"{u}}nther, Michael and Kr{\"{o}}tzsch, Markus and Mendez, Julian and Vrande{\v{c}}i{\'{c}}, Denny},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-11964-9},
isbn = {9783319119632},
issn = {16113349},
pages = {50--65},
title = {{Introducing wikidata to the linked data web}},
volume = {8796},
year = {2014}
}

% similarity (information theory)
@inproceedings{Lin1998,
 author = {Lin, Dekang},
 title = {An Information-Theoretic Definition of Similarity},
 booktitle = {Proceedings of the Fifteenth International Conference on Machine Learning},
 series = {ICML '98},
 year = {1998},
 isbn = {1-55860-556-8},
 pages = {296--304},
 numpages = {9},
 url = {http://dl.acm.org/citation.cfm?id=645527.657297},
 acmid = {657297},
 publisher = {Morgan Kaufmann Publishers Inc.},
 address = {San Francisco, CA, USA},
}
% vector similarity
@phdthesis{Weber2000,
abstract = {For a large variety of multimedia applications, similarity search provides an effective tool to find relevant information. A common implementation of similarity search transforms the objects into high-dimensional feature vectors. Searching for the most similar object to a given reference object then corresponds to finding the nearest neighbor (NN) in the feature space. Although a large number of algorithms provide efficient solutions for the NN-search problem in low-dimensional spaces, none of the proposed methods provides satisfactory response times for high-dimensional spaces as required for similarity retrieval in multimedia collections. A first contribution of this work is to formally show that all hierarchical organization finally degenerate to traversing the entire structure to find the nearest neighbor. We further show practically that a brute-force scan through the vector data outperforms such organizations already for low feature space dimensionalities in the order of 5. For NN-searches in high-dimensional spaces, we typically observe that a linear method performs faster than hierarchical approaches by more than a factor of 10. As a second contribution, we propose a new indexing method optimized for high-dimensional spaces. The so-called vector approximation file (VA-File) neither clusters data points nor maintains them in a hierarchical structure. Rather, its approach is to approximate the vectors with a quantization scheme and to use mainly these small approximations for the search. Experiments confirm the superiority over hierarchical structures for highdimensional data (i.e. more than 10 dimensions). Furthermore, we provide in-depth analysis of the method and investigate the influence of various parameters on the search efficiency. However, the method is still linear in the number of vectors and in the number of dimensions. As a consequence, it may not allow interactive-time retrieval for large collections if the dimensionality of the feature space is very large ({\textgreater} 100). Therefore, our third contribution is to extend the VA-File method towards faster evaluation schemes. The first approach trades result quality for response time, i.e. we allow for a relatively small error in the result in trade of reduced execution costs. The second approach deploys parallel resources to speedup query evaluation. We describe an evaluation scheme for a parallel version of the VA-File on a cluster of workstations. A further and important contribution of this work relates to quality aspects of NN-search. Recently, it was argued that high-dimensionality prohibits meaningful applications of NN-search. Our investigations, however, revealed that NN-search indeed makes sense, both from a theoretical point of view, and a practical point of view.},
author = {Weber, Roger},
pages = {210},
school = {SWISS FEDERAL INSTITUTE OF TECHNOLOGY ZURICH},
title = {{Similarity Search in High-Dimensional Vector Spaces}},
year = {2000}
}
% distance measures
@Inbook{Houle2010,
author="Houle, Michael E.
and Kriegel, Hans-Peter
and Kr{\"o}ger, Peer
and Schubert, Erich
and Zimek, Arthur",
editor="Gertz, Michael
and Lud{\"a}scher, Bertram",
title="Can Shared-Neighbor Distances Defeat the Curse of Dimensionality?",
bookTitle="Scientific and Statistical Database Management: 22nd International Conference, SSDBM 2010, Heidelberg, Germany, June 30--July 2, 2010. Proceedings",
year="2010",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="482--500",
isbn="978-3-642-13818-8",
doi="10.1007/978-3-642-13818-8_34",
url="http://dx.doi.org/10.1007/978-3-642-13818-8_34"
}
% nearest neighbor in high dimensions, curse of dimensionality
@article{Aggarwal2001,
abstract = {In recent years, the effect of the curse of high dimensionality has been studied in great detail on several problems such as clustering, nearest neighbor search, and indexing. In high dimensional space the data becomes sparse, and traditional indexing and algorithmic techniques fail from a effciency and/or effectiveness perspective. Recent research results show that in high dimensional space, the concept of proximity, distance or nearest neighbor may not even be qualitatively meaningful. In this paper, we view the dimensionality curse from the point of view of the distance metrics which are used to measure the similarity between objects. We specifically examine the behavior of the commonly used L k norm and show that the problem of meaningfulness in high dimensionality is sensitive to the value of k. For example, this means that the Manhattan distance metric L(1 norm) is consistently more preferable than the Euclidean distance metric L(2 norm) for high dimensional data mining applications. Using the intuition derived from our analysis, we introduce and examine a natural extension of the L k norm to fractional distance metrics. We show that the fractional distance metric provides more meaningful results both from the theoretical and empirical perspective. The results show that fractional distance metrics can significantly improve the effectiveness of standard clustering algorithms such as the k-means algorithm.},
archivePrefix = {arXiv},
arxivId = {0812.0624},
author = {Aggarwal, Charu C. and Hinneburg, Alexander and Keim, Daniel A.},
doi = {10.1007/3-540-44503-X_27},
eprint = {0812.0624},
isbn = {978-3-540-41456-8},
issn = {0956-7925},
journal = {Database Theory – ICDT 2001},
mendeley-groups = {Bachelor thesis},
pages = {420--434},
pmid = {25246403},
publisher = {Springer Berlin Heidelberg},
title = {{On the surprising behavior of distance metrics in high dimensional space}},
url = {http://link.springer.com/10.1007/3-540-44503-X{\_}27},
year = {2001}
}
% ML in general
@article{Domingos2012,
 author = {Domingos, Pedro},
 title = {A Few Useful Things to Know About Machine Learning},
 journal = {Commun. ACM},
 issue_date = {October 2012},
 volume = {55},
 number = {10},
 month = oct,
 year = {2012},
 issn = {0001-0782},
 pages = {78--87},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/2347736.2347755},
 doi = {10.1145/2347736.2347755},
 acmid = {2347755},
 publisher = {ACM},
 address = {New York, NY, USA},
} 
% ontology mapping, similarity
@article{Rodriguez2003,
 author = {Rodr\'{\i}guez, M. Andrea and Egenhofer, Max J.},
 title = {Determining Semantic Similarity Among Entity Classes from Different Ontologies},
 journal = {IEEE Trans. on Knowl. and Data Eng.},
 issue_date = {February 2003},
 volume = {15},
 number = {2},
 month = feb,
 year = {2003},
 issn = {1041-4347},
 pages = {442--456},
 numpages = {15},
 url = {http://dx.doi.org/10.1109/TKDE.2003.1185844},
 doi = {10.1109/TKDE.2003.1185844},
 acmid = {642948},
 publisher = {IEEE Educational Activities Department},
 address = {Piscataway, NJ, USA},
 keywords = {Semantic similarity, ontology integration, information integration, semantic interoperability, semantic matching.},
} 
% multi-label kNN
@inproceedings{Zhang2005,
    author = {Zhang, Min-Ling and Zhou, Zhi-Hua},
    citeulike-article-id = {1164920},
    citeulike-linkout-0 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=1547385},
    journal = {IEEE International Conference on Granular Computing},
    organization = {The IEEE Computational Intelligence Society},
    pages = {718--721 Vol. 2},
    posted-at = {2011-03-21 06:26:51},
    priority = {2},
    title = {{A k-Nearest Neighbor Based Algorithm for Multi-label Classification}},
    url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=1547385},
    volume = {2},
    year = {2005}
}
% similarity-based classification
@article{Chen2009,
 author = {Chen, Yihua and Garcia, Eric K. and Gupta, Maya R. and Rahimi, Ali and Cazzanti, Luca},
 title = {Similarity-based Classification: Concepts and Algorithms},
 journal = {J. Mach. Learn. Res.},
 issue_date = {12/1/2009},
 volume = {10},
 month = jun,
 year = {2009},
 issn = {1532-4435},
 pages = {747--776},
 numpages = {30},
 url = {http://dl.acm.org/citation.cfm?id=1577069.1577096},
 acmid = {1577096},
 publisher = {JMLR.org},
} 
% evaluation
@inproceedings{Dellschaft2006,
 author = {Dellschaft, Klaas and Staab, Steffen},
 title = {On How to Perform a Gold Standard Based Evaluation of Ontology Learning},
 booktitle = {Proceedings of the 5th International Conference on The Semantic Web},
 series = {ISWC'06},
 year = {2006},
 isbn = {3-540-49029-9, 978-3-540-49029-6},
 location = {Athens, GA},
 pages = {228--241},
 numpages = {14},
 url = {http://dx.doi.org/10.1007/11926078_17},
 doi = {10.1007/11926078_17},
 acmid = {2127063},
 publisher = {Springer-Verlag},
 address = {Berlin, Heidelberg},
}
% evaluation of hierarchical classification
@article{Kosmopoulos2014,
  author    = {Aris Kosmopoulos and
               Ioannis Partalas and
               {\'E}ric Gaussier and
               Georgios Paliouras and
               Ion Androutsopoulos},
  title     = {Evaluation Measures for Hierarchical Classification: a unified
               view and novel approaches},
  journal   = {Data Mining and Knowledge Discovery, Springer (accepted for publication)},
  year      = {2014},
  url        = {http://arxiv.org/abs/1306.6802},
month={june}
}
% about n-grams
@incollection{Jurafsky2014,
author = {Jurafsky, Daniel and Martin, James H.},
booktitle = {Speech and Language Processing},
chapter = {N-Grams},
title = {{N-Grams}},
year = {2014}
}
% about completeness in rdf data (e.g. wikidata)
@Inbook{Darari2016,
author="Darari, Fariz
and Razniewski, Simon
and Prasojo, Radityo Eko
and Nutt, Werner",
editor="Bozzon, Alessandro
and Cudre-Maroux, Philippe
and Pautasso, Cesare",
title="Enabling Fine-Grained RDF Data Completeness Assessment",
bookTitle="Web Engineering: 16th International Conference, ICWE 2016, Lugano, Switzerland, June 6-9, 2016. Proceedings",
year="2016",
publisher="Springer International Publishing",
address="Cham",
pages="170--187",
isbn="978-3-319-38791-8",
doi="10.1007/978-3-319-38791-8_10",
url="http://dx.doi.org/10.1007/978-3-319-38791-8_10"
}
% recurrent neural embeddings
@article{Ororbia2017,
  author    = {Alexander G. Ororbia II and
               Tomas Mikolov and
               David Reitter},
  title     = {Learning Simpler Language Models with the Delta Recurrent Neural Network
               Framework},
  journal   = {CoRR},
  volume    = {abs/1703.08864},
  year      = {2017},
  url       = {http://arxiv.org/abs/1703.08864},
  timestamp = {Mon, 03 Apr 2017 12:41:34 +0200},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/OrorbiaMR17},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}
@inproceedings{Mikolov2010,
  added-at = {2015-04-09T00:00:00.000+0200},
  author = {Mikolov, Tomas and Karafiát, Martin and Burget, Lukás and Cernocký, Jan and Khudanpur, Sanjeev},
  biburl = {https://www.bibsonomy.org/bibtex/2aee1e280d06e82474b17c4996aaea076/dblp},
  booktitle = {INTERSPEECH},
  editor = {Kobayashi, Takao and Hirose, Keikichi and Nakamura, Satoshi},
  ee = {http://www.isca-speech.org/archive/interspeech_2010/i10_1045.html},
  interhash = {9cabdcef2ed097906bff2cb7a327e61e},
  intrahash = {aee1e280d06e82474b17c4996aaea076},
  keywords = {dblp},
  pages = {1045-1048},
  publisher = {ISCA},
  timestamp = {2015-06-21T06:10:05.000+0200},
  title = {Recurrent neural network based language model.},
  url = {http://dblp.uni-trier.de/db/conf/interspeech/interspeech2010.html#MikolovKBCK10},
  year = 2010
}


% wikidata subclass of documentation
@misc{WikidataP279,
title={Property talk: P279},
howpublished={https://www.wikidata.org/wiki/Property\_talk:P279},
note={Accessed: 2017-01-29}
}
% wikidata entity talk
@misc{WikidataQ35120,
title={Talk: Q35120},
howpublished={https://www.wikidata.org/wiki/Talk:Q35120},
note={Accessed: 2017-01-29}
}
% wikidata project chat about subclass tree
@misc{WikidataSubclassTree,
title={Wikidata project chat: Top of the subclass tree},
howpublished={https://www.wikidata.org/w/index.php?title=Wikidata:Project_chat\&oldid=91040961\#Top\_of\_the\_subclass\_tree},
note={Accessed: 2017-01-30}
}
% wikidata new item
@misc{WikidataNewItem,
title={Wikidata: Create a new item},
howpublished={https://www.wikidata.org/wiki/Special:NewItem},
note={Accessed: 2017-02-05}
}
@misc{WikidataGame,
title={Wikidata Game},
howpublished={https://tools.wmflabs.org/wikidata-game/},
note={Accessed: 2017-02-12},
author={Manske, Magnus},
year={2014}
}
% Taxonomy browser
@phdthesis{Stratan2016,
author = {Stratan, Serghei},
number = {March},
school = {Technische Universit{\"{a}}t Dresden},
title = {Software Implementation for Taxonomy Browsing and Ontology Evaluation for the case of Wikidata},
type = {Master thesis},
year = {2016}
}
% wikidata dump of 2016-11-07
@misc{WikidataDump,
title={Wikidata entity dump},
year={2016},
month={11},
day={7},
howpublished={https://dumps.wikimedia.org/wikidatawiki/entities/},
note={Accessed: 2016-11-20}
}
% gensim
@inproceedings{Rehurek2010,
      title = {{Software Framework for Topic Modelling with Large Corpora}},
      author = {Radim {\v R}eh{\r u}{\v r}ek and Petr Sojka},
      booktitle = {{Proceedings of the LREC 2010 Workshop on New
           Challenges for NLP Frameworks}},
      pages = {45--50},
      year = 2010,
      month = May,
      day = 22,
      publisher = {ELRA},
      address = {Valletta, Malta},
      note={\url{http://is.muni.cz/publication/884893/en}},
      language={English}
}
% scipy
@Misc{Jones2001,
  author =    {Eric Jones and Travis Oliphant and Pearu Peterson and others},
  title =     {{SciPy}: Open source scientific tools for {Python}},
  year =      {2001--},
  url = "http://www.scipy.org/",
  note = {[Online; accessed 2017-03-16]}
}
@article{scikit-learn,
 title={Scikit-learn: Machine Learning in {P}ython},
 author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
         and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
         and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
         Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
 journal={Journal of Machine Learning Research},
 volume={12},
 pages={2825--2830},
 year={2011}
}
% wikidata gene bot
@article {Putman2017,
	author = {Putman, Tim E and Lelong, Sebastien and Burgstaller-Muelhbacher, Sebastian and Waagmeester, Andra and Diesh, Colin and Dunn, Nathan and Munoz-Torres, Monica and Stupp, Gregory and Su, Andrew and Good, Benjamin M},
	title = {WikiGenomes: an open Web application for community consumption and curation of gene annotation data in Wikidata.},
	year = {2017},
	doi = {10.1101/102046},
	publisher = {Cold Spring Harbor Labs Journals},
	abstract = {With the advancement of genome sequencing technologies new genomes are being sequenced daily. While these sequences are deposited in publicly available data warehouses, their functional and genomic annotations mostly reside in the text of primary publications. Biocurators are hard at work extracting those annotations from the literature for the most studied organisms and depositing them in structured databases. However, the resources don{\textquoteright}t exist to fund the comprehensive curation of the thousands of newly sequenced organisms in this manner. Here we describe WikiGenomes (wikigenomes.org), a web application that facilitates the consumption and curation of genomic data by the entire scientific community. WikiGenomes is based on Wikidata, an openly editable knowledge graph with the goal of aggregating published knowledge into a free and open database. WikiGenomes empowers community curation of genomic and biomedical knowledge through a domain-specific application built on top of Wikidata, bringing that curated knowledge to the public domain.},
	URL = {http://biorxiv.org/content/early/2017/01/24/102046},
	eprint = {http://biorxiv.org/content/early/2017/01/24/102046.full.pdf},
	journal = {bioRxiv}
}

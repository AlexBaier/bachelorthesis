The notion of neural networks is introduced with the example for feedforward networks with backpropagation.
The task of representing semantic information encoded in text and graphs as feature vectors using neural embeddings is then discussed.
Solutions for generating word and graph embeddings by \fullcite{Mikolov2013}, \fullcite{Ristoski2016}, \fullcite{Cao2016} are presented.

\subsubsection{Introduction to neural networks}
A neural network is a triple $(N, C, W)$, where $N$ are neurons, $C = \{ (i, j) | i, j \in N \}$ is a set of connections between neurons, and $W$ are the weights corresponding
to the connections, where $W_{ij}$ is the weight for the connection $(i, j) \in C$ \cite{Kriesel2005}. 
Each neuron has an activation function $f_\mathit{act}$,
which commonly is the Fermi function
\begin{align}
& f_\mathit{act} = \frac{1}{1 + e^{-x}}
\end{align}
mapping values to the interval of $(0,1)$. This function is non-linear and therefore allows the neural network to solve non-linear problems.
Input neurons typically use the identity function $\mathit{id}$ as activation function \cite{Kriesel2005}. 
The output of a neuron is computed by summing up the products of weights and inputs
of the neuron, which is called net input, and applying the activation function on this value:
\begin{align}
&o_i = f_{act}(\sum_{k=1}^{n} w_{ki} o_i) 
\end{align} 
The input for input neurons (neurons on the first layer) are the inputs of the network.

A feedforward neural network consists of an input layer with $s_i$ input neurons, an output layer with $s_o$ output neurons and $d$ hidden layers with
$h$ neurons per layer. A neural network with many hidden layers is called deep neural network.
The connections in a feedforward network only connect the neurons of a layer $i$ with the next layer $i+1$.
This means that a neuron can never influence itself. Networks, which allow this, are called recurrent neural networks.
See in Figure~\ref{fig:nn example} a feedforward neural network with $s_i =2, s_o=1, d=1, h=2$
with already initialized weights.

\begin{figure}
\centering
\includegraphics[width=0.75\textwidth]{images/neural_networks/nn_example.pdf}
\caption{Example for feedforward neural network}
\label{fig:nn example}
\end{figure}

Forward propagation pushes an input through all layers of the network in a step-wise manner (layer per layer).
In the first step, the input vector $\vec{x}$ is pushed to the hidden layer and outputs of the hidden layer are computed, which results in a vector 
$\vec{h}$ in the hidden layer:
\begin{align} \label{align:fp step1}
&\vec{h} =
f_\mathit{act}(
\begin{bmatrix}
w_{1,3} & w_{2,3} \\
w_{1,4} & w_{2,4} \\
\end{bmatrix}
\vec{x})
\end{align}
In the second step, the output of the hidden layer is pushed to the output layer resulting in the output $y^*$:
\begin{align}
&y^* =
f_\mathit{act}(
\begin{bmatrix}
w_{3,5} & w_{4,5}\\
\end{bmatrix}
\vec{h})
\end{align}
Assuming the expected output for an input $\vec{x}$ is $y$, the error function is defined as
\begin{align}
E = \frac{1}{2}(y - y^*)^2
\end{align}
Training objective is to minimize the error function $E$.
This can be accomplished by adjusting the weights $w_{ij} \in W$.
Rather than brute-forcing the optimal weights for a network, backpropagation using gradient descent is applied in training.
The idea of gradient descent is to look at the slope of the error function in respect to the current weights and adjust the weights into the descending direction.
To do so the partial gradient in respect to each weight $w_ij$ has to be derived from the error function \cite{Nielsen2017}:
\begin{align}
&\frac{\partial E}{\partial w_{ij}} = \delta_j o_i \: \textnormal{with} \\
& \delta_j = \frac{\partial E}{\partial o_j} \frac{\partial o_j}{\partial \mathit{net}_j}
\end{align}
Subsequently, the weights can be updated by adding $\Delta w_{ij} = - \alpha \frac{\partial E}{\partial w_{ij}}$ to each corresponding weight \cite{Nielsen2017}.
Repeating the backpropagation for a set of training samples will train the neural network to approximate a specific function.

In the following subsections, the concept of embeddings will be used to generate word and graph vector representations.
By learning a certain task like predicting the probability for a certain word to appear in a context, a neural network implicitly learns
hidden embeddings for the inputs. Embeddings are, in the case of Word2Vec \cite{Mikolov2013}, the connection weights between the input and hidden layer.
More specifically, if the weights are represented as a matrix (e.g. see Equation~\ref{align:fp step1}), each row is an embedding for the corresponding input neuron.

\subsubsection{Word embeddings}\label{section:word2vec}
\fullcite{Mikolov2013} introduce two neural network language models, Continuous Bag-of-Words
and Skip-gram, which have proven to be very effective at creating word embeddings.
The generated word embeddings of both models encode the semantics and linguistic regularities of
words.
Words, which are semantically close, are also close in the word embedding vector space.
Calculating the offset between words makes it possible to answer more complex semantic questions.
It is possible to answer more complex questions than similarity about the relationship between words.
For example, the question ''What is the word that is similar to \textit{small}
 in the same sense as \textit{biggest} is similar to \textit{big}?''
can be answered by calculating the offset between \textit{big} and \textit{biggest} and adding the vector
of \textit{small} to it: $\mathit{vector}(\textnormal{''biggest''})-\mathit{vector}(\textnormal{''big''})
+\mathit{vector}(\textnormal{''small''})$ \cite{Mikolov2013}.

Further research in this topic has shown that SGNS
is generally more effective
in generating high-quality word embeddings than CBOW \cite{Mikolov2013} \cite{Mikolov2013a} \cite{Levy2015}, 
as it is better in representing rare words.
Therefore only the skip-gram model with negative sampling will be introduced in this thesis.
The following description is based on work by \fullcite{Mikolov2013} \cite{Mikolov2013a},
\fullcite{Levy2014} \cite{Levy2014a} \cite{Goldberg2014}, \fullcite{Rong2014}, and \fullcite{Levy2015}.

\begin{figure}
\centering
\includegraphics[width=0.5\textwidth]{images/neural_networks/skipgram.pdf}
\caption{Skip-gram model}
\label{fig:skipgram}
\end{figure}

The skip-gram model with negative sampling uses a word corpus $w \in V_W$ and the corresponding context
corpus $c \in W_C$. $V_W$ and $V_C$ are vocabularies.
Given a sequence of words $w_1, \dots, w_n$, e.g. a text corpus like a Wikipedia text dump,
the context to a word $w_i$ are the $2*L$ words around it:
$w_{i-L}, \dots, w_{i-1}, w_{i+1}, \dots, w_{i+L}$. $L \in \mathbb{N}$ is the size of the context window.
The network (see Figure~\ref{fig:skipgram}) is trained for the task of predicting the context
of an input word. Each word $w \in V_W$ is mapped to an embedding vector $\vec{W} \in \mathbb{R}^d$,
and each context $c \in V_C$ is mapped to an embedding vector $\vec{c} \in \mathbb{R}^d$,
where $d$ is the embedding size. Both embeddings are parameters, which are learned by the network.
The embeddings can be represented as matrices $W \in \mathbb{R}^{\abs{V_W} \times d}$ and 
$C \in \mathbb{R}^{\abs{V_C} \times d}$, where $W$ maps the word input to the projection layer
and $C$ maps the projection to the output layer, and therefore the context of the entered word
\cite{Rong2014}.

Given a word $w$ and a context $c$ the model wants to maximize the probability $p(D=1 | w, c)$,
that $(w, c)$ is in the data $D$. The probability distribution is modeled as \cite{Levy2014}:
\begin{align}
&p(D=1 | w, c) = \frac{1}{1 + e^{- \vec{w} \cdot \vec{c}}}
\end{align},
which leads to the maximization objective \cite{Levy2014a}:
\begin{align}\label{align:sg objective}
&\max_{\vec{w}, \vec{c}} \sum_{(w, c) \in D} \log \frac{1}{1 + e^{- \vec{w} \cdot \vec{c}}}
\end{align}
This problem has a trivial solution with $\vec{c} = \vec{w}$ and $\vec{c} \cdot \vec{w} = K$, for a large enough $K$
\cite{Levy2014a} \cite{Levy2014}.
 Using negative sampling solves the problem of having a trivial solution and also benefits
the quality of word embeddings, as it increases the distance between word-context pairs, which do not occur in the data.
Negative sampling is represented with the probability $p(D=0|w,c) = 1 - p(D=1 | w,c)$, that a pair $(w,c)$
does not occur in the data $D$. The negative sampling training objective can be written as \cite{Levy2014a}:
\begin{align}
&\max_{\vec{w}, \vec{c}} (\sum_{(w, c) \in D} \log \sigma(\vec{c} \cdot \vec{w}) + \sum_{(w, c) \in D'} \log \sigma
(- \vec{c} \cdot \vec{w}))
\end{align},
where $D'$ is a set of negative training samples, which are not in $D$, and $\sigma(x) = \frac{1}{1 + e^x}$.
For this objective $p(D=1 | w, c)$ needs to produce small values for $(w, c) \in D'$ and high values for
$(w, c) \in D$, which counteracts the trivial solution possible for objective~\ref{align:sg objective}.

Additionally \fullcite{Mikolov2013} introduces parameters to further influence the
quality of word embeddings. Rare words that occur less than a certain threshold are not considered as
words or context. Additionally very frequent words are down-sampled (occur less often).
This is done before context generation and increases the effective size of context windows \cite{Levy2014},
which improves the quality of word embeddings \cite{Mikolov2013a}.

Research has shown that the effectiveness of word embeddings is highly dependent
on the choice of hyperparameters \cite{Levy2015} \cite{Mikolov2013a}.
Experiments by \fullcite{Levy2015} indicate that SGNS prefers a high number of negative samples.
Additionally subsampling of very frequent words (e.g. ''in'', ''a'') benefits the embeddings, since these
words provide less information than less frequent words \cite{Mikolov2013a}.

SGNS has a log-linear computational complexity $O$ in respect to the vocabulary size $V$
\begin{align}
& O = E \times T \times C \times (D + D \times \log_2 (V))
\end{align}
where $E$ is the number of training epochs, $T$ is the number of words in the training set, $C$ is the context window, and $D$ is the word embedding size \cite{Mikolov2013}.

\subsubsection{Graph embeddings}\label{section:graph embeddings}
Similar to generating vector embeddings for words, it is beneficial to create graph embeddings.
As it allows the extraction of useful information about vertices in its graph context \cite{Cao2016}.

Where sentences are directly representable as linear sequences and can therefore be directly
used in SGNS, this is not the case for graph structures like RDF graphs or protein networks.
\fullcite{Ristoski2016} proposes the use of graph walks to generate linear sequence samples.
Given an undirected, weighted graph $G=(V, E)$, graph walk will generate all graph walks $P_v$ of depth $d$
starting in vertex $v$, for all vertices $v \in V$. Breadth-first search is used for generating the graph walks.
This results in a set of sequences of the format $v_i \rightarrow e_{ij} \rightarrow v_j \rightarrow \dots$, where
$v_i, v_j \in V$ and $e_{ij} \in E$. The number of generated walks increases exponentially with depth $d$.
Therefore, instead of generating all graph walks for each vertice, a random walk approach as developed
by \fullcite{Perozzi2014} is used, where the number of walks per vertice is limited. A random walk $W_v$
rooted at vertice $v$ consists of random variables $W_v^1, W_v^2, \dots, W_v^k$. $W_v^{i+1}$ is a vertice,
which is chosen randomly from the neighbors of $W_v^{i}$.
Random walking allows an easier parallelization, since multiple workers can simultaneously
generate walks in different parts of the graph \cite{Perozzi2014}.
Evaluation results show that this approach outperforms standard feature generation approaches \cite{Ristoski2016}.

\fullcite{Cao2016} develop a deep neural network for graph representation (DNGR).
Instead of generating graph walks, a random surfing model similar to Google's PageRank is used
to generate a probabilistic co-occurrence matrix, which indicates the probability of reaching a vertice $j$
after a number of steps $k$ from a starting vertice $v_i$.
Similar to PageRank a teleportation probaility $\alpha$ is used,
which indicates the chance whether the random surfing continues or is reset to the starting vertice.
A row $p_k$ of the co-occurrence matrix is therefore defined as follows \cite{Cao2016}:
\begin{align}
&p_k = \alpha \cdot p_{k-1} A + (1 - \alpha)p_0
\end{align}
with $p_{0_i} = 1, p_{0_j} = 0, j \neq i$.
Based on the co-occurrence relation, a vector representation $r$ can be defined.
It can be assumed that vertices, which are close to the original vertice should have higher weight than
distant vertices. This leads to the vector representation $r$ for the starting vertice $v_i$ \cite{Cao2016}:
\begin{align}
&r = \sum_{k=1}^K w(k) \cdot p_k^*
\end{align}
with $p_k^* = p_{k-1}^* A = p_0 A^k$ being the probabilities of arriving in exactly $k$ steps, if no random restart occurs,
and $w$ being a decreasing weight function.
Finally a stacked denoising autoencoder is used to produce a non-linear mapping from the representations to low dimensional vectors.
Stacked implies that the autoencoder has multiple hidden layers (deep neural network), which allows the learning better embeddings with each layer \cite{Cao2016}.
The neural network encodes and decodes the inputted vectors, which performs a meaningful dimension reduction by removing redundant information
and noise \cite{Cao2016}.
Evaluation with comparison to other word embedding approaches like SGNS \cite{Mikolov2013}, DeepWalk \cite{Perozzi2014}, etc. show
that the combination of random surfing and deep neural networks is effective, as the approach performs better than the other baselines.
However, the approach has a linear complexity in regards to the number of vertices in the graph, while SGNS has a log-linear computational complexity \cite{Mikolov2013}.
This is problematic in the thesis' use case, since Wikidata contains over $20$ million vertices.

Random surfing has multiple advantages to sampling approaches like graph walking.
Linear sequences have finite lengths and can therefore fail to capture relevant contextual information. 
Using random surfing overcomes this problem as it is able to consider walks of every length.
Additionally, a desired property of embedding approaches is the ability to weight
context based on its distance to original word or vertice \cite{Mikolov2013} \cite{Cao2016}.
Random surfing allows, similarly to Word2Vec (see Section~\ref{section:word2vec}, the weighting
of words based on its distance, which is important to create good word representations \cite{Cao2016}.

Manually building ontologies is an expensive, tedious and error-prone process \cite{Hazman2011}.
\fullcite{Maedche2001} recognize that the manual construction of ontologies results in a \textbf{knowledge acquisition bottleneck},
which motivates the research into the field of \textbf{ontology learning (OL)}.
The field of OL supports ontology engineers in the construction and maintenance of ontologies by providing OL techniques in the form
of tools like \textit{OntoEdit} \cite{Maedche2001}. The process of OL can be divided into different subtasks. OL tools consist of
different components, which automatically or semi-automatically support this process. The field of ontology learning
exploits different research fields like natural language processing, machine learning, ontology engineering, etc. \cite{Cimiano2009}.

The process of OL and the components of a generic OL architecture are summarized and the task of the thesis is categorized.
Following, basic algorithms for learning taxonomic relations are summarized. 
Both subsections are based on work by \fullcite{Cimiano2009}, \fullcite{Maedche2001}, and \fullcite{Hazman2011}.
Finally, related work, which exploits neural networks, is analyzed and compared to the thesis' task. 
The novelty and additional benefits of this work are justified.

\subsubsection{Process and architecture for ontology learning}
The process of OLg can be divided into subtasks. \fullcite{Maedche2001} define a ontology learning cycle, consisting of the following steps:
\begin{enumerate}
\item \textbf{Import/Reuse} is the merging of existing structures and mapping between the structures and the target ontology.
\item \textbf{Extraction} defines the modeling of the target ontology by feeding from web documents.
\item \textbf{Prune} takes the generated model and adjusts the ontology to its purpose.
\item \textbf{Refine} completes the ontology at a fine granularity.
\item \textbf{Apply} applies the resulting ontology on its target application. This serves as a measure of validation.
\end{enumerate}
These 5 steps can be repeated as often as necessary to include additional domains and updating it with new content.
The thesis' task can be categorized to the \textit{Refine} step, as its goal is to improve the completeness of an existing ontology.

\fullcite{Cimiano2009} introduces a generic OL architecture and its major components. The described tool is semi-automatic, meaning
that it support an ontology engineer rather than fully automatizing the process of ontology construction and maintenance.
The architecture consists of the following components:
\begin{enumerate}
\item \textbf{Ontology management component} provides an interface between the ontology and learning algorithms.
	Learned concepts, relations and axioms should be added to the ontology using this component.
	It is also used for manipulating the ontology.
	More specifically, for the importing, browsing, modification, versioning and evolution of ontologies.
\item \textbf{Coordination component} is the user interface, which should allow the ontology engineer to choose input data, learning and resource processing methods.
\item \textbf{Resource processing component} allows the discovery, import, analysis and transformation of unstructured, semi-structured and structured input.
	For this purpose the component needs different natural language processing components to parse and analyze input on different levels, word to sentence-level.  
\item \textbf{Algorithm library component} contains the algorithms, which are applied for the purpose of ontology learning. These algorithms are generic
	standard machine learning methods, as well as specialized OL methods. Additionally, different similarity and collocation measures should be available.
\end{enumerate}
In the context of Wikidata, there exists no single comprehensive OL tool. However, in the Wikimedia technology space different tools exist,
which mainly support the task of refining and maintaining the current ontology.
For example, \fullcite{Stratan2016} develops a taxonomy browser for Wikidata, which is able to evaluate the quality of the taxonomy by detecting different types 
of cycles, redundancies, errors and unlinked classes. This tool is an ontology learning component, as it provides the ability to browse and evaluate the ontology of Wikidata.

\subsubsection{Approaches for learning taxonomic relations}\label{section:taxonomy learning}
A subgroup of algorithms for OL is concerned with learning taxonomic relations.
The following approaches, categorized by \fullcite{Cimiano2009}, use text as input.

\textbf{Lexico-syntactic patterns} are word patterns, which are used to identify hypernym-hyponym pairs (superclass-subclass pairs) in natural text.
For example, such a pattern is 
\begin{align*}
\mathit{NP}_{\mathit{hyper}} \: \textnormal{such as} \: \{  \mathit{NP}_\mathit{hypo} , \}^*  \:  \{ ( \textnormal{and} \mid  \textnormal{or} ) \} \: \mathit{NP}_\mathit{hypo}
\end{align*},
where $\mathit{NP}$ stands for noun phrase, and $\mathit{NP}_\mathit{hyper}$ is a hypernym or superclass, while $\mathit{NP}_\mathit{hypo}$ are hyponyms or subclasses.
These patterns provide reasonable results, but the manual creation of patterns is involve high cost and time investments \cite{Wong2012}.

\textbf{Clustering} uses some measure of similarity to organize objects into groups. This can be achieved by representing the words or terms as vectors \cite{Cimiano2005},
on which different distance or similarity measures can be applied. Clustering methods can be categorized to three different types.
Agglomerative clustering initializes each term as its own cluster and merges in each step the most similar terms into one cluster.
Divisive clustering approaches the problem the opposite way by starting with a single cluster, containing all words, and then dividing them into smaller groups.
Both approaches generate hierarchies. Agglomerative clustering is doing so bottom-up and divisive
clustering top-down.

\textbf{Phrase analysis} analyses noun phrases directly. It is assumed that nouns, which have additional modifiers are subclasses of the noun without modifiers.
For example, this could be applied on the labeled unlinked classes of Wikidata. For example the classes
\textit{Men's Junior European Volleyball Championship (Q169359)}
and  \textit{Women's Junior European Volleyball Championship (Q169956)}  could be subclasses of \textit{European Volleyball Championship (Q6834)}.
In this case, phrase analysis interprets \textit{Men's Junior} and \textit{Women's Junior} as modifiers, which denote these classes as specialization to \textit{Q6834}.

\textbf{Classification}-based approaches can be used, when a taxonomy is already present. In this case, classification can be used to add unclassified concepts
to the existing hierarchy. Challenging with this task is that a taxonomy typically contains a large amount of classes and therefore classification
methods like SVM are not suited for the task. Specific algorithms, which only consider, a subset of relevant classes are necessary to carry out an efficient classification.
For example, \fullcite{Pekar2002} solves this problem by exploiting the taxonomy's tree structure using tree-ascending or tree-descending algorithms.

The algorithm developed by this thesis uses a classification-based approach, since a large taxonomy is already given.
Instead of exploiting the hierarchic structure of the taxonomy, a variant of k-nearest-neighbors is used to address the "high number of labels" problem 
(mentioned in Section~\ref{section:problem statement}).

\subsubsection{Neural networks in ontology learning}\label{section:ol nn}
\fullcite{Fu2014} develop a method to construct a semantic hierarchy (taxonomy) given a hyponym (subclass) and a list of corresponding hypernyms (superclasses),
as shown in Figure~\ref{fig:concept hierarchy}. In this example, the input is the subclass \textit{computer (Q68)} and its superclasses \textit{tool (Q39546)} etc.
\fullcite{Fu2014}'s solution uses neural word embeddings, as discussed in Section~\ref{section:word2vec}, to construct a more specific and precise concept hierarchy.
For this example, the direct superclasses of \textit{computer (Q68)} are \textit{tool (Q39546)} and \textit{electrical apparatus (Q2425052)},
while the other superclasses are transitive superclasses to \textit{computer (Q68)}.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{images/ontology_learning/concept_hierarchy.pdf}
\caption{Construction of semantic hierarchy in \fullcite{Fu2014}.}
\label{fig:concept hierarchy}
\end{figure}

The approach proposed by \fullcite{Fu2014} exploits the linguistic regularities of neural word embeddings.
It is assumed that there is a subclass vector offset between the subclass and its superclass embeddings,
which behaves similar to the prominent example by \fullcite{Mikolov2013}:
$\textnormal{Queen} = \textnormal{King} - \textnormal{Man} + \textnormal{Woman}$.
Computing such subclass offsets for random subclass-superclass pairs for \citeauthor{Fu2014}'s dataset as well as the Wikidata dataset \cite{WikidataDump}
analyzed in this thesis (see Section~\ref{section:vector offset}) shows that clusters of very similar subclass offsets exist.
Analysis by \citeauthor{Fu2014} reveals that the offsets in one cluster refer to similar topic, e.g. single cluster concerned about people's occupations \cite{Fu2014}.
The subclass offsets in the Wikidata dataset reveal similar characteristic (see Section~\ref{section:vector offset}).

\fullcite{Fu2014} conclude that the subclass-of relation is too complex to be described by a translation.
Instead a linear projection is trained by minimizing the mean squared error between projection of the subclass $\vec{x} \in \mathbb{R}^d$
and the actual superclass $\vec{y} \in \mathbb{R}^d$, where $d$ is the embedding size.
The following minimization problem is thereby solved:
\begin{align}
&\Phi^* = \min_\Phi \frac{1}{N} \sum_{(\vec{x}, \vec{y})} \norm{\Phi \vec{x} - \vec{y}}^2 
\end{align}
where $N$ is the number of $(\vec{x}, \vec{y})$ subclass-superclass pairs in the training data \cite{Fu2014}.
The problem described by this minimization problem is a multivariate linear regression problem \cite{Fu2014}.

\citeauthor{Fu2014} argues that because of the varied types of subclass-of relations, a single matrix is not adequate at fitting all subclass-superclass pairs.
Therefore piecewise linear projection is implemented. 
The subclass-superclass pairs are clustered by their subclass-of offsets  and a projection for each cluster is trained.
Piecewise linear projection learns a projection matrix $\Phi_k$ for each identified cluster in the data set \cite{Fu2014}:

\begin{align}
&\Phi^* = \min_{\Phi_k} \frac{1}{N_k} \sum_{(\vec{x}, \vec{y})\in C_k} \norm{\Phi_k \vec{x} - \vec{y}}^2 
\end{align}
where $N_k$ is the amount of word pairs in the $k^\mathit{th}$ cluster $C_k$ \cite{Fu2014}.
The number of clusters is fine-tuned based on the data set \cite{Fu2014}.

To determine whether a given pair of concepts $\vec{x}, \vec{y} \in \mathbb{R}^d$ have a subclass-superclass relation, 
the cluster $k$, which has the closest cluster center to the subclass offset $\vec{y} - \vec{x}$, is chosen and the projection $\Phi_k \vec{x}$ computed \cite{Fu2014}.
The pair $(\vec{x}, \vec{y})$ is a subclass-superclass pair, if either the distance between the projection $\Phi_k \vec{x}$ and $\vec{y}$ falls below a certain threshold $\delta$
or if there exist another concept $\vec{z}$, which is superclass of $\vec{x}$ and subclass of $\vec{y}$, and therefore allows the use of the transitivity of the subclass-of relation
\cite{Fu2014}. This approach allows one subclass to have multiple superclasses and is also able to identify the case, in which no valid subclass-of relation exists.

\citeauthor{Fu2014} evaluated the piecewise linear projection on a test set of $418$ subclasses and their superclasses.
The embeddings are trained on a 30 million Chinese sentence corpus and the projection is trained
on  $15247$ hypernym-hyponym word pairs. The approach achieves an F-score of $73.74\%$ on a manually labeled dataset,
and outperforms existing state-of-the-art methods.

As argued in Chapter~\ref{section:neural networks}, the use of neural word embeddings seems promising, which is supported by the good results of \citeauthor{Fu2014}'s work.
A similar approach, which is presented in Chapter~\ref{section:algorithm}, can be applied in the given task.
The main difference in the problems is the expected input, which is one of the reasons why \fullcite{Fu2014}'s approach works so well.
\citeauthor{Fu2014} assumes that a subclass and its superclasses are given as input.
This is advantageous for multiple reasons.
Only a small set of classes has to be considered for the classification decision, which are all related to subclass and are therefore related to similar topics.
Additionally, the input already provides a set of correct subclass-of relations.
In the thesis' case, the only given input is the subclass. 
Therefore every class in the taxonomy has to be assumed as a possible superclass rather than a small subset.
The problem statement also assumes that for each class a superclass has to be found.
Consequently the identification of subclass-of relations by \citeauthor{Fu2014} is not applicable anymore and another approach has to be implemented.
Although piecewise linear projection is still a promising approach to use neural word embeddings for the subclass classification,
the difficulty of the given problem implies worse results than in the presented work by \fullcite{Fu2014}.



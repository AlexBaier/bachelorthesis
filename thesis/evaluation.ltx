\subsection{Method}
The hybrid algorithms are evaluated using a gold standard.
The gold standard is fetched from classes in the \dumpdate{} Wikidata dump \cite{WikidataDump}.
$1,283,128$ classes with superclasses, where retrieved from the dump.
As test data $200,000$ classes were randomly chosen. 
The remaining $1,083,128$ classes are used as training data.
The retrieved subclass-superclass relations should be sufficient as gold standard,
because they are curated by experts of the Wikidata community.

Precision, recall and F1-score cannot effectively be used to evaluate classification problems with high amounts of classes in a taxonomy \cite{Kosmopoulos2014}.
These measures are typically used for flat classification problems with small amounts of classes, which are not related.
It has to be recognized that a misclassification in the thesis' use case is not a binary problem.
Assuming a pair of prediction and gold standard, the predicted class could be very similar or very different to the gold standard.
The evaluation should consider the similarity between prediction and gold standard, rather than only differentiating between correct and incorrect classification.
Good evaluation measures for ontologies should additionally evaluate the results in different dimensions \cite{Dellschaft2006}.
Relevant dimensions, which can be used to evaluate the classification task, include the similarity of word embeddings and the distance in the taxonomy.
The similarity of word embeddings can be computed using cosine similarity and represents a semantic similarity measure,
because the embedding vectors encode the semantic information \cite{Mikolov2013} and therefore similar vectors equates to similar classes.
Distance of prediction and gold standard also describes a type of semantic similarity, since classes describing similar concepts should be grouped close in the taxonomy.
Additionally, the taxonomic distance allows to evaluate other characteristics of the classification algorithm, which are described in the following paragraph.

\begin{figure}[H]
\centering
\includegraphics[width=0.75\textwidth]{images/evaluation/hierarchical_classification_cases.pdf}
\caption{Possible taxonomic relations between prediction and gold standard}
\label{fig:relation between prediction and gold}
\medskip
\small
Legend: red box = gold standard, blue circle = prediction.
\end{figure}

Different taxonomic relations between the prediction and gold standard are possible in a hierarchical classification task \cite{Kosmopoulos2014}.
The desired case is a correct classification, such that the prediction is equal to the gold standard. 
In the case of misclassification, three cases of taxonomic relations are possible. These are represented in Figure~\ref{fig:relation between prediction and gold}.
Evaluation should give an insight whether the evaluated algorithm tends to over- or underspecialization or if it tends to miss the appropriate branch 
\textbf{Note: maybe find a better word than branch}.
Additionally, the distance in the taxonomy between the prediction and gold standard should be included in this analysis,
since the taxonomic distance can serve as a measure of similarity and therefore is therefore able to more accurately show the performance of the evaluated algorithm.
\textbf{consider case where distance between gold and prediction is too high}

A taxonomy is modeled as a directed acyclic graph. 
Therefore it is possible for each class to have multiple superclasses, subsequently for each input class more than one class could be considered as gold standard.
It has to be defined, how the predicted class is paired to the multiple gold standard classes.
Following the decision by \fullcite{Kosmopoulos2014}, the predicted class is paired with the closest gold standard class for each measure.
This achieves a minimization of the classification error, which is a sensible approach.



Scores:
\begin{itemize}
\item \textbf{True positive ratio} $\mathit{TP}$.
\item \textbf{Mean squared error} $\mathit{MSE}$.
For an algorithm $\mathit{alg}$, the set $R_\mathit{alg}$ with $\abs{R_\mathit{alg}} = n$ consisting of prediction-gold standard pairs.
The corresponding gold standard is chosen, as previously stated, to be the closest given option to the prediction.
Based on this set, the classification error for a given algorithm can be computed using the mean squared error, as follows
\begin{align}
&\mathit{MSE} = \frac{1}{n} \sum_{(\vec{p}, \vec{g}) \in R_\mathit{alg}} (1 - \mathit{sim}_\mathit{cos}(\vec{p}, \vec{g}))^2
\end{align}
In comparison to the true positive ratio, the classification error $\mathit{MSE}$ also considers the severity of misclassifications.
The $\mathit{MSE}$ signifies how far a prediction of the given algorithm is from the gold standard.
Therefore a lower value is preferred since this induces that the classifier predicts on average very close to the gold standard.
A low $\mathit{MSE}$ does not necessarly also imply a high $\mathit{TP}$. 
A classifier could for example have a relatively high $\mathit{TP}$, because it predicts well for a specific subset of unknowns,
which occurs frequently in the test samples, but otherwise predicts very badly. 
Using the $\mathit{MSE}$, it is possible to identify this case.

\item weighted $F_1$-score
\end{itemize}

Plots:
\begin{itemize}
\item Density function over prediction to gold standard similarities.
\item Distribution function $P[x \geq X]$ over similarities.
\item Taxonomic distances for misclassification for 3 cases described in Figure~\ref{fig:relation between prediction and gold}
\end{itemize}

\subsection{About the dataset}
Triple sentences generated $72,386,886$ sentences.


\subsection{Results}
Different hybrid algorithms were executed based on the components proposed in Chapter~\ref{section:algorithm}.
In Table~\ref{table:algorithms}, the evaluated hybrid algorithms and their components are listed.

\begin{table}[H]
\center
\begin{tabular}{ l | l l}
name & Wikidata2Sequence & Classification \\
\hline
baseline & triple sentences & KRI-kNN \\
distknn & triple sentences & distance-based kNN \\
linproj & triple sentences & linear projection \\
pwlinproj & triple sentences & piecewise linear projection \\
gw & graph walk sentences & KRI-kNN \\
gw+pwlinproj & graph walk sentences & piecewise linear projection
\end{tabular}
\caption{Evaluated hybrid algorithms}
\label{table:algorithms}
\end{table}

distknn uses the euclidian distance as weights. It is compared to baseline.
It is expected that KRI-kNN provides better results than a distance-based kNN,
since experiments by \fullcite{Chen2009} has shown generally better classification results for KRI-kNN.
The comparison between these classifiers will check whether this is the case for the applied usecase.

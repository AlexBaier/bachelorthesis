\subsection{Method}
The hybrid algorithms are evaluated using a gold standard.
The gold standard is fetched from the set of relevant classes (Section~\ref{section:relevant classes}) in the \dumpdate{} Wikidata dump \cite{WikidataDump}.
$208,502$ classes and their corresponding superclasses were retrieved.

The gold standard test set $G$ consists of sample tuples $(c_i, S_i)$, where $c_i \in R$ is a subclass and $S_i = \mathit{succ}(c_i)$ is the set of direct superclasses of $c_i$,
where $R$ is the set of relevant classes and $i \in [1, |G|]$.
Because it is assumed that every class, except the root class, should have a superclass, $|S_i| > 0$ applies for all gold samples $(c_i, S_i)$ with $i \in [1, |G|]$.
Therefore the computation of the $F_1$-score as an evaluation measure is applicable, because only correct and false classification can occur.
Instead the first applied evaluation measure is the accuracy $\mathit{acc}_\mathit{alg}$, which is calculated as follows for an algorithm $\mathit{alg}$:
\begin{align}
&\mathit{acc}_\mathit{alg} = \frac{1}{|G|} \sum_{i=1}^{|G|} \begin{cases} 1, & \textit{for } p_\mathit{alg}(c_i) \in S_i \\ 0, & \textit{for } p_\mathit{alg}(c_i) \notin S_i \end{cases}
\end{align}
where $p_\mathit{alg} : R \mapsto R$ is the prediction function of $\mathit{alg}$, which returns the predicted superclass of $\mathit{alg}$ given an unknown class.

However, accuracy alone is not sufficient in evaluating a classification in a hierarchical context like taxonomies \cite{Kosmopoulos2014}.
Accuracy ignores the taxonomy and other possible dimensions, as it only measures how often the algorithm is able to correctly guess the superclass.
\citeauthor{Dellschaft2006} states that a good measure should "allow[s] to evaluate an ontology along multiple dimensions" \cite{Dellschaft2006}.
Another dimension, next to the accuracy of the algorithm, which can be observed is the taxonomic relation between the predictions and their corresponding gold standards.
Given two algorithms $\mathit{algA}$ and $\mathit{algB}$ with equal accuracies $\mathit{acc}_\mathit{algA} = \mathit{acc}_\mathit{algB}$,
assume $\mathit{algB}$ predicts either correctly or predicts the root class, while $\mathit{algA}$ predicts either correctly or overspecializes by predicting the subclass of the gold standard.
Which one of the two algorithms performs better? If only the accuracy is considered, both algorithms are equally good and it would not matter, which algorithm would be chosen
for practical application. But in practice $\mathit{algA}$ would be preferred, since even false predictions are useful to some degree, while $\mathit{algB}$'s false predictions are
not usable.
Therefore the taxonomy dimension should be included in the evaluation. For this the \textbf{taxonomic overlap} measure, defined by \fullcite{Dellschaft2006}, is also used
in the evaluation. The taxonomic overlap counts how many superclasses are shared by the prediction and the gold standard for each sample.
This is called the \textbf{semantic upwards cotopy} $\mathit{sc}$ defined as follows \cite{Dellschaft2006}:
\begin{align}
&\mathit{sc}(c, T) = \{ c_i | c_i, c \in C \land c \subclassof{T} c_i \}
\end{align}
where $T=(C, \_)$ is a taxonomy and $c \in C$ is a class in the taxonomy $T$.
Based on the semantic upwards cotopy, the taxonomic overlap $\mathit{to}_\mathit{sc}$ between classes $c_1$ and $c_2$ in $C$ is defined as follows \cite{Dellschaft2006}:
\begin{align}
&\mathit{to}_\mathit{sc}(c_1, c_2, T) = \frac{|\mathit{sc}(c_1, T) \cap \mathit{sc}(c_2, T) |}{|\mathit{sc}(c_1, T) \cup \mathit{sc}(c_2, T) |}
\end{align}
The taxonomic overlap between two classes ranges between $0$, if the classes have no common superclasses, and $1$ if the classes are identical or all direct superclasses are identical.
To apply the taxonomic overlap in the evaluation the overlaps for all prediction-gold pairs have to summed up, which leads to the \textbf{average taxonomic overlap} 
$\mathit{ato}_{\mathit{sc}, \mathit{alg}}$ for an algorithm $\mathit{alg}$:
\begin{align}
&\mathit{ato}_{\mathit{sc}, \mathit{alg}} =\frac{1}{|G|}  \sum_{i = 1}^{|G|} \max_{s \in S_i}(\mathit{to}_\mathit{sc}(p_\mathit{alg}(c_i), s, T))
\end{align}
where $G$ is gold standard, which samples $(c_i, S_i)$ with $i \in [1, |G|]$ and $p_\mathit{alg}$ the prediction function of an algorithm $\mathit{alg}$.
The maximum overlap between prediction and gold is chosen, if there are multiple possible superclass for a sample,
since the taxonomic overlap is a function, which should be maximized by the algorithm \cite{Kosmopoulos2014}.

Each possible combination of components is evaluated as a separate hybrid algorithm.
For each classification component different hyperparameters are tested to find a good configuration.
A naming scheme is used to uniquely identify each hybrid algorithm with their corresponding hyperparameters.
Because the SGNS configuration is constant over all hybrid algorithms, an algorithm combination can be identified by their SequenceGen and Classification component,
as well as their classification parameters.
Consequently, hybrid algorithms have the following name format:
\begin{align*}
&\text{<SequenceGen>}+\text{<Classification>}(\text{<hyperparameters>})
\end{align*}
To shorten the identifiers, the following abbreviations for components are used:
\begin{figure}[H]
\begin{tabular}{l l}
component & abbreviation \\
\hline
triple sentences & ts \\
graph walk sentences & gw \\
distance-based kNN & distknn \\
piecewise linear projection & linproj 
\end{tabular}
\end{figure}
The following hybrid algorithms are evaluated:
\begin{itemize}
\item ts+distknn($k=?$) with $k=5,10,15,20$
\item ts+linproj($c=?$) with $c=1,25,50$
\item gw+distknn($k=?$) with $k=5,10,15,20$
\item gw+linproj($c=?$) with $c=1,25,50$
\end{itemize} 
where $k$ is the number of neighbors in kNN and $c$ is the number of clusters in piecewise linear projection.

\subsection{About the dataset}
% How gold standard was retrieved? (What dump?)
% Number of training/test samples.
% Triple sentences
% Graph walk sentences
% Problem with embeddings (not all relevant class embeddings are represented in SGNS, because of memory issues, subsampling)
% => how many are lost for testing
% characteristic of relevant classes, see Taxonomy analysis.
\begin{figure}[H]
\begin{mdframed}
\centering
\includegraphics[width=0.8\textwidth]{images/evaluation/gold_standard_example.pdf}
\caption{Example for creating a gold standard sample.}
\label{fig:gold standard example}
\begin{flushleft}
A subclass, e.g. \textit{province of Ireland (Q202156)}, and its direct superclasses are retrieved from Wikidata.
The numeric item ids are extracted from these classes, e.g. $\textit{Q202156} \mapsto 202156$, and yields the gold sample $(202156, \{ 3356092, 1620908, 34876 \}) \in G$,
where $G$ is the set of all gold test samples.
\end{flushleft}
\end{mdframed}
\end{figure}

\subsection{Results}


\subsection{Method}\label{section:evaluation method}
The hybrid algorithms are evaluated using a gold standard.
The gold standard is fetched from the set of relevant classes (Section~\ref{section:relevant classes}) in the \dumpdate{} Wikidata dump \cite{WikidataDump}.
$208,502$ classes and their corresponding superclasses were retrieved.

The gold standard test set $G$ consists of sample tuples $(c_i, S_i)$, where $c_i \in R$ is a subclass and $S_i = \mathit{succ}(c_i)$ is the set of direct superclasses of $c_i$,
where $R$ is the set of relevant classes and $i \in [1, |G|]$.
Because it is assumed that every class, except the root class, should have a superclass, $|S_i| > 0$ applies for all gold samples $(c_i, S_i)$ with $i \in [1, |G|]$.
Therefore the computation of the $F_1$-score as an evaluation measure is applicable, because only correct and false classification can occur.
Instead the first applied evaluation measure is the accuracy $\mathit{acc}_\mathit{alg}$, which is calculated as follows for an algorithm $\mathit{alg}$:
\begin{align}
&\mathit{acc}_\mathit{alg} = \frac{1}{|G|} \sum_{i=1}^{|G|} \begin{cases} 1, & \textit{for } p_\mathit{alg}(c_i) \in S_i \\ 0, & \textit{for } p_\mathit{alg}(c_i) \notin S_i \end{cases}
\end{align}
where $p_\mathit{alg} : R \mapsto R$ is the prediction function of $\mathit{alg}$, which returns the predicted superclass of $\mathit{alg}$ given an unknown class.

However, accuracy alone is not sufficient in evaluating a classification in a hierarchical context like taxonomies \cite{Kosmopoulos2014}.
Accuracy ignores the taxonomy and other possible dimensions, as it only measures how often the algorithm is able to correctly guess the superclass.
\citeauthor{Dellschaft2006} states that a good measure should "allow[s] to evaluate an ontology along multiple dimensions" \cite{Dellschaft2006}.
Another dimension, next to the accuracy of the algorithm, which can be observed is the taxonomic relation between the predictions and their corresponding gold standards.
Given two algorithms $\mathit{algA}$ and $\mathit{algB}$ with equal accuracies $\mathit{acc}_\mathit{algA} = \mathit{acc}_\mathit{algB}$,
assume $\mathit{algB}$ predicts either correctly or predicts the root class, while $\mathit{algA}$ predicts either correctly or overspecializes by predicting the subclass of the gold standard.
Which one of the two algorithms performs better? If only the accuracy is considered, both algorithms are equally good and it would not matter, which algorithm would be chosen
for practical application. But in practice $\mathit{algA}$ would be preferred, since even false predictions are useful to some degree, while $\mathit{algB}$'s false predictions are
not usable.
Therefore the taxonomy dimension should be included in the evaluation. For this the \textbf{taxonomic overlap} measure, defined by \fullcite{Dellschaft2006}, is also used
in the evaluation. The taxonomic overlap counts how many superclasses are shared by the prediction and the gold standard for each sample.
This is called the \textbf{semantic upwards cotopy} $\mathit{sc}$ defined as follows \cite{Dellschaft2006}:
\begin{align}
&\mathit{sc}(c, T) = \{ c_i | c_i, c \in C \land c \subclassof{T} c_i \}
\end{align}
where $T=(C, \_)$ is a taxonomy and $c \in C$ is a class in the taxonomy $T$.
Based on the semantic upwards cotopy, the taxonomic overlap $\mathit{to}_\mathit{sc}$ between classes $c_1$ and $c_2$ in $C$ is defined as follows \cite{Dellschaft2006}:
\begin{align}
&\mathit{to}_\mathit{sc}(c_1, c_2, T) = \frac{|\mathit{sc}(c_1, T) \cap \mathit{sc}(c_2, T) |}{|\mathit{sc}(c_1, T) \cup \mathit{sc}(c_2, T) |}
\end{align}
The taxonomic overlap between two classes ranges between $0$, if the classes have no common superclasses, and $1$ if the classes are identical or all direct superclasses are identical.
To apply the taxonomic overlap in the evaluation the overlaps for all prediction-gold pairs have to summed up, which leads to the \textbf{average taxonomic overlap} 
$\mathit{ato}_{\mathit{sc}, \mathit{alg}}$ for an algorithm $\mathit{alg}$:
\begin{align}
&\mathit{ato}_{\mathit{sc}, \mathit{alg}} =\frac{1}{|G|}  \sum_{i = 1}^{|G|} \max_{s \in S_i}(\mathit{to}_\mathit{sc}(p_\mathit{alg}(c_i), s, T))
\end{align}
where $G$ is gold standard, which samples $(c_i, S_i)$ with $i \in [1, |G|]$ and $p_\mathit{alg}$ the prediction function of an algorithm $\mathit{alg}$.
The maximum overlap between prediction and gold is chosen, if there are multiple possible superclass for a sample,
since the taxonomic overlap is a function, which should be maximized by the algorithm \cite{Kosmopoulos2014}.

\textbf{TODO: Introduce most common classifier and how it should compare to the other methods}

Each possible combination of components is evaluated as a separate hybrid algorithm.
For each classification component different hyperparameters are tested to find a good configuration.
A naming scheme is used to uniquely identify each hybrid algorithm with their corresponding hyperparameters.
Because the SGNS configuration is constant over all hybrid algorithms, an algorithm combination can be identified by their classification component, as well as their  classification hyperparameters.
Consequently, hybrid algorithms have the following name format:
\begin{align*}
&\text{<Classification>}(\text{<hyperparameters>})
\end{align*}
To shorten the identifiers, the following abbreviations for components are used:
\textbf{TODO: add new component names}
\begin{table}[H]
\center
\begin{tabular}{l l}
component & abbreviation \\
\hline
distance-based kNN & distknn \\
piecewise linear projection & linproj 
\end{tabular}
\caption{Abbreviations for components}
\end{table}
The following hybrid algorithms are evaluated:
\textbf{TODO: add new algorithms}
\begin{itemize}
\item distknn($k=?$) with $k=5,10,15,20$
\item linproj($c=?$) with $c=1,25,50$
\end{itemize} 
where $k$ is the number of neighbors in kNN and $c$ is the number of clusters in piecewise linear projection.

\subsection{About the dataset}
% How gold standard was retrieved? (What dump?)
% Number of training/test samples.
% Triple sentences
% Problem with embeddings (not all relevant class embeddings are represented in SGNS, because of memory issues, subsampling)
% => how many are lost for testing
% characteristic of relevant classes, see Taxonomy analysis.
The Wikidata dump from \dumpdate{} \cite{WikidataDump} was used in the evaluation.
The gold standard is retrieved from the set of relevant classes (Section~\ref{section:relevant classes}).
A total of $178,771$ gold sample tuples $(t_i, S_i)$ was retrieved.
$10,000$ of the samples are used for testing, the $168,771$ remaining samples are used for training.

Figure~\ref{fig:gold standard example} depicts the extraction of gold samples from the Wikidata taxonomy.
A subclass, e.g. \textit{province of Ireland (Q202156)}, and its direct superclasses are retrieved from Wikidata.
The numeric item ids are extracted from these classes, e.g. $\textit{Q202156} \mapsto 202156$, and yields the gold sample $(202156, \{ 3356092, 1620908, 34876 \}) \in G$,
where $G$ is the set of all gold test samples.

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{images/evaluation/gold_standard_example.pdf}
\caption{Example for creating a gold standard sample.}
\label{fig:gold standard example}
\end{figure}

A total of $109,267,818$ triple sentences were extracted from the Wikidata dump \cite{WikidataDump}.
All entities, which have irrelevant properties, as defined in Section~\ref{section:relevant classes}, were excluded.
All sentences representing test sample subclass-of relations were also excluded.

Due to memory limitations, SGNS is only able to capture a certain vocabulary size. Therefore only $5,000,000$ words are represented by SGNS.
Less frequent words have therefore no embeddings. This also affects a small number of relevant classes, which occur in the test samples.
\textbf{TODO: ensure this number is correct}
Therefore only $8,943$ out of $10,000$ test samples can be used in the evaluation.

\subsection{Results}
The hybrid algorithms with the previously described parameters were executed.
The accuracies and average taxonomic overlaps are  listed in Table~\ref{table:evaluation results}.

\textbf{TODO: add missing results}
\begin{table}[H]
\center
\begin{tabular}{| l | c | c |}
\hline
algorithm & accuracy & taxonomic overlap \\
\hline
\hline
baseline & $8.8\%$ & $31.26\%$ \\
\hline
\hline
ts+distknn (k=5) & $21.38\%$ & $45.17\%$ \\
\hline
ts+distknn (k=10) & $22.26\%$ & $46.17\%$ \\
\hline
ts+distknn (k=15) & $22.62\%$ & $46.56\%$ \\
\hline
ts+distknn (k=20) & $22.83\%$ & $47.10\%$ \\
\hline
\hline
ts+linproj (c=1) & $7.84\%$ & $31.76\%$ \\
\hline
ts+linproj (c=25) & $11.06\%$ & $32.14\%$\\
\hline
ts+linproj (c=50) & $11.99\%$ & $32.82\%$\\
\hline
\end{tabular}
\caption{Evaluation results for all hybrid algorithms}
\label{table:evaluation results}
\end{table}


\textbf{TODO: compare algorithm performances between classifiers for triple sentences}


\textbf{TODO: interpret results}


\textbf{TODO: Explain taxonomic overlap histogram}
\textbf{TODO: Show significant histograms of best performing algorithms}

\textbf{TODO: Interpret low performance in regards to taxonomic overlap}


\subsection{Discussion}

The problem of classification with a very high number of labels ($>100,000$) is very difficult.
No easily comparable problem was found in literature, therefore no comparison with state-of-the-art methods was possible.
\textbf{TODO: Repeat motivation for most-common classifier}.
Therefore it is at least possible to confirm that the hybrid algorithms proposed in this work could help users in identifying appropiate superclasses for given orphan classes.

The following  problems could be identified, which future work could attempt to solve.

The implemented linear projection classifier is flawed.
Because subclass-superclass pairs are clustered by the subclass and one subclass can have multiple superclasses, different types of subclass-of relations are grouped
into one cluster.
This leads to problems in training the corresponding projection, since dissimilar offsets have to represented by a single projection and therefore the projection
cannot converge to an optimal solution.
Instead clustering by subclass-of offsets, as done by \fullcite{Fu2014}, could provide better results, if an effective method for identifying the correct cluster for an unknown class
can be found.
Since subclass-of clusters are related to single topics  as shown in Section~\ref{section:vector offset},
it may be feasible to identify the correct cluster for an unknown class by analyzing its properties and matching it to the most common properties in a subclass-of cluster.

The problem statement is too difficult. A classification problem with over $100k$ labels is difficult to solve.
Approaches in reducing the number of possible superclasses should be considered. 
Most classes have a distance of $6$ to $10$ to the root class (Section~\ref{section:root taxonomy}),
therefore it may be applicable to only consider classes with distance $5$ to $9$ as superclasses.
A topic-based reduction of possible superclasses may have benefits, since classes with similar topics are closely related in the taxonomy,
it may be sensible to only observe a certain excerpt of the taxonomy for each classification decision.
This approach would however require a method of clustering the taxonomy into topic-based subgraphs and also identifying the correct topic for a given unknown class.
\textbf{TODO: Motivate use of taxonomy-based classifiers}


\textbf{TODO: Motivate more extensive use of neural networks for classification/regression}

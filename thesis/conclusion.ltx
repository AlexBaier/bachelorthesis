In this thesis, hybrid algorithms using neural word embeddings for taxonomy enrichment were developed.
Algorithms were developed in consideration of the Wikidata knowledge base as use case.
Because Wikidata is mainly curated and extended by users, the developed algorithms should be able to support Wikidata users
in the taxonomy enrichment task by suggesting super classes for orphan classes, thereby adding new taxonomic relations to Wikidata's taxonomy.
The work consists of two main results: Analysis of Wikidata's taxonomy and hybrid algorithms for taxonomy enrichment.

Wikidata's taxonomy was analyzed in the thesis. Insights for possible future work with Wikidata was gained.
Automated mapping of other specialized knowledge bases like the Entrez Gene knowledge base skews the distribution of classes.
Classes, which are relevant to human editors, have a  low share of $\approx 15\%$ in the taxonomy.
Future work on Wikidata should therefore consider removing every entitity, which has undesirable properties, as a measure for improving validity of evaluation results.
An incomplete list of undesirable properties in regards to Wikidata's taxonomy is given in Section~\ref{section:relevant classes}.
The taxonomy is in a good state, since most classes ($\approx 97\%$) are in the root taxonomy.
Additionally, it can be argued that the constant curation by human editors improves the quality of content in Wikidata. 
Therefore using Wikidata's taxonomy as an easy to retrieve gold standard may be applicable for future work.

The enrichment task was modeled as a classification task with over $100,000$ labels.
The taxonomy was enriched by adding new subclass-of relations between orphan classes and classes in the root taxonomy.
The developed hybrid algorithm consisted of three sequential components.
SequenceGen, which generates word sequences given a data source, e.g. Wikidata.
A Word2Vec neural network \cite{Mikolov2013}, specifically SGNS, is trained on the SequenceGen output to produce embeddings for all relevant classes.
Finally, a classification component, trained on subclass-superclass pairs, exploits the characteristics of word embeddings to
classify orphan classes.
Distance-based kNN,  linear projection, and non-linear regression multi-network model were implemented as classification components. 
Linear projection and multi-network model are regression-based approaches, which predict the superclass embedding instead of the label.
Evaluation has shown that kNN outperforms the other classifiers. Distance-based kNN with $20$ neighbors achieves an accuracy of $22.83\%$.
It was concluded that an application using a hybrid algorithm with a kNN classification component would be suitable for supporting
Wikidata users, as it would correctly predict a superclass in approximately $1$ of $4$ cases.

Future work in regards to improving the algorithm for the given classification task would have multiple venues to explore.

Sequences were generated by directly translating Wikidata statements into triple sentences.
Other more complex approaches may provide better training data for more effective word embeddings.
Creating sequences with random graphs walks was motivated in the thesis, but preliminary experimentation with graph walk sentences produced unexpected, bad results.
Other research, such as \fullcite{Ristoski2016}, provides contradictory results. \fullcite{Ristoski2016} shows that embeddings generated with graph walks
produces higher quality embeddings than triple sentences.
Assuming the implementation was not at fault for the experienced failure,
further experiments and analysis may answer, why graph walk sentences failed in the taxonomy enrichment task.

Replacing the SGNS model with another model for word embeddings may also improve the performance of the hybrid algorithms.
Recurrent or deep neural networks could provide more effective embeddings \cite{Arisoy2012} \cite{Ororbia2017}.
Deep neural graph embeddings may either replace or enrich the word embeddings generated by other models \cite{Cao2016}.

All developed hybrid algorithms were performing badly in regards to taxonomic overlap. Improving the taxonomic overlap may also improve the accuracy.
Using taxonomy-based classifiers as classification component is likely to result in higher taxonomic overlaps,
since the current algorithms disregard the taxonomy.

The use of word embeddings in ontology learning is promising.
Subclass-of relations in a taxonomy can be represented by embedding offsets.
Different types of subclass-of relations, which are topically related,  exist in Wikidata's taxonomy.
The successful use of kNN also shows that embeddings effectively represent classes, since similar classes were grouped close together.
Further exploration on how embeddings can represent entities in knowledge bases could be beneficial.
For example, the thesis' approach could be adjusted for the classification of instances and other relations, e.g. \textit{occupation (P106)}.
This would be done by replacing the subclass-superclass pairs by e.g. instance-class pairs for \textit{instance of (P31)}  or person-job pairs for \textit{occupation (P106)}.
Word embeddings abstract from the actual implementation of data in the knowledge base.
Entities are replaced by corresponding word embeddings, thereby allowing the exploitation of a wide range of general purpose algorithms.
As shown in this work, abstraction via embeddings allows the application of classification methods, such as kNN, linear regressions, and neural networks.
Different data sources can also be exploited with word embeddings. This requires an existing mapping between Wikidata's ontology and the ontology of other knowledge bases
or a mapping between Wikidata IDs and words in natural text.

Ontology learning can benefit from using word embeddings, because they are able to effectively represent entities.
This abstraction opens ontology learning to an existing wide range of methods, which operate on vectors, such as neural networks, regression methods and general classifiers.
Insights on the usage of Wikidata for ontology learning were gained.
Wikidata has a high degree of completeness in regards to connectedness, as only $\approx 3\%$ are not connected to the root taxonomy.
Due to automated mapping of specialized knowledge bases, only a small percentage of classes can be considered as relevant for the Wikidata taxonomy.

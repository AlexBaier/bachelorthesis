In this thesis, hybrid algorithms using neural word embeddings for taxonomy enrichment were presented.
The enrichment task was modeled as a classification task with over $100,000$ labels.
The taxonomy was enriched by adding new subclass-of relations between orphan classes and classes in the root taxonomy.
The developed hybrid algorithm consisted of three sequential components.
SequenceGen, which generates sequences given a data source, e.g. Wikidata.
A Word2Vec neural network \cite{Mikolov2013}, specifically SGNS, is trained on the SequenceGen output to produce embeddings for all relevant classes.
Finally, a classification component, trained on subclass-superclass pairs, exploits the characteristics of word embeddings to
classify orphan classes.
Triple sentences and graph walk sentences were implemented as SequenceGen components.
\textbf{TODO: update based on evaluation results}
Due to the fact that graph walks increase the amount of context given to each word, the performance of hybrid algorithm using graph walk sentences was better.
kNN and linear projection were implemented as classification component. 
As shown by the evaluation, the linear projection approach is not suited to the given difficult classification task.
kNN outperformed linear projection significantly.

Future work in regards to improving the algorithm for the given classification task would have multiple venues to explore.
Other approaches for generating embeddings may be beneficial.
Instead of a simple feedforward model implemented by SGNS, recurrent or deep neural networks could provide more effective embeddings \cite{Arisoy2012} \cite{Ororbia2017} \cite{Mikolov2010}.
Deep neural graph embeddings may either replace or enrich the word embeddings generated by other models \cite{Cao2016}.

The use of word embeddings in ontology learning is promising.
Subclass-of relations in a taxonomy can be represented by embedding offsets.
Different types of subclass-of relations, which are topically related,  exist in Wikidata's taxonomy.
The successful use of kNN also shows that embeddings effectively represent classes, since similar classes were grouped close together.
Further exploration on how embeddings can represent entities in knowledge bases could be beneficial.
For example, the thesis' approach could be adjusted for the classification of instances and potentially other more complex relations, e.g. \textit{occupation (P106)}.

Wikidata's taxonomy was analyzed in the thesis. Insights for possible future work with Wikidata was gained.
Automated mapping of other specialized knowledge bases like the Entrez Gene knowledge base skews the distribution of classes.
Classes, which are relevant to human editors, have a  low share of $\approx 15\%$ in the taxonomy.
Future work on Wikidata should therefore consider removing every entitity, which has undesirable properties, as a measure for improving validity of evaluation results.
An incomplete list of undesirable properties in regards to Wikidata's taxonomy is given in Section~\ref{section:relevant classes}.
The taxonomy is in a good state, since most classes ($\approx 97\%$) are in the root taxonomy.
Additionally, it can be argued that the constant curation by human editors improves the quality of content in Wikidata. 
Therefore using Wikidata's taxonomy as an easy to retrieve gold standard may be applicable for future work.
In this thesis, hybrid algorithms using neural word embeddings for taxonomy enrichment were developed.
Algorithms were developed in consideration of the Wikidata knowledge base as use case.
Because Wikidata is mainly curated and extended by users, the developed algorithms should be able to support Wikidata users
in the taxonomy enrichment task by suggesting super classes for orphan classes, thereby adding new taxonomic relations to Wikidata's taxonomy.
The work consists of two main results: An analysis of Wikidata's taxonomy and hybrid algorithms for taxonomy enrichment on a real data set extracted from Wikidata.

Insights for possible future work with Wikidata was gained.
Automated mapping of other specialized knowledge bases like the Entrez Gene knowledge base  \cite{Maglott2005}, 
which contains information about fully sequenced genomes,  skews the distribution of classes.
Classes, which are relevant to human editors, have a  low share of ${\sim} 15\%$ in the taxonomy.
Future work on Wikidata should therefore consider removing every entitity, which has undesirable properties, as a measure for improving validity of evaluation results.
An incomplete list of undesirable properties in regards to Wikidata's taxonomy is given in Section~\ref{section:relevant classes}.
The taxonomy is in a good state, since most classes (${\sim} 97\%$) are in the root taxonomy.
Additionally, it can be argued that the constant curation by human editors improves the quality of content in Wikidata. 
Therefore using Wikidata's taxonomy as an easy to retrieve gold standard may be applicable for future work.

The enrichment task was modeled as a classification task with over $100,000$ labels.
The taxonomy was enriched by adding new subclass-of relations between orphan classes and classes in the root taxonomy.
The developed hybrid algorithm consisted of three sequential components.
SequenceGen, which generates word sequences given a data source, e.g. Wikidata.
A Word2Vec neural network \cite{Mikolov2013}, specifically SGNS, is trained on the SequenceGen output to produce embeddings for all relevant classes.
Finally, a classification component, trained on subclass-superclass pairs, exploits the characteristics of word embeddings to
classify orphan classes.
Distance-based kNN,  linear projection, and non-linear regression multi-network model were implemented as classification components. 
Linear projection and multi-network model are regression-based approaches, which predict the superclass embedding instead of the label.

Evaluation has shown that kNN outperforms the other classifiers, which are non-linear regression via neural networks and linear projection. 
Distance-based kNN with $20$ neighbors achieves an accuracy of $22.83\%$.
Further analysis has shown that the algorithm performs well for classes, which are well described and which have related classes.
Thereby classes with more frequent properties, such as  \textit{sport (P17)} and \textit{country (P17)}, 
can be accurately predicted.
Therefore it can be concluded that the hybrid algorithm can support Wikidata users in orphan class labeling for classes, which have frequently occurring properties.

The use of word embeddings in ontology learning is promising.
Subclass-of relations in a taxonomy can be represented by embedding offsets.
Different types of subclass-of relations, which are topically related,  exist in Wikidata's taxonomy.
The successful use of kNN also shows that embeddings effectively represent classes, since similar classes were grouped close together.
Further exploration on how embeddings can represent entities in knowledge bases could be beneficial.
For example, the thesis' approach could be adjusted for the classification of instances and other relations, e.g. \textit{occupation (P106)}.
This would be done by replacing the subclass-superclass pairs by e.g. instance-class pairs for \textit{instance of (P31)}  or person-job pairs for \textit{occupation (P106)}.
Word embeddings abstract from the actual implementation of data in the knowledge base.
Entities are replaced by corresponding word embeddings, thereby allowing the exploitation of a wide range of general purpose algorithms.
As shown in this work, abstraction via embeddings allows the application of classification methods, such as kNN, linear regressions, and neural networks.
Different data sources can also be exploited with word embeddings. This requires an existing mapping between Wikidata's ontology and the ontology of other knowledge bases
or a mapping between Wikidata IDs and words in natural text.

Ontology learning can benefit from using word embeddings, because they are able to effectively represent entities.
This abstraction opens ontology learning to an existing wide range of methods, which operate on vectors, such as neural networks, regression methods and general classifiers.
Insights on the usage of Wikidata for ontology learning were gained.
Wikidata has a high degree of completeness in regards to connectedness, as only ${\sim} 3\%$ are not connected to the root taxonomy.
Due to automated mapping of specialized knowledge bases, only a small percentage of classes can be considered as relevant for the Wikidata taxonomy.

Future work in regards to improving the algorithm for the given classification task could entail multiple venues, such as improving the generation of input sequences for
SGNS or other neural networks, applying other networks for word embeddings, and usage of taxonomy-based classification algorithms.

Sequences were generated by directly translating Wikidata statements into triple sentences.
Other more complex approaches may provide better training data for more effective word embeddings.
Creating sequences with random graphs walks was motivated in the thesis, but preliminary experimentation with graph walk sentences produced unexpected, bad results.
Other research, such as \fullcite{Ristoski2016}, provides contradictory results. \fullcite{Ristoski2016} shows that embeddings generated with graph walks
produces higher quality embeddings than triple sentences.
Assuming the implementation was not at fault for the experienced failure,
further experiments and analysis may answer, why graph walk sentences failed in the taxonomy enrichment task.

Replacing the SGNS model with another model for word embeddings may also improve the performance of the hybrid algorithms.
Recurrent or deep neural networks could provide more effective embeddings \cite{Arisoy2012} \cite{Ororbia2017}.
Deep neural graph embeddings may either replace or enrich the word embeddings generated by other models \cite{Cao2016}.

All developed hybrid algorithms were performing badly in regards to taxonomic overlap. Improving the taxonomic overlap may also improve the accuracy.
Using taxonomy-based classifiers as classification component is likely to result in higher taxonomic overlaps,
since the current algorithms disregard the taxonomy.

In conclusion, the representation of ontological classes and instances is an interesting and promising approach, which enables the application of a variety of existing general purpose algorithms.
Further research in the capabilities of word embeddings in ontology learning is recommended.
Wikidata's data is freely available, represents a general ontology, and is of good quality, due to human curation, therefore it is well suited as use case for ontology learning.
However, filtering of the dataset is necessary to remove automatically added, domain-specific knowledge from third-party KBs, which is irrelevant to most real life applications
of ontology learning.



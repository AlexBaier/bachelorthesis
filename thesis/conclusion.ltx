In this thesis, hybrid algorithms using neural word embeddings for taxonomy enrichment were presented.
The enrichment task was modeled as a classification task with over $100,000$ labels.
The taxonomy was enriched by adding new subclass-of relations between orphan classes and classes in the main taxonomy.
The developed hybrid algorithm consisted of three sequential components.
SequenceGen, which generates sequences given a data source, e.g. Wikidata.
A Word2Vec neural network \cite{Mikolov2013}, specifically SGNS, is trained on the SequenceGen output to produce embeddings for all relevant classes.
Finally, a classification component, trained on subclass-superclass pairs, exploits the characteristics of word embeddings to
classify orphan classes.
Triple sentences and graph walk sentences were implemented as SequenceGen components.
\textbf{TODO: update based on evaluation results}
Due to the fact that graph walks increase the amount of context given to each word, the performance of hybrid algorithm using graph walk sentences was better.
kNN and linear projection were implemented as classification component. 
As shown by the evaluation, the linear projection approach is not suited to the given difficult classification task.
kNN outperformed linear projection significantly.

Future work in regards to improving the algorithm for the given classification task would have multiple venues to explore.
Other approaches for generating embeddings may be beneficial.
Instead of a simple feedforward model implemented by SGNS, recurrent or deep neural networks could provide more effective embeddings \cite{Arisoy2012} \cite{Ororbia2017} \cite{Mikolov2010}.
Deep neural graph embeddings may either replace or enrich the word embeddings generated by other models \cite{Cao2016}.

The use of word embeddings in ontology learning is promising.
It was shown the subclass-of relation in a taxonomy can be represented by offsets.
Different types of subclass-of relations, which are topically related,  seem to exist in Wikidata's taxonomy.
The successful use of kNN also shows that embeddings effectively represent classes, since similar classes were grouped close together.
Further exploration on how embeddings can represent entities in knowledge bases could be beneficial.
For example, the thesis' approach could be adjusted for the classification of instances and potentially other more complex relation, e.g. \textit{occupation (P106)}.
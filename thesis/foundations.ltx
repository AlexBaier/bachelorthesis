% Wikidata
\subsection{Wikidata}\label{section:wikidata}
Wikidata is an open, free, multilingual and collaborative knowledge base. It is a structured knowledge source for other Wikimedia projects. 
It tries to model the real world, meaning every concept, object, animal, person, etc.
Wikidata is mostly edited and extended by humans, which in general improves the quality of entries compared to fully-automated systems, 
because different editors can validate and correct occurring errors.
However, Wikidata, like most knowledge bases, is incomplete and therefore has to be operated under the \textit{Open World Assumption} (OWA).
OWA states that if a statement is not contained in a knowledge base, it is not necessarily false but rather unknown \cite{Galarraga2016}.

In Wikidata items and properties exist. Items are the aforementioned concepts, objects, etc. While properties are used to make claims about
items, e.g. \textit{photographic film (Q6293)} is a \textit{subclass of (P279)} \textit{data storage device (Q193395)} (see Figure~\ref{fig:class example}).
Each item and property has an unique identifier, which starts with the letter Q for items and the letter P for properties and is followed by a numeric code.
The identifiers in Wikidata are essential to avoid ambiguity and to make items and properties multilingual.

Items consist of labels, aliases and descriptions in different languages. 
Sitelinks connect items to their corresponding pages of Wikimedia projects like Wikipedia articles.
Most importantly item are described by statements. 
Statements are in their simplest form a pair of property and value, assigned to a specific item. A value is either a literal value or another item.
It should be noted that an item can have multiple statements with the same property. The set of statements with the same property is called statement group.
Statements can be annotated with qualifiers, which specify the context of the statement, e.g. population at a certain point of time.
Additionally, references can be used for statements to include its source. See Figure~\ref{fig:class example} for an example of a Wikidata item.
\begin{figure}
\centering
\includegraphics[width=\textwidth]{images/foundations/item_example.png}
\caption{Example of Wikidata class: photographic film (Q6239)}
\label{fig:class example}
\end{figure}

Following, the terms of item and statement are defined in the context of Wikidata.
\begin{definition}[Item]\label{definition:item}
An\textnormal{item} is a tuple $(\mathit{id},  \mathit{label}, \mathit{aliases}, \mathit{description}, \mathit{sitelinks})$:
	\begin{itemize}
	\item $\mathit{id} \in \mathbb{N}$ is the numerical item ID;
	\item $\mathit{label} \in \mathit{String}$ is the English label of the item;
	\item $\mathit{aliases} \in \mathcal{P}(\mathit{String})$ is the set of English synonyms for the label;
	\item $\mathit{description} \in \mathit{String}$ is a short sentence describing the item;
	\item $\mathit{sitelinks} \in \mathit{String} \times \mathit{String}$ is a set of tuples $(\mathit{site}, \mathit{title})$, where $\mathit{site}$ refers to a specific site of the Wikimedia
	 projects, e.g. enwiki, and $\mathit{title}$ is the corresponding article title of the item on this site.
	\end{itemize}
\end{definition}
\begin{definition}[Statement]
A \textnormal{statement} is a tuple $(\mathit{itemid}, \mathit{pid}, \mathit{value}, \mathit{refs}, \mathit{qualifiers})$:
\begin{itemize}
\item $\mathit{itemid} \in \mathbb{N}$ is a numerical item ID, to which the statement belongs;
\item $\mathit{pid} \in \mathbb{N}$ is a numerical property ID;
\item $\mathit{value}$ is either a constant value like string, int, etc., or an item ID;
\item $\mathit{refs}$ is a set of references, containing the source of information for the statement;
\item $\mathit{qualifiers}$ is a set of qualifiers, which further specifies the statement.
\end{itemize}
\end{definition}
In Wikidata, there is no strict distinction between classes and instances. Both groups are represented as items.
This leads to the issue, that recognizing, whether an item is a class or instance is not trivial.
Based on which statements connect two items, a distinction can be made.
A class is any item, which has instances, subclasses or is the subclass of another class.
In Wikidata, the properties \textit{instance of (P31)} and \textit{subclass of (P279)} exist, which describe this relation between items.
Therefore to identify whether an item is a class, it needs to be checked, whether the items fulfills any of the three above criteria.
\begin{definition}[Class]
Given a set of items $I$ and a set of statements $R$. $c = (\mathit{classid}, \_, \_, \_, \_)  \in I$ is a \textnormal{class}, if at least one of the following assertions are true:
\begin{align*}
&\exists i=(\mathit{instanceid}, \_, \_, \_, \_) \in I \; \exists s=(\mathit{itemid}, \mathit{pid}, \mathit{value}, \_, \_) \in R :  \\
&\phantom{\exists i=(\mathit{instanceid}, \_, \_, \_, \_) \in I} \mathit{instanceid} = \mathit{itemid} \land \mathit{pid} = 31 \land \mathit{value} = \mathit{classid} \: 
	\textnormal{(has instance)}\\
&\exists s=(\mathit{itemid}, \mathit{pid}, \_, \_, \_) \in I: \mathit{itemid} = \mathit{classid} \land \mathit{pid} = 279 \: \textnormal{(is subclass)}\\
&\exists i=(subclassid, \_, \_, \_, \_) \in I \; \exists s=(itemid, pid, value, \_, \_) \in R : \\
&\phantom{\exists i=(\mathit{subclassid}, \_, \_, \_, \_)\in I } \mathit{itemid} = \mathit{subclassid} \land \mathit{pid} = 279 \land \mathit{value} = \mathit{classid} \: 
	\textnormal{(has subclass)}
\end{align*}
\end{definition}
$\_$ is used as an anonymous placeholder, for the purpose of not naming unused elements in tuples.
For example, \textit{photographic film (Q6293)} (Figure~\ref{fig:class example}) is a class, because it is the subclass of three other classes.

% Taxonomy
\subsection{Taxonomy}\label{section:taxonomy}

``\textit{Ontologies} are (meta)data schemas, 
providing a controlled vocabulary of concepts, each with an explicitly defined and machine processable semantics'' \cite{Maedche2001}. Additionally it is possible for ontologies to
contain axioms used for validation and constraint enforcement. Ontologies enable the modeling and sharing of knowledge in a specific domain and support
the knowledge exchange via web by extending syntactic to semantic interoperability \cite{Hazman2011}.
In comparison, a knowledge base like Wikidata can be seen as an instantiation of such an ontology,
since every knowledge base has to be conceptualized by an ontology \cite{Wong2012}. Different types of ontologies can grouped by their level of formality and expressiveness.
\fullcite{Wong2012} differentiates ontologies as lightweight and heavyweight ontologies (see Figure~\ref{fig:ontology spectrum}). 
\textit{Taxonomies} are concept or class hierarchies.
They typically represent a parent-child structure, which can be formalized with a single relationship called for example \textit{subclass-of} in the case of Wikidata.
The observed taxonomy in Wikidata belongs to the category of lightweight ontologies, specifically \textit{principled, informal hierarchies}, 
as the only enforced rule for the subclass-of relation is that it should connect two entities \cite{WikidataP279}.
\begin{figure}
\centering
\includegraphics[width=\textwidth]{images/foundations/wong2012_spectrum_of_ontology_kinds.png}
\caption{The spectrum of ontology kinds. \cite{Wong2012}}
\label{fig:ontology spectrum}
\end{figure}

For the purpose of developing a formal definition of the thesis' problem statement the notion of taxonomy needs to be formalized.
\fullcite{Cimiano2006} defines a heavyweight ontology, which includes a taxonomy, as follows:\\
``
	\begin{definition*}[Ontology]\label{Ontology}
		An \textnormal{ontology} is a structure
		\begin{equation*} 
			\mathcal{O} := (C, \taxon, R, \relsig, \relhier, \mathcal{A}, \attsig, \mathcal{T})
		\end{equation*}
		consisting of
		\begin{itemize}
			\item four disjoint sets $C$, $R$, $\mathcal{A}$, and $\mathcal{T}$ whose elements are called \textnormal{concept identifiers}, 
				\textnormal{relation identifiers}, \textnormal{attribute identifiers} and \textnormal{data types}, respectively,
			\item a semi-upper latice $\taxon$ on $C$ with top element $\rootc$, called \textnormal{concept hierarchy} or \textnormal{taxonomy},	
			\item a function $\relsig: R \rightarrow C^+$ called \textnormal{relation signature},
			\item a partial order $\relhier$ on $R$, called \textnormal{relation hierarchy}, where $r_1 \relhier r_2$ implies $\abs{\relsig (r_1)} = \abs{\relsig (r_2)}$ and
				$\proj{i}{\relsig (r_1)} \taxon \proj{i}{\relsig (r_2)}$, for each $1 \le i \le \abs{\relsig (r_1)}$, and
			\item a function $\attsig : \mathcal{A} \rightarrow C \times \mathcal{T}$, called \textnormal{attribute signature},
			\item a set $\mathcal{T}$ of datatypes such as strings, integers, etc.
		\end{itemize}
	\end{definition*}
	Hereby, $\proj{i}{t}$ is the i-th component of tuple $t$. [...] Further, a semi-upper lattice $\le$ fulfills the following conditions:
	\begin{align*}
		&\forall x (x \le x) \: \text{(reflexive)} \\
		&\forall x \forall y (x \le y \land y \le x \implies x = y) \: \text{(anti-symmetric)} \\
		&\forall x \forall y \forall z (x \le y \land y \le z \implies x \le z) \: \text{(transitive)}\\
		&\forall x x \le top \: \text{(top element)}\\
		&\forall x \forall y \exists z (z \ge \land z \ge y \land \forall w (w \ge x \land w \ge y \implies w \ge z)) \: \text{(supremum)}
	\end{align*}
	So every two elements have a unique most specific supremum.
''\\

A taxonomy can be modeled as a semi-upper lattice. This induces two important assumptions about the structure and to some degree completeness of the
observed taxonomies. First, there is only one \textit{root class}, top element of the lattice, of which every other class is (transitively) a subclass. Second,
because of the supremum property, the taxonomy is fully connected, which means each class, but the root class, has a superclass.
Wikidata's taxonomy does therefore not fulfill the definition by \fullcite{Cimiano2006}, as it is not fully connected.\\
In the following, definitions wil bel presented, which attempt to model an incomplete taxonomy based on the already presented data model and structure of Wikidata.
Refer to Appendix~\ref{section:graphs} for the necessary definitions on graphs.

In Wikidata, a class can have multiple superclasses, therefore a tree structure is not sufficient to model the taxonomy.
However a directed acyclic graph, can model the taxonomy. The acyclic constraint is necessary to ensure that no class is transitively a subclass of itself.
\begin{definition}[Taxonomy]
A \textnormal{taxonomy} $T=(C, S)$ is a directed  acyclic graph, where $C$ is a set of \textnormal{class identifiers}, 
and $S$ is the set of edges, which describe the \textnormal{subclass-of relation}  between two classes. such that $c_1$ is the subclass of $c_2$, if $(c_1, c_2) \in S$.
\end{definition}
\begin{definition}[Subclass-of relation]\label{subclass of}
The transitive binary relation $\subclassof{T}$ on the taxonomy $T=(C, S)$ represents the subclass relationship
of two classes in $T$. Given $c_1, c_2 \in C$, $c_1 \subclassof{T} c_2$, if there is a walk $W=(c_1, \ldots, c_2)$ with length 
$n \ge 1$, which connects $c_1$ and $c_2$. \subclassof{T} is transitive,  $\forall c_1, c_2, c_3 \in C:
c_1 \subclassof{T} c_2 \land c_2 \subclassof{T} c_3 \implies c_1 \subclassof{T} c_3$.
\end{definition}
If the taxonomy defined by \fullcite{Cimiano2006} is mapped on this graph-based taxonomy model, the following
assumption is true, for $T=(C,S)$: 
\begin{align}
 \abs{\{  c \in C \mid \neg \exists s \in C: c \subclassof{T} s \}} = 1
 \end{align}
Only one class in this taxonomy has no superclasses. This class is called \textit{root class}. However
in the case of Wikidata, this assumption does not hold true. The following state is the case:
\begin{align}
\abs{\{  c \in C \mid \neg \exists s \in C: c \subclassof{T} s \}} > 
\end{align}
There are classes other than the root class, which also have no superclasses. These classes will be called
\textit{unlinked classes}.
\begin{definition}[Root class]\label{root class}
Given a taxonomy $T=(C, S)$, the \textnormal{root class} $root_T$ is a specific, predefined class with no superclasses in 
$T$. For $root_T$, $\abs{succ_T(root_T)} = 0$ applies.
\end{definition}
\begin{definition}[Unlinked class]\label{unlinked class}
Given a taxonomy $T=(C,S)$ with a root class $root_T$, a class $u \in C$ is called \textnormal{unlinked class},
if $u \neq root_T \land \abs{succ_T(u)} = 0$.
\end{definition}
In Wikidata, the root class is \entity{} \cite{WikidataQ35120}. 
All other classes, which are not subclasses of \entity{}, are therefore either unlinked classes, or subclasses of unlinked classes.
In Section~\ref{section:taxonomy analysis}, it will be shown that the Wikidata taxonomy graph is not fully
connected. But the component, which contains the root class \entity{}, contains $97\%$ of all classes.
This component will be referred to as \textit{root taxonomy} in later sections.

% Similarity
\subsection{Similarity}\label{section:similarity}
\begin{itemize}
\item semantic similarity e.g. distributional similarity \\
	\fullcite{Lin1998} \\
	\fullcite{Rodriguez2003}
\item geometrical similarity e.g. distance based-similarity, cosine similarity
\end{itemize}
For the task of ontology learning \cite{Hazman2011} as well as classification, e.g. k-nearest-neighbors,
the concept of similarity is of importance. A basic intuition of similarity is for example given by 
\fullcite{Lin1998}. Similarity is related to the commonalities and differences between two objects.
More commonalities implies higher similarity. Vice versa, more differences implies lower similarity.
Two identical objects should have the maximum similarity. In addition, only identical objects should be
able to achieve maximum similarity. A similarity measure computes the similarity between two objects \cite{Weber2000}:
\begin{definition}[Similarity measure]
$\mathit{sim}: \Omega \times \Omega \mapsto [ 0, 1 ]$ is called \textnormal{similarity measure} for a set of objects $\Omega$.
\end{definition}
A similarity of $1$ implies identical objects and a similarity of $0$ implies that there are no commonalities between the input objects.

\textbf{TODO: Whole paragraph needs citations}
In ontology learning, semantic similarity is used to great effect . 
Semantic similarity compares the semantic content of objects or documents . This can be achieved by comparing which features
can be found in both objects (commonalities) and which features are unique to the compared objects (differences).
\fullcite{Rodriguez2003} develops a semantic similarity measure for comparing entity classes in ontologies.
\textbf{Write}

Similarity between vectors can be computed by calculating their distance \cite{Weber2000}.
\textbf{Write}



% Problem statement 
\subsection{Problem statement}\label{section:problem statement}
The task of this thesis is the classification of unlinked classes in Wikidata. In other words a function
is needed, which given an unlinked class $u$ of a taxonomy $T = (C, S)$ with a root class $root_T$,
find an appropriate superclass for $T$. \fullcite{Doan2002} suggests that for the task of placing a class
into an appropriate position in $T$,
either finding the most similar class, most specific superclass, or most general subclasses of $u$,
are sensible approaches.
This induces that the appropriate superclass for an unlinked class $u$ is the superclass of the most
similar class to $u$.
Therefore we can define the problem, as follows:
\begin{definition}[Problem definition]\label{problem definition}
Given a taxonomy $T = (C, S)$ with root class $root_T$ and a similarity function $sim$ over $T$,
find a function $f: \mathbb{N} \mapsto \mathcal{P}(\mathbb{N})$, 
which, given an unlinked class $u \in C$, returns a set of classes $P = f(u)$, 
fulfilling the following criteria:
\begin{align}
& \neg(\forall p \in S: p \subclassof{T} u \: \textnormal{no children} \label{no children}\\
& = \mathit{succ}(\underset{c \in C}{\max(sim(u, s))}) \: \textnormal{superclasses of most similar classes} 
\label{most similar class}
\end{align}
\end{definition}
Because it is possible for a class to have multiple superclasses, it is necessary to define $f$ in such a way,
that it returns a set of classes. Therefore the described problem is a multi-label classification problem.

The stated problem induces several challenges, which will be listed here, but addressed in later sections.

\begin{enumerate}
\item \textbf{Multi-label classification}. Algorithms for classification typically map entered objects to one label 
\textbf{TODO:Reference}.
It has to be decided whether the solution will be simplified to assign one superclass per input,
or attempt multi-label classification.
\item \textbf{High number of labels}. An entered, unlinked class has to be assigned to a class in the root taxonomy.
The only restricting condition is that the chosen class cannot be a subclass of the unlinked class.
As shown in Section~\ref{section:taxonomy analysis} $97\%$ of $12999501$ classes are part of the root
taxonomy. Classification like SVM or neural networks are usually used for classification with a small number of labels.
SVM is for example a binary classification method. A classification method, which is able to handle over a million
of labels, has to be found.
\item \textbf{Representation of items}. Items in Wikidata are structured information, similar to nodes in RDF graphs.
Items are nodes and statements can be simplified to triples of $(\mathit{itemid}, \mathit{pid}, \mathit{value}$.
As motivated in Section~\ref{section:introduction}, the solution should exploit the apparent power of
neural networks. Neural networks, which are introduced in Section~\ref{section:neural networks},
require input to be represented as vectors. Therefore it will be necessary to map items to vectors.
\end{enumerate}

% Similarity-based classification
\subsection{k-nearest-neighbors classification}\label{section:knn}
Based on the characteristics of the classification problem, described by the problem statement, and the challenges attached to it,
the k-nearest-neighbor algorithm (kNN) seems like an appropriate tool for solving the task.
Nearest-neighbors classification is a lazy method, as it does not require training before testing.
This is useful for applications with high amounts of data, large numbers of classes, and changing data \cite{Zhang2005} \cite{Chen2009}.
For the considered use case of classification in Wikidata, these are very important strengths,
as the number of classes in the taxonomy is very high and Wikidata is being constantly edited.

k-nearest-neighbors can be defined as a similarity-based classification method.
kNN uses a pairwise similarity or distance measure (see Section~\ref{section:similarity}).
Access to the features of the classified objects is therefore not required \cite{Chen2009}.

\begin{figure}
\centering
\includegraphics[width=0.5\textwidth]{images/foundations/knn_example.pdf}
\caption{Example for k-nearest neighbors for 3 classes with k=4 and k=10.}
\label{fig:kNN example}
\end{figure}

The basic notion of k-nearest-neighbors is presented in Figure~\ref{fig:kNN example}.
Given a set of points in $\mathbb{R}^2$ with classes blue, red and yellow. 
To classify an unknown class $u$, the closest $k$ classes are selected.
In the example, the solid circle indicates $k=4$ and the dashed circle indicates $k=10$.
The $k$ selected neighbors vote for their own class with a given weight, which is typically dependent on its similarity to $u$.
In this simple example, it is assumed that all points have a uniform weight.
Following, for $k=4$ $u$ is classified as yellow and for $k=10$ as red.

In comparison to the example, the weights assigned to each neighbor are not uniform.
Weights are assigned, so that similar neighbors are given larger weights (affinity).
Because in practical applications many neighbors may be very similar to each other, which can skew the classification results, 
\fullcite{Chen2009} propose to additionally down-weight very similar neighbors (diversity).
This can be accomplished by using kernel ridge interpolation (KRI) \cite{Chen2009}. The following quadratic programming has to be solved to calculate the weights:
\begin{align}\label{def:kri knn}
\begin{split}
& \min_{w} \frac{1}{2} w^T S w - s^T w + \frac{\lambda}{2} w^T w \\
& \textnormal{subject to} \: \sum_{i=1}^{k} w_i = 1, w_i \ge 0, i=1,\dots,k,
\end{split}
\end{align}
where $w \in \mathbb{R}^{k \times 1}$ the weights of the nearest neighbors,
 $S \in [0,1]^{k \times k}$ is the similarity matrix between the nearest neighbors of $u$, $s \in [0,1]^{k \times 1}$ is the similarity between $u$ and its nearest neighbors,
$\lambda > 0$ is a regularization parameter.
It is common to assume that weights are positive and the sum of weights is $1$.
Minimizing $- s^T w$ alone would return all weight on the nearest neighbor with $w_1 = 1$.
The ridge regularization term $\frac{\lambda}{2} w^T w$, which controls the variance of the weights in $w$, counteracts this by pushing the weights closer to uniform weights.
Increasing $\lambda$ increases the uniformity of the weights. Together these terms fulfill the affinity requirement.
The term $ \frac{1}{2} w^T S w$ represents the diversity requirement, as the weights of very similar neighbors are down-weighted to minimize the term.
Solving this problem therefore returns weights, which fulfill the requirements of affinity and diversity.
Experiments show that KRI kNN on average generates better results than uniformly weighted kNN \cite{Chen2009}.

% Wikidata
\subsection{Wikidata}\label{section:wikidata}
Wikidata is an open, free, multilingual and collaborative KB. It is a structured knowledge source for other Wikimedia projects. 
It tries to model the real world, meaning every concept, object, animal, person, etc.
Wikidata is mostly edited and extended by humans, which in general improves the quality of entries compared to fully-automated systems, 
because different editors can validate and correct occurring errors.
However, Wikidata, like most knowledge bases, is incomplete and therefore has to be operated under the \textbf{Open World Assumption (OWA)}.
OWA states that if a statement is not contained in a knowledge base, it is not necessarily false but rather unknown \cite{Galarraga2016}.

In Wikidata \textbf{items} and \textbf{properties} exist. Items are the aforementioned concepts, objects, etc. Properties are used to make claims about
items, e.g. \textit{photographic film (Q6293)} is a \textit{subclass of (P279)} \textit{data storage device (Q193395)} (see Figure~\ref{fig:class example}).
Each item and property has an unique identifier, which starts with the letter Q for items and the letter P for properties and is followed by a numeric code.
The identifiers in Wikidata are essential to avoid ambiguity and make items and properties multilingual.

Items consist of labels, aliases and descriptions in different languages. 
Sitelinks connect items to their corresponding pages of Wikimedia projects like Wikipedia articles.
Most importantly item are described by \textbf{statements}. 
Statements are in their simplest form a pair of property and value, assigned to a specific item. A value is either a literal value or another item.
It should be noted that an item can have multiple statements with the same property. The set of statements with the same property is called statement group.
Statements can be annotated with qualifiers, which specify the context of the statement, e.g. population at a certain point of time.
Additionally, references can be used for statements to include its source. See Figure~\ref{fig:class example} for an example of a Wikidata item.

\begin{figure}
\centering
\includegraphics[width=0.9\textwidth]{images/foundations/item_example.png}
\caption{Example of Wikidata class: photographic film (Q6239)}
\label{fig:class example}
\end{figure}

Following, the terms of item and statement are defined in the context of Wikidata.
\begin{definition}[Item]\label{definition:item}
An \textnormal{item} is a tuple $(\mathit{id},  \mathit{label}, \mathit{aliases}, \mathit{description}, \mathit{sitelinks})$:
	\begin{itemize}
	\item $\mathit{id} \in \mathbb{N}$ is the numerical item ID;
	\item $\mathit{label} \in \mathit{String}$ is the English label of the item;
	\item $\mathit{aliases} \in \mathcal{P}(\mathit{String})$ is the set of English synonyms for the label;
	\item $\mathit{description} \in \mathit{String}$ is a short sentence describing the item;
	\item $\mathit{sitelinks} \in \mathit{String} \times \mathit{String}$ is a set of tuples $(\mathit{site}, \mathit{title})$, where $\mathit{site}$ refers to a specific site of the Wikimedia
	 projects, e.g. enwiki, and $\mathit{title}$ is the corresponding article title of the item on this site.
	\end{itemize}
\end{definition}
\begin{definition}[Statement]
A \textnormal{statement} is a tuple $(\mathit{itemid}, \mathit{pid}, \mathit{value}, \mathit{refs}, \mathit{qualifiers})$:
\begin{itemize}
\item $\mathit{itemid} \in \mathbb{N}$ is a numerical item ID, to which the statement belongs;
\item $\mathit{pid} \in \mathbb{N}$ is a numerical property ID;
\item $\mathit{value}$ is either a constant value like string, int, etc., or an item ID;
\item $\mathit{refs}$ is a set of references, containing the source of information for the statement;
\item $\mathit{qualifiers}$ is a set of qualifiers, which further specifies the statement.
\end{itemize}
\end{definition}
In Wikidata, there is no strict distinction between classes and instances. Both groups are represented as items.
This leads to the issue, that recognizing, whether an item is a class or instance is not trivial.
Based on which statements connect two items, a distinction can be made.
A class is any item, which has instances, subclasses or is the subclass of another class.
In Wikidata, the properties \textit{instance of (P31)} and \textit{subclass of (P279)} exist, which describe these relations between items.
Therefore to identify whether an item is a class, it needs to be checked, whether the items fulfills any of the three mentioned criteria.
\begin{definition}[Class]
Given a set of items $I$ and a set of statements $R$. $c = (\mathit{classid}, \_, \_, \_, \_)  \in I$ is a \textnormal{class}, if at least one of the following assertions are true:
\begin{align*}
&\exists i=(\mathit{instanceid}, \_, \_, \_, \_) \in I \; \exists s=(\mathit{itemid}, \mathit{pid}, \mathit{value}, \_, \_) \in R :  \\
&\phantom{\exists i=(\mathit{instanceid}, \_, \_, \_, \_) \in I} \mathit{instanceid} = \mathit{itemid} \land \mathit{pid} = 31 \land \mathit{value} = \mathit{classid} \: 
	\textnormal{(has instance)}\\
&\exists s=(\mathit{itemid}, \mathit{pid}, \_, \_, \_) \in I: \mathit{itemid} = \mathit{classid} \land \mathit{pid} = 279 \: \textnormal{(is subclass)}\\
&\exists i=(subclassid, \_, \_, \_, \_) \in I \; \exists s=(itemid, pid, value, \_, \_) \in R : \\
&\phantom{\exists i=(\mathit{subclassid}, \_, \_, \_, \_)\in I } \mathit{itemid} = \mathit{subclassid} \land \mathit{pid} = 279 \land \mathit{value} = \mathit{classid} \: 
	\textnormal{(has subclass)}
\end{align*}
\end{definition}
$\_$ is used as an anonymous placeholder, for the purpose of not naming unused elements in tuples.
For example, \textit{photographic film (Q6293)} (Figure~\ref{fig:class example}) is a class, because it is the subclass of three other classes.

% Taxonomy
\subsection{Taxonomy}\label{section:taxonomy}

``Ontologies are (meta)data schemas, 
providing a controlled vocabulary of concepts, each with an explicitly defined and machine processable semantics'' \cite{Maedche2001}. Additionally it is possible for ontologies to
contain axioms used for validation and constraint enforcement. Ontologies enable the modeling and sharing of knowledge in a specific domain and support
the knowledge exchange via web by extending syntactic to semantic interoperability \cite{Hazman2011}.
In comparison, a KB like Wikidata can be seen as an instantiation of such an ontology,
since every KB has to be conceptualized by an ontology \cite{Wong2012}. Different types of ontologies can grouped by their level of formality and expressiveness.
\fullcite{Wong2012} differentiates ontologies as lightweight and heavyweight ontologies (see Figure~\ref{fig:ontology spectrum}). 
\textbf{Taxonomies} are concept or class hierarchies.
They typically represent a parent-child structure, which can be formalized with a single relationship called for example \textit{subclass-of} in the case of Wikidata.
The observed taxonomy in Wikidata belongs to the category of lightweight ontologies, specifically principled, informal hierarchies, 
as the only enforced rule for the subclass-of relation is that it should connect two entities \cite{WikidataP279}.

\begin{figure}
\centering
\includegraphics[width=\textwidth]{images/foundations/wong2012_spectrum_of_ontology_kinds.png}
\caption{The spectrum of ontology kinds. \cite{Wong2012}}
\label{fig:ontology spectrum}
\end{figure}

For the purpose of developing a formal definition of the thesis' problem statement the notion of taxonomy needs to be formalized.
\fullcite{Cimiano2006} defines a heavyweight ontology, which includes a taxonomy, as follows:\\
``
	\begin{definition*}[Ontology]\label{Ontology}
		An \textnormal{ontology} is a structure
		\begin{equation*} 
			\mathcal{O} := (C, \taxon, R, \relsig, \relhier, \mathcal{A}, \attsig, \mathcal{T})
		\end{equation*}
		consisting of
		\begin{itemize}
			\item four disjoint sets $C$, $R$, $\mathcal{A}$, and $\mathcal{T}$ whose elements are called \textnormal{concept identifiers}, 
				\textnormal{relation identifiers}, \textnormal{attribute identifiers} and \textnormal{data types}, respectively,
			\item a semi-upper latice $\taxon$ on $C$ with top element $\rootc$, called \textnormal{concept hierarchy} or \textnormal{taxonomy},	
			\item a function $\relsig: R \rightarrow C^+$ called \textnormal{relation signature},
			\item a partial order $\relhier$ on $R$, called \textnormal{relation hierarchy}, where $r_1 \relhier r_2$ implies $\abs{\relsig (r_1)} = \abs{\relsig (r_2)}$ and
				$\proj{i}{\relsig (r_1)} \taxon \proj{i}{\relsig (r_2)}$, for each $1 \le i \le \abs{\relsig (r_1)}$, and
			\item a function $\attsig : \mathcal{A} \rightarrow C \times \mathcal{T}$, called \textnormal{attribute signature},
			\item a set $\mathcal{T}$ of datatypes such as strings, integers, etc.
		\end{itemize}
	\end{definition*}
	Hereby, $\proj{i}{t}$ is the i-th component of tuple $t$. [...] Further, a semi-upper lattice $\le$ fulfills the following conditions:
	\begin{align*}
		&\forall x (x \le x) \: \text{(reflexive)} \\
		&\forall x \forall y (x \le y \land y \le x \implies x = y) \: \text{(anti-symmetric)} \\
		&\forall x \forall y \forall z (x \le y \land y \le z \implies x \le z) \: \text{(transitive)}\\
		&\forall x x \le top \: \text{(top element)}\\
		&\forall x \forall y \exists z (z \ge \land z \ge y \land \forall w (w \ge x \land w \ge y \implies w \ge z)) \: \text{(supremum)}
	\end{align*}
	So every two elements have a unique most specific supremum.
''\\

A taxonomy can be modeled as a semi-upper lattice. This induces two important assumptions about the structure and to some degree completeness of the
observed taxonomies. First, there is only one \textit{root class}, top element of the lattice, of which every other class is (transitively) a subclass. Second,
because of the supremum property, the taxonomy is fully connected, which means each class, but the root class, has a superclass.
Wikidata's taxonomy does therefore not fulfill the definition by \fullcite{Cimiano2006}, as it is not fully connected.\\
In the following, definitions wil bel presented, which attempt to model an incomplete taxonomy based on the already presented data model and structure of Wikidata.
Refer to Appendix~\ref{section:graphs} for the necessary definitions on graphs.

In Wikidata, a class can have multiple superclasses, therefore a tree structure is not sufficient to model the taxonomy.
However a directed acyclic graph, can model the taxonomy. The acyclic constraint is necessary to ensure that no class is transitively a subclass of itself.
\begin{definition}[Taxonomy]
A \textnormal{taxonomy} $T=(C, S)$ is a directed  acyclic graph, where $C$ is a set of \textnormal{class identifiers}, 
and $S$ is the set of edges, which describe the \textnormal{subclass-of relation}  between two classes. such that $c_1$ is the subclass of $c_2$, if $(c_1, c_2) \in S$.
\end{definition}
\begin{definition}[Subclass-of relation]\label{subclass of}
The transitive binary relation $\subclassof{T}$ on the taxonomy $T=(C, S)$ represents the subclass relationship
of two classes in $T$. Given $c_1, c_2 \in C$, $c_1 \subclassof{T} c_2$, if there is a walk $W=(c_1, \ldots, c_2)$ with length 
$n \ge 1$, which connects $c_1$ and $c_2$. \subclassof{T} is transitive,  $\forall c_1, c_2, c_3 \in C:
c_1 \subclassof{T} c_2 \land c_2 \subclassof{T} c_3 \implies c_1 \subclassof{T} c_3$.
\end{definition}
If the taxonomy defined by \fullcite{Cimiano2006} is mapped on this graph-based taxonomy model, the following
assumption is true, for $T=(C,S)$: 
\begin{align}
 \abs{\{  c \in C \mid \neg \exists s \in C: c \subclassof{T} s \}} = 1
 \end{align}
Only one class in this taxonomy has no superclasses. This class is called \textbf{root class}. However
in the case of Wikidata, this assumption does not hold true. The following state is the case:
\begin{align}
\abs{\{  c \in C \mid \neg \exists s \in C: c \subclassof{T} s \}} > 
\end{align}
There are classes other than the root class, which also have no superclasses. These classes will be called orphan classes.
\begin{definition}[Root class]\label{root class}
Given a taxonomy $T=(C, S)$, the \textnormal{root class} $root_T$ is a specific, predefined class with no superclasses in 
$T$. For $root_T$, $\abs{succ_T(root_T)} = 0$ applies.
\end{definition}
\begin{definition}[Orphan class]\label{orphan class}
Given a taxonomy $T=(C,S)$ with a root class $root_T$, a class $u \in C$ is called \textnormal{orphan class},
if $u \neq root_T \land \abs{succ_T(u)} = 0$.
\end{definition}
In Wikidata, the root class is \entity{} \cite{WikidataQ35120}. 
All other classes, which are not subclasses of \entity{}, are therefore either orphan classes, or subclasses of orphan classes.
In Section~\ref{section:taxonomy analysis}, it will be shown that the Wikidata taxonomy graph is not fully
connected. But the component, which contains the root class \entity{}, contains $97\%$ of all classes.
This component will be referred to as \textbf{root taxonomy} in later sections.

% Similarity
\subsection{Similarity}\label{section:similarity}
For the task of ontology learning \cite{Hazman2011} as well as classification, e.g. k-nearest-neighbors \cite{Chen2009},
the concept of \textbf{similarity} is of importance. A basic intuition of similarity is for example given by 
\fullcite{Lin1998}. Similarity is related to the commonalities and differences between two objects.
More commonalities implies higher similarity. Vice versa, more differences implies lower similarity.
Two identical objects should have the maximum similarity. In addition, only identical objects should be
able to achieve maximum similarity. A \textbf{similarity measure} computes the similarity between two objects \cite{Weber2000}:
\begin{definition}[Similarity measure]
$\mathit{sim}: \Omega \times \Omega \mapsto [ 0, 1 ]$ is called \textnormal{similarity measure} for a set of objects $\Omega$.
\end{definition}
A similarity of $1$ implies identical objects and a similarity of $0$ implies that there are no commonalities between the input objects. The information-theoretic definition of similarity states that a similarity measure
should factor in both commonalities as well as differences to compute the similarity between objects \cite{Lin1998}.

Many types of information can be encoded as feature vectors in $\mathbb{R}^n$, e.g.
words \cite{Levy2015} \cite{Mikolov2013}, graphs \cite{Ristoski2016} \cite{Cao2016}, etc.
The similarity between feature vectors can be measured using the distance $\delta_S(\vec{p}, \vec{q})$ 
between two vectors $\vec{p}$ and $\vec{q}$. 
The distance represents the differences between two feature vectors. 
A typical distance measure is the \textbf{$L_S$-Norm} \cite{Weber2000}:
\begin{align}
&L_s: \delta_S (\vec{p}, \vec{q}) = \sqrt[S]{\sum_{i=0}^{n-1}\abs{\vec{p}_i - \vec{q}_i}^S}
\end{align}
For example, the $L_2$-Norm is the euclidean distance:
\begin{align}
&L_2: \delta_2 (\vec{p}, \vec{q}) = \sqrt{\sum_{i=0}^{n-1}\abs{\vec{p}_i - \vec{q}_i}^2}
\end{align}
Similarity between vectors can now be defined by mapping the distances to similarity values using a correspondence
function \cite{Weber2000}:
\begin{definition}[Correspondence function]
$h: \mathbb{R}^+ \mapsto [0,1]$ is correspondence function, if it fulfills the following properties:
\begin{enumerate}
\item $h(0), h(\infty) = 0$,
\item $\forall x,y: x > y \implies h(x) \le h(y)$
\end{enumerate}
\end{definition}
\fullcite{Aggarwal2001} show that $L_S$ distance measures especially suffer the curse of dimensionality for
increasing values of $S$. The curse of dimensionality implies that
''the ratio of the distances of the nearest and farthest neighbors to a given target in high dimensional space 
is almost $1$ for a wide variety of data distributions and distance functions'' \cite{Aggarwal2001}.
\textbf{Cosine similarity} can offset this problem to some degree by including the direction of the vectors
into the calculation of the similarity \cite{Houle2010}. 
This is done by computing the dot product between two vector. It is defined as follows:
\begin{align}
& \mathit{sim}_\mathit{cos}(\vec{p}, \vec{q}) = 
\frac{\vec{p}^T \cdot \vec{q}}{||\vec{p}||_2 \cdot ||\vec{q}||_2}
\end{align}  
Cosine similarity maps to $[-1,1]$. It returns $1$ if the vectors are identical, $0$ if the vectors are orthogonal,
and $-1$ if the angle between the vectors is $\pi$ and therefore the vectors are opposites.
However, if the feature vector space $\Omega$ contains only positive vectors, 
$\forall \vec{p} \in \Omega: \vec{p}_i \ge 0 \forall i=0,\dots,n-1$,
then the cosine similarity maps to $[0, 1]$ and fulfills the definition of a similarity measure.
For example, feature vectors representing word frequencies in documents are positive,
as a word can not occur less than $0$ times.

In ontology learning, \textbf{semantic similarity} is used to great effect, 
e.g  for clustering objects to create hierarchies\cite{Hazman2011} \cite{Wong2012} 
or mapping between different ontologies  \cite{Doan2002} \cite{Rodriguez2003}. 
Semantic similarity compares the semantic content of objects or documents . 
This can be achieved by comparing which features can be found in both objects (commonalities) 
and which features are unique to the compared objects (differences).
\fullcite{Rodriguez2003} develops a semantic similarity measure for comparing entity classes in ontologies.
Given two objects $a, b \in \Omega$. $A$ and $B$ are their corresponding descriptions sets,
e.g. for Wikidata the aliases and statements. $\alpha$ is a function, which defines the importance of
differences between $a$ and $b$. $A \cap B$ is the set of commonalities, and $A/B$ the set of differences between
the $a$ and $b$. The defined similarity function is not symmetric, $\mathit{sim}(a,b) \neq \mathit{sim}(a,b)$.
\begin{align}
&\mathit{sim}(a, b) = \frac{\abs{A \cap B}}{\abs{A \cap B} + \alpha (a,b) \abs{A/B} + (1 - \alpha (a,b)) \abs{B/A}}
\end{align}
for $0 \le \alpha \le 1$.

Calculating the similarity between vectors is an easier task than calculating the similarity between objects in
an ontology. Using neural word or graph embeddings, which are presented in Sections~\ref{section:word2vec}
and~\ref{section:graph embeddings}, enables the representation of classes and instances in ontologies
as feature vectors. The mentioned curse of dimensionality is a non-issue as it applies, if the the number
of irrelevant of features is high. This is typically solved by reducing the dimension of the feature vectors
to include only relevant features \cite{Domingos2012}. 
Neural embeddings however seem to implicitly capture only relevant features \cite{Mikolov2013}.

% Problem statement 
\subsection{Problem statement}\label{section:problem statement}
The task of this thesis is the classification of orphan classes in Wikidata. In other words a function is needed, 
which given an orphan class $u$ of a taxonomy $T = (C, S)$ with a root class $root_T$, find an appropriate superclass for $T$.

\fullcite{Doan2002} suggests that for the task of placing a class into an appropriate position in $T$,
either finding the most similar class, most specific superclass, or most general subclasses of $u$, are sensible approaches.
Therefore a ''most-specific-parent" similarity measure $\mathit{sim}_\mathit{msp}(a, b)$ is assumed, which defines similarity between a subclass $a$ and a superclass $b$.
$\mathit{sim}_\mathit{msp}(a, b)$ is $0$, if $b$ is not a superclass of $a$.

The problem is defined as follows:
\begin{definition}[Problem definition]\label{problem definition}
Given a taxonomy $T = (C, S)$ with root class $root_T$ and a similarity function $sim$ over $T$,
find a function $f: \mathbb{N} \mapsto \mathcal{P}(\mathbb{N})$, 
which, given an orphan class $u \in C$, returns a class $s = f(u)$, 
fulfilling the following criteria:
\begin{align}
& \forall p \in P: \neg (p \subclassof{T} u) \: \textnormal{no children} \label{no children}\\
& s =\max_{s \in C}(\textit{sim}_\textit{msp}(u, c)) \: \textnormal{most specific parent} 
\label{most similar class}
\end{align}
\end{definition}

The stated problem induces several challenges, which will be listed here, but addressed in later sections.

\begin{enumerate}
\item \textbf{Multi-label classification}. Algorithms for classification typically map entered objects to one label.
In Wikidata and other ontologies, it is possible for one class to have multiple superclasses.
It has to be decided whether the solution will be simplified to a single-label classification,
or attempt multi-label classification.
\item \textbf{High number of labels}. An orphan class has to be assigned to a class in the root taxonomy.
The only restricting condition is that the chosen class cannot be a subclass of the orphan class.
As shown in Section~\ref{section:taxonomy analysis} $97\%$ of $12999501$ classes are part of the root
taxonomy. Classification methods, like SVM or neural networks, usually classify with a small number of labels.
A classification method, which is able to handle over a million labels, is required.
\item \textbf{Representation of items}. Items in Wikidata are structured information, similar to nodes in RDF graphs
and Wikidata can be represented as such \cite{Ristoski2016} \cite{Erxleben2014}.
As motivated in Section~\ref{section:introduction}, the solution should exploit the apparent power of
neural networks. Neural networks, which are introduced in Section~\ref{section:neural networks},
require input to be represented as vectors. Therefore it will be necessary to map items to vectors.
\end{enumerate}

% Similarity-based classification
\subsection{k-nearest-neighbors classification}\label{section:knn}
Based on the characteristics of the classification problem, described by the problem statement, and the challenges attached to it,
the \textbf{k-nearest-neighbor algorithm (kNN)} seems like an appropriate tool for solving the task.
Nearest-neighbors classification is a lazy method, as it does not require training before testing.
This is useful for applications with high amounts of data, large numbers of classes, and changing data \cite{Zhang2005} \cite{Chen2009}.
For the considered use case of classification in Wikidata, these are very important strengths,
as the number of classes in the taxonomy is very high and Wikidata is being constantly edited.

kNNs can be defined as a similarity-based classification method.
kNN uses a pairwise similarity or distance measure (see Section~\ref{section:similarity}).
Access to the features of the classified objects is therefore not required \cite{Chen2009}.

\begin{figure}
\centering
\includegraphics[width=0.5\textwidth]{images/foundations/knn_example.pdf}
\caption{Example for k-nearest neighbors for 3 classes with k=4 and k=10.}
\label{fig:kNN example}
\end{figure}

The basic notion of kNN is presented in Figure~\ref{fig:kNN example}.
Given a set of points in $\mathbb{R}^2$ with classes blue, red and yellow. 
To classify an unknown class $u$, the closest $k$ classes are selected.
In the example, the solid circle indicates $k=4$ and the dashed circle indicates $k=10$.
The $k$ selected neighbors vote for their own class with a given weight, which is typically dependent on its similarity to $u$.
In this simple example, it is assumed that all points have a uniform weight.
Following, for $k=4$ $u$ is classified as yellow and for $k=10$ as red.

In comparison to the example, the weights assigned to each neighbor are not uniform.
Weights are assigned, so that similar neighbors are given larger weights (affinity).
Because in practical applications many neighbors may be very similar to each other, which can skew the classification results, 
\fullcite{Chen2009} propose to additionally down-weight very similar neighbors (diversity).
This can be accomplished by using kernel ridge interpolation (KRI) \cite{Chen2009}. The following quadratic programming has to be solved to calculate the weights:
\begin{align}\label{def:kri knn}
\begin{split}
& \min_{w} \frac{1}{2} w^T S w - s^T w + \frac{\lambda}{2} w^T w \\
& \textnormal{subject to} \: \sum_{i=1}^{k} w_i = 1, w_i \ge 0, i=1,\dots,k,
\end{split}
\end{align}
where $w \in \mathbb{R}^{k \times 1}$ the weights of the nearest neighbors,
 $S \in [0,1]^{k \times k}$ is the similarity matrix between the nearest neighbors of $u$, $s \in [0,1]^{k \times 1}$ is the similarity between $u$ and its nearest neighbors,
$\lambda > 0$ is a regularization parameter.
It is common to assume that weights are positive and the sum of weights is $1$.
Minimizing $- s^T w$ alone would return all weight on the nearest neighbor with $w_1 = 1$.
The ridge regularization term $\frac{\lambda}{2} w^T w$, which controls the variance of the weights in $w$, counteracts this by pushing the weights closer to uniform weights.
Increasing $\lambda$ increases the uniformity of the weights. Together these terms fulfill the affinity requirement.
The term $ \frac{1}{2} w^T S w$ represents the diversity requirement, as the weights of very similar neighbors are down-weighted to minimize the term.
Solving this problem therefore returns weights, which fulfill the requirements of affinity and diversity.
Experiments show that KRI-kNN on average generates better results than uniformly or distance-weighted kNN \cite{Chen2009}.

%%%%%%
% Wikidata
%%%%%%
\subsection{Wikidata}\label{section:wikidata}
Wikidata is an open, free, multilingual and collaborative knowledge base (KB). It is a structured knowledge source for other Wikimedia projects. 
It models the real world, such as concepts, objects, people, events, etc.
Wikidata is mostly edited and extended by humans, which in general improves the quality of entries compared to fully-automated systems, 
because different editors can validate and correct occurring errors.
However, Wikidata, as an continuously growing KB, is incomplete in terms of missing facts about different entities \cite{Darari2016}.
Due to the inherent incompleteness of Wikidata, as it is never possible to completely model all information, it is operated under the \textbf{Open World Assumption (OWA)}.
OWA states that if a statement is not contained in a knowledge base, it is not necessarily false but rather unknown \cite{Galarraga2016}.

In Wikidata \textbf{items} and \textbf{properties} exist. Items are the aforementioned concepts, objects, etc. Properties are used to make claims about
items, e.g. \textit{photographic film (Q6293)} is a \textit{subclass of (P279)} \textit{data storage device (Q193395)} (see Figure~\ref{fig:class example}).
Each item and property has an unique identifier, which starts with the letter Q for items and the letter P for properties and is followed by a numeric code.
The identifiers in Wikidata are essential to avoid ambiguity and make it possible to refer to interact with Wikidata independent of language constraints,
which is a supporting factor in allowing the multilingualism of Wikidata.

Items are described by  labels, aliases and descriptions in different languages. 
Sitelinks connect items to their corresponding pages of Wikimedia projects like Wikipedia articles.
Most importantly, an item is described by \textbf{statements}. 
Statements are in their simplest form a pair of property and value, assigned to a specific item. A value is either a literal value or another item.
It should be noted that an item can have multiple statements with the same property. The set of statements with the same property is called statement group.
Statements can be annotated with qualifiers, which specify the context of the statement, e.g. population at a certain point of time.
Additionally, references can be used for statements to include its source. 

Figure~\ref{fig:class example} shows an example for a Wikidata item.
The item has the \textbf{label} "photographic film" and the \textbf{unique identifier} "Q6239".
Below the label and identifier, the \textbf{description} (short sentence describing the item) can be found.
The \textbf{alias} "film" follows in the next line. An item can have multiple aliases.
Statements are grouped into \textbf{statement groups} (blue boxes) by their \textbf{property}.
In this example, two statement groups with properties \textit{topic's main category (P910)} and \textbf{subclass of (P279)} are shown.
Each \textbf{statement} can have a value (orange box), \textbf{references} (green box), and \textbf{qualifiers}.
In this example, the only statement with property \textit{topic's main category (P910)} has the value \textit{Category: Photographic film (Q8363301)} and a reference, which consists of
a property \textit{imported from (P143)} and a value \textit{Chinese Wikipedia (Q30239)}. References refer to the sources and other meta-data about the source of the statement.
In this example, no qualifiers exist. Qualifiers are used to give additional details to statements.
A common use for qualifiers is the annotation of statements with time stamps, if the a different value is true for different points in time e.g. population of a country over time.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{images/foundations/item_example.png}
\caption{Example excerpt of Wikidata item: photographic film (Q6239). An item consists of a label, aliases,
description, and statement group. Each statement group is associated with a specific property such as 
\textit{subclass of (P270)} and can have multiple statements. The provenance of a statement can be
signified by one or multiple references.}
\label{fig:class example}
\end{figure}

Following, the concepts of item and statement are formally defined:
\begin{definition}[Item]\label{definition:item}
An \textbf{item} is a tuple $(\mathit{id},  \mathit{label}, \mathit{aliases}, \mathit{description}, \mathit{sitelinks})$:
	\begin{itemize}
	\item $\mathit{id} \in \mathbb{N}$ is the numerical item ID;
	\item $\mathit{label} \in \mathit{String}$ is the English label of the item;
	\item $\mathit{aliases} \in \mathcal{P}(\mathit{String})$ is the set of English synonyms for the label;
	\item $\mathit{description} \in \mathit{String}$ is a short sentence describing the item;
	\item $\mathit{sitelinks} \in \mathit{String} \times \mathit{String}$ is a set of tuples $(\mathit{site}, \mathit{title})$, where $\mathit{site}$ refers to a specific site of the Wikimedia
	 projects, e.g. enwiki, and $\mathit{title}$ is the corresponding article title of the item on this site.
	\end{itemize}
\end{definition}

For example, the item \textit{photographic film (Q6239)} (Figure~\ref{fig:class example}) can be formally represented, as follows:
\begin{align*}
&(\\
&6239,\\
&\textnormal{"photographic film"},\\
&\{ \textnormal{"film"} \},\\
&\textnormal{"sheet of plastic coated with light-sensitive chemicals"},\\
&\{ (\textnormal{"enwiki"}, \textnormal{"Photographic film"}), (\textnormal{"dewiki"}, \textnormal{"Fotografischer Film"}), \dots \}\\
)
\end{align*}

\begin{definition}[Statement]
A \textbf{statement} is a tuple $(\mathit{itemid}, \mathit{pid}, \mathit{value}, \mathit{refs}, \mathit{qualifiers})$:
\begin{itemize}
\item $\mathit{itemid} \in \mathbb{N}$ is a numerical item ID, to which the statement belongs;
\item $\mathit{pid} \in \mathbb{N}$ is a numerical property ID;
\item $\mathit{value}$ is either a constant value like string, int, etc., or an item ID;
\item $\mathit{refs}$ is a set of references, containing the source of information for the statement;
\item $\mathit{qualifiers}$ is a set of qualifiers, which further specifies the statement.
\end{itemize}
\end{definition}

The statement with property \textit{topic's main category (P910)} of the item \textit{photographic film (Q6239)} can be formalized:
\begin{align*}
&(6239, 910, \textnormal{Q30239}, \{ \dots \}, \varnothing)
\end{align*}

In Wikidata, there is no strict distinction between classes and instances. Both groups are represented as items.
This leads to the issue, that recognizing, whether an item is a class or instance is not trivial.
Based on which statements connect two items, a distinction can be made.
A class is any item, which has instances, subclasses or is the subclass of another class.
In Wikidata, the properties \textit{instance of (P31)} and \textit{subclass of (P279)} exist, which describe these relations between items.
Therefore to identify whether an item is a class, it needs to be checked, whether the items fulfills any of the three mentioned criteria.
\begin{definition}[Class]\label{def:class}
Given a set of items $I$ and a set of statements $R$. $c = (\mathit{classid}, \_, \_, \_, \_)  \in I$ is a \textbf{class}, if at least one of the following assertions are true:
\begin{align*}
&\exists i=(\mathit{instanceid}, \_, \_, \_, \_) \in I \; \exists s=(\mathit{itemid}, \mathit{pid}, \mathit{value}, \_, \_) \in R :  \\
&\phantom{\exists i=(\mathit{instanceid}, \_, \_, \_, \_) \in I} \mathit{instanceid} = \mathit{itemid} \land \mathit{pid} = 31 \land \mathit{value} = \mathit{classid} \: 
	\textnormal{(has instance)}\\
&\exists s=(\mathit{itemid}, \mathit{pid}, \_, \_, \_) \in I: \mathit{itemid} = \mathit{classid} \land \mathit{pid} = 279 \: \textnormal{(is subclass)}\\
&\exists i=(subclassid, \_, \_, \_, \_) \in I \; \exists s=(itemid, pid, value, \_, \_) \in R : \\
&\phantom{\exists i=(\mathit{subclassid}, \_, \_, \_, \_)\in I } \mathit{itemid} = \mathit{subclassid} \land \mathit{pid} = 279 \land \mathit{value} = \mathit{classid} \: 
	\textnormal{(has subclass)}
\end{align*}
\end{definition}
$\_$ is used as an anonymous placeholder, for the purpose of not naming unused elements in tuples.
For example, \textit{photographic film (Q6293)} (Figure~\ref{fig:class example}) is a class, because it is the subclass of three other classes.

%%%%%%%
% Taxonomy
%%%%%%%
\subsection{Taxonomy}\label{section:taxonomy}

``Ontologies are (meta)data schemas, 
providing a controlled vocabulary of concepts, each with an explicitly defined and machine processable semantics'' \cite{Maedche2001}. Additionally it is possible for ontologies to
contain axioms used for validation and constraint enforcement. Ontologies enable the modeling and sharing of knowledge in a specific domain and support
the knowledge exchange via web by extending syntactic to semantic interoperability \cite{Hazman2011}.
In comparison, a KB like Wikidata can be seen as an instantiation of such an ontology,
since every KB has to be conceptualized by an ontology \cite{Wong2012}. Different types of ontologies can be grouped by their level of formality and expressiveness.
\fullcite{Wong2012} differentiates ontologies as lightweight and heavyweight ontologies (see Figure~\ref{fig:ontology spectrum}). 
\textbf{Taxonomies} are concept or class hierarchies.
They typically represent a parent-child structure, which can be formalized with a single relationship called for example \textit{subclass-of} in the case of Wikidata.
The observed taxonomy in Wikidata belongs to the category of lightweight ontologies, specifically principled, informal hierarchies, 
as the only enforced rule for the subclass-of relation is that it should connect two entities \cite{WikidataP279}.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{images/foundations/wong2012_spectrum_of_ontology_kinds.png}
\caption{The spectrum of ontology kinds (extracted from \cite{Wong2012})}
\label{fig:ontology spectrum}
\end{figure}

For the purpose of developing a formal definition of the thesis' problem statement the notion of taxonomy needs to be formalized.
\citeauthor{Cimiano2006} defines a heavyweight ontology, which includes a taxonomy, as follows \cite{Cimiano2006}:
\newpage
\begin{mdframed}
\begin{quotation}
	\begin{definition*}[Ontology]\label{Ontology}
		An \textbf{ontology} is a structure
		\begin{equation*} 
			\mathcal{O} := (C, \taxon, R, \relsig, \relhier, \mathcal{A}, \attsig, \mathcal{T})
		\end{equation*}
		consisting of
		\begin{itemize}
			\item four disjoint sets $C$, $R$, $\mathcal{A}$, and $\mathcal{T}$ whose elements are called \textnormal{concept identifiers}, 
				\textnormal{relation identifiers}, \textnormal{attribute identifiers} and \textnormal{data types}, respectively,
			\item a semi-upper latice $\taxon$ on $C$ with top element $\rootc$, called \textnormal{concept hierarchy} or \textnormal{taxonomy},	
			\item a function $\relsig: R \rightarrow C^+$ called \textnormal{relation signature},
			\item a partial order $\relhier$ on $R$, called \textnormal{relation hierarchy}, where $r_1 \relhier r_2$ implies $\abs{\relsig (r_1)} = \abs{\relsig (r_2)}$ and
				$\proj{i}{\relsig (r_1)} \taxon \proj{i}{\relsig (r_2)}$, for each $1 \le i \le \abs{\relsig (r_1)}$, and
			\item a function $\attsig : \mathcal{A} \rightarrow C \times \mathcal{T}$, called \textnormal{attribute signature},
			\item a set $\mathcal{T}$ of datatypes such as strings, integers, etc.
		\end{itemize}
	\end{definition*}
	Hereby, $\proj{i}{t}$ is the i-th component of tuple $t$. [...] Further, a semi-upper lattice $\le$ fulfills the following conditions:
	\begin{align*}
		&\forall x (x \le x) \: \text{(reflexive)} \\
		&\forall x \forall y (x \le y \land y \le x \implies x = y) \: \text{(anti-symmetric)} \\
		&\forall x \forall y \forall z (x \le y \land y \le z \implies x \le z) \: \text{(transitive)}\\
		&\forall x x \le top \: \text{(top element)}\\
		&\forall x \forall y \exists z (z \ge \land z \ge y \land \forall w (w \ge x \land w \ge y \implies w \ge z)) \: \text{(supremum)}
	\end{align*}
	So every two elements have a unique most specific supremum.
\end{quotation}
\end{mdframed}

This definition by \fullcite{Cimiano2006} can be mapped to Wikidata.
In the Wikidata context, $C$ is the set of classes (see Definition~\ref{def:class} (Class)), $R$ is the set of properties, which target other items,
and $A$ is the set of properties, which target literals like strings.

As shown in the preceding definition by \fullcite{Cimiano2006}, a taxonomy can be modeled as a semi-upper lattice $\taxon$. 
This induces two important assumptions about the structure and to some degree completeness of the
observed taxonomies.
 First, there is only one \textit{root class}, top element of the lattice, of which every other class is (transitively) a subclass. 
Second, because of the supremum property, the taxonomy is fully connected, which implies that every class has at least one root class,
excluding the root class $\rootc$.
Wikidata's taxonomy does therefore not fulfill the definition, as it is not fully connected.

In the following, definitions will be presented, which attempt to model an incomplete taxonomy based on the already presented data model and structure of Wikidata.
Refer to Appendix~\ref{section:graphs} for the necessary definitions on graphs.

In Wikidata, a class can have multiple superclasses, therefore a tree structure is not sufficient to model the taxonomy.
However a \textbf{directed acyclic graph (DAG)}, can model the taxonomy. 
The acyclic constraint is necessary to ensure that no class is transitively a subclass of itself.
\begin{definition}[Taxonomy]\label{def:taxonomy}
A \textbf{taxonomy} $T=(C, S)$ is a DAG, where $C$ is a set of \textnormal{class identifiers}, 
and $S$ is the set of edges, which describe the \textnormal{subclass-of relation}  between two classes. such that $c_1$ is the subclass of $c_2$, if $(c_1, c_2) \in S$.
\end{definition}
\begin{definition}[Subclass-of relation]\label{subclass of}
The transitive binary relation $\subclassof{T}$ on the taxonomy $T=(C, S)$ represents the transitive subclass relationship of two classes in $T$. 
Given $c_1, c_2 \in C$, $c_1 \subclassof{T} c_2$ applies, if there is a walk $W=(c_1, \ldots, c_2)$ with length  $n \ge 1$, which connects $c_1$ and $c_2$. 
\subclassof{T} is transitive,  $\forall c_1, c_2, c_3 \in C: c_1 \subclassof{T} c_2 \land c_2 \subclassof{T} c_3 \implies c_1 \subclassof{T} c_3$.
\end{definition}
If the taxonomy defined by \fullcite{Cimiano2006} is mapped on this graph-based taxonomy model, the following
assumption is true, for $T=(C,S)$: 
\begin{align}
 \abs{\{  c \in C \mid \neg \exists s \in C: c \subclassof{T} s \}} = 1
 \end{align}
Only one class in this taxonomy has no superclasses. This class is called \textbf{root class}. However
in the case of Wikidata, this assumption does not hold true. The following state is the case:
\begin{align}
\abs{\{  c \in C \mid \neg \exists s \in C: c \subclassof{T} s \}} \ge 1
\end{align}
There are classes other than the root class, which also have no superclasses. These classes will be called orphan classes.
\begin{definition}[Root class]\label{root class}
Given a taxonomy $T=(C, S)$, the \textbf{root class} $root_T$ is a specific, predefined class with no superclasses in 
$T$. For $root_T$, $\abs{succ_T(\mathit{root}_T)} = 0$ applies.
\end{definition}
\begin{definition}[Orphan class]\label{orphan class}
Given a taxonomy $T=(C,S)$ with a root class $\mathit{root}_T$, a class $u \in C$ is called \textbf{orphan class},
if $u \neq \mathit{root}_T \land \abs{\mathit{succ}_T(u)} = 0$.
\end{definition}
In Wikidata, the root class is \entity{} \cite{WikidataQ35120}. 
All other classes, which are not subclasses of \entity{}, are therefore either orphan classes, or subclasses of orphan classes.
In Chapter~\ref{section:taxonomy analysis}, it is shown that $97\%$ of all classes are subclasses of the root class \entity{}.
This set $R = \{ c \in C | c \subclassof{T} \mathit{root}_T \lor c = \mathit{root}_T \}$ will be referred to as \textbf{root taxonomy} in later sections.

In Figure~\ref{fig:taxonomy dag} the defined concepts of taxonomy, subclass-of relation, root class, orphan class
and root taxonomy are illustrated.
A \textbf{taxonomy} $T = (C, S)$ with \textbf{root class} $root_T = 1$ can be defined as DAG (see Definition~\ref{def:taxonomy}).
Each node represents a class with its class identifier.
The directed edges represent the \textbf{subclass-of} relation between the classes.
Node $1$ is the root of the taxonomy. 
All gray-colored nodes are connected to $root_T$ and therefore part of the \textbf{root taxonomy}.
It can be seen that the white-colored classes $7$ and $8$ are not connected to $root_T = 1$, therefore they do not belong to the root taxonomy.
Class $7$ is an \textbf{orphan class}, since it is not the root class and has no outgoing edges.

\begin{figure}[H]
\centering
\includegraphics[width=0.5\textwidth]{images/foundations/taxonomy_dag_example.pdf}
\caption{Example: Taxonomy DAG with root class $1$ and orphan class $7$. The classes $7$ and $8$ don't belong
to the root taxonomy.}
\medskip
\label{fig:taxonomy dag}
\end{figure}

%%%%%%
% Similarity
%%%%%%
\subsection{Similarity}\label{section:similarity}
For the task of ontology learning \cite{Hazman2011} as well as classification, e.g. k-nearest-neighbors and SVM \cite{Chen2009}, the concept of \textbf{similarity} is of importance. 
A \textbf{similarity measure} computes the similarity between two objects \cite{Weber2000}:
\begin{definition}[Similarity measure]
$\mathit{sim}: \Omega \times \Omega \mapsto [ 0, 1 ]$ is called \textbf{similarity measure} for a set of objects $\Omega$.
Given two objects $A, B \in \Omega$, $\mathit{sim}(A, B) = 1$ if $A = B$ and $\mathit{sim}(A, B) < 1$ if $A \neq B$.
\end{definition}
\fullcite{Lin1998} states three intuitions, which a similarity measure between two objects $A$ and $B$ should fulfill:
\begin{itemize}
\item "The similarity between $A$ and $B$ is related to their commonality. The more commonality they share, the more similar they are." \cite{Lin1998}
\item "The similarity between $A$ and $B$ is related to the differences between them. The more differences they have, the less similar they are." \cite{Lin1998}
\item "The maximum similarity between $A$ and $B$ is reached when $A$ and $B$ are identical, no matter how much commonality they share." \cite{Lin1998}
\end{itemize}
Therefore a similarity measure $\mathit{sim(A, B)}$ should return $1$ only if $A = B$ and $0$ if the $A$ and $B$ share no commonalities.
A similarity measure should factor in both commonalities and differences to compute the similarity between objects \cite{Lin1998}.

Different types of information can be encoded as feature vectors in $\mathbb{R}^d$, e.g.
words \cite{Levy2015} \cite{Mikolov2013}, graphs \cite{Ristoski2016} \cite{Cao2016}, etc., where $d \in \mathbb{N}$ is the dimension of the vector space.
The similarity between feature vectors can be measured using a distance measure between two vectors $\vec{p}, \vec{q} \in \mathbb{R}^d$ \cite{Weber2000}.
Distance measures $\delta$ have the signature: $\delta: \mathbb{R}^d \times \mathbb{R}^d \mapsto \mathbb{R}_0^+$.
$\delta (\vec{p}, \vec{q}) = 0$ implies $\mathit{sim}(\vec{p}, \vec{q}) = 1$, while $\delta (\vec{p}, \vec{q}) \rightarrow \infty$ implies $\mathit{sim}(\vec{p}, \vec{q}) = 0$.
Therefore it is necessary to find a mapping of distance to similarity, if a distance measure should be used as similarity measure.
A typical distance measure is the \textbf{$L_S$-Norm} \cite{Weber2000}:
\begin{align}
&L_S: \delta_S (\vec{p}, \vec{q}) = \sqrt[S]{\sum_{i=0}^{n-1}\abs{\vec{p}_i - \vec{q}_i}^S}
\end{align}
with $S \in \mathbb{R}^+$.
For example, the $L_2$-Norm is the euclidean distance:
\begin{align}
&L_2: \delta_2 (\vec{p}, \vec{q}) = \sqrt{\sum_{i=0}^{n-1}\abs{\vec{p}_i - \vec{q}_i}^2}
\end{align}
Similarity between vectors can now be defined by mapping the distances to similarity values using a correspondence function \cite{Weber2000}:
\begin{definition}[Correspondence function]
$h: \mathbb{R}^+ \mapsto [0,1]$ is a \textbf{correspondence function}, if it fulfills the following properties:
\begin{enumerate}
\item $h(0), h(\infty) = 0$,
\item $\forall x,y: x > y \implies h(x) \le h(y)$
\end{enumerate}
\end{definition}
\fullcite{Aggarwal2001} show that $L_S$ distance measures especially suffer the curse of dimensionality for
increasing values of $S$. The curse of dimensionality implies that
''the ratio of the distances of the nearest and farthest neighbors to a given target in high dimensional space 
is almost $1$ for a wide variety of data distributions and distance functions'' \cite{Aggarwal2001}.
\fullcite{Aggarwal2001} also show that for increasing dimensions, the distance between the nearest and farthest neighbors converges against $\infty$ for $L_1$,
against some constant $c \in \mathbb{R}$ for $L_2$, and for higher $L_S$-norms the distance converges against $0$.
This implies that $L_S$ norms with higher $S$ are bad at contrasting between vectors in high-dimensional space \cite{Aggarwal2001}.
Therefore the distance-based kNN used in the thesis should use either the $L_1$- or the $L_2$-norm as distance measure to avoid the low contrast associated with the other $L_S$ measures.

In ontology learning, \textbf{semantic similarity} is used to great effect, 
e.g  for clustering objects to create hierarchies\cite{Hazman2011} \cite{Wong2012} 
or mapping between different ontologies  \cite{Doan2002} \cite{Rodriguez2003}. 
Semantic similarity compares the semantic content of objects or documents . 
This can be achieved by comparing which features can be found in both objects (commonalities) 
and which features are unique to the compared objects (differences).
\fullcite{Rodriguez2003} develops a semantic similarity measure for comparing entity classes in ontologies.
Given two objects $a, b \in \Omega$, $A$ and $B$ are their corresponding descriptions sets,
e.g. for Wikidata the aliases and statements. $\alpha$ is a function, which defines the importance of
differences between $a$ and $b$. $A \cap B$ is the set of commonalities, and $A/B$ the set of differences between
the $a$ and $b$. The defined similarity function is not symmetric, $\mathit{sim}(a,b) \neq \mathit{sim}(b, a)$.
\begin{align}
&\mathit{sim}(a, b) = \frac{\abs{A \cap B}}{\abs{A \cap B} + \alpha (a,b) \abs{A/B} + (1 - \alpha (a,b)) \abs{B/A}}
\end{align}
for $0 \le \alpha \le 1$ \cite{Rodriguez2003}.

Calculating the similarity between vectors is advantageous to calculating the similarity between objects in an ontology, 
because vectors abstract from the specific objects and their relations to other objects in the ontology.
Using neural word or graph embeddings, which are presented in Sections~\ref{section:word2vec}
and~\ref{section:graph embeddings}, enables the representation of classes and instances in ontologies
as feature vectors. The mentioned curse of dimensionality is a non-issue as it applies, if the the number
of irrelevant of features is high. This is typically solved by reducing the dimension of the feature vectors
to include only relevant features \cite{Domingos2012}. 
Neural embeddings however seem to implicitly capture only relevant features \cite{Mikolov2013},
therefore no dimensionality reduction is necessary, if neural embeddings are used.

%%%%%%%%%%%
% Problem statement 
%%%%%%%%%%%
\subsection{Problem statement}\label{section:problem statement}
The task of this thesis is the classification of orphan classes in Wikidata. In other words a function is needed, 
which given an orphan class $u$ of a taxonomy $T = (C, S)$ with a root class $root_T$, finds an appropriate superclass for $u$.

An optimal similarity function $\mathit{sim}_\mathit{opt}$ is assumed for the definition of the problem statement.
Let $s \in C$ be the most specific, appropiate superclass of $u$.
The following property should be fulfilled by $\mathit{sim}_\mathit{opt}$:
\begin{align}
&\forall c \in C\setminus \{ s \} 1 = \mathit{sim}_\mathit{opt}(u, u) > \mathit{sim}_\mathit{opt}(u, s) > \mathit{u, c}
\end{align}
This definition implies that the most similar class to $u$ in the taxonomy $T$, other than $u$ itself, is the most appropiate superclass $s$ of $u$.

The thesis' problem is defined as follows:
\begin{definition}[Problem definition]\label{problem definition}
Given a taxonomy $T = (C, S)$ with root class $root_T$ and a similarity function $sim$ over $T$,
find a function $f: C \mapsto C$, 
which, given an orphan class $u \in C$, returns a class $s = f(u)$, 
fulfilling the following criteria:
\begin{align}
& \forall p \in P: \neg (p \subclassof{T} u) \: \textnormal{no children} \label{no children}\\
& s =\max_{s \in C}(\textit{sim}_\textit{msp}(u, c)) \: \textnormal{most specific parent} 
\label{most similar class}
\end{align}
\end{definition}

The stated problem induces several challenges, which is listed here, but addressed in later sections.

\begin{enumerate}
\item \textbf{High number of labels}. An orphan class has to be assigned to a class in the root taxonomy.
The only restricting condition is that the chosen class cannot be a subclass of the orphan class.
As shown in Section~\ref{section:taxonomy analysis}, $195,124$ classes of the taxonomy are considered as relevant to the classification task. 
Classification methods, like SVM or neural networks, usually classify with a small number of labels.
But in the context of this thesis, a method is required, which is able to handle over $100,000$ labels.
\item \textbf{Representation of items}. Items in Wikidata are structured information, similar to nodes in RDF graphs
and Wikidata can be represented as such \cite{Ristoski2016} \cite{Erxleben2014}.
As motivated in Section~\ref{section:introduction}, the solution should exploit the apparent power of
neural networks. Neural networks, which are introduced in Section~\ref{section:neural networks},
require input to be represented as vectors. Therefore, it will be necessary to map items to vectors.
\end{enumerate}

%%%%%%%%%%%%%%%
% Similarity-based classification
%%%%%%%%%%%%%%%
\subsection{k-nearest-neighbors classification}\label{section:knn}
Based on the characteristics of the classification problem, described by the problem statement, and the challenges attached to it,
the \textbf{k-nearest-neighbor algorithm (kNN)} seems like an appropriate tool for solving the task.
Nearest-neighbors classification is a lazy method, as it does not require training before testing.
This is useful for applications with high amounts of data, large numbers of classes, and changing data \cite{Zhang2005} \cite{Chen2009}.
For the considered use case of classification in Wikidata, these are very important strengths,
as the number of classes in the taxonomy is very high and Wikidata is being constantly edited.

kNNs can be defined as a similarity-based classification method.
kNN uses a pairwise similarity or distance measure (see Section~\ref{section:similarity}).
Access to the features of the classified objects is therefore not required \cite{Chen2009}.

Given a set of classified objects $\Phi \subset \mathbb{R}^n$ with the set of classes $C$, where the function $h: \Phi \mapsto C$ returns the class for a given object.
Given an unclassified object $\vec{u} \in \mathbb{R}^n$, the parameter $k$ defines the number of nearest neighbors $\{ \vec{n_i} \in \Phi | k=1, \dots, k \}$ of $\vec{u}$, which are
considered for the classification decision.
The unknown is classified by a vote.
Each nearest neighbor $n_i$ votes for its own class $c_i = h(n_i)$. 
The class, which has the majority vote, is then chosen as the class of $\vec{u}$.

The votes of each neighbor can be weighted based on different criteria.
Uniform weights give each vote the same weight, therefore the distance between the neighbors to the unknown is not relevant for this weighting scheme.

Results produced by uniform weighted kNN are not always desirable.
Assume the case, in which the parameter $k$ was chosen too high for a given unknown, which leads to the problem that very dissimilar objects
could be part of the $k$ nearest neighbors.
Using uniform weighting would be problematic, since the dissimilar objects would negatively influence the majority vote.
Votes of distant objects are not as relevant to the classification as close objects \cite{Chen2009}.
This leads to the criteria of \textbf{affinity}, which states that similar neighbor should be given larger weights \cite{Chen2009}.
A typical weighting approach to achieve affinity is distance-based weighting.
The distance between each neighbor and the unknown is computed.
For example in $\mathbb{R}^n$, the euclidean norm $\delta_2$ could be used.
Each neighbor $\vec{n_i}$ of $\vec{u}$ is assigned the inverse distance as weight $w_i = \delta_2(\vec{u}, \vec{n_i})^{-1}$.
Therefore the weight of each neighbor is directly proportional to its similarity to $\vec{u}$.

\begin{figure}[H]
\centering
\includegraphics[width=0.5\textwidth]{images/foundations/knn_example.pdf}
\caption{Example for kNN with uniform weights.}
\label{fig:knn example}
\end{figure}

Figure~\ref{fig:knn example} describes a classification using kNN with uniform weights.
kNN considers only the closest k neighbors in its classification decision.
In this example, the circle with name $u$ is the unknown class, which has to be classified.
$10$ other objects, where each belongs to one of the $3$ classes (square, pentagon, octagon), are present.
Different weighting schemes are used by kNN. 
Each neighbor votes on the class of $u$. 
The weight controls to what degree a single neighbor can influence the class of $u$.
The simplest approach is uniform weighting, which is used in this example.
Uniform weighting assigns each object the same weight.
Therefore, the distance of the neighbors to $u$ does not matter in the classification decision.

If $k=4$ then all objects in the inner circle are considered.
$2$ objects have the class pentagon, while the classes square and octagon are represented with $1$ object each in the $4$ neighbors.
Using uniform weighting, the class pentagon gets $2$ votes and therefore $u$ would be classified as pentagon.

Changing the parameter $k$ has influence on the results produced by kNN.
If $k$ would be set as $10$ instead of $4$, all object in the outer dashed circle would be considered.
The classes square and pentagon have each $3$ objects in this set of neighbors,
while the class octagon has $4$ objects in the set.
Using uniform weights would therefore classify $u$ as octagon.

It could be argued that the classification result using $k=10$ may be inaccurate, since all objects of class octagon have a high distance to $u$
 in comparison to the objects in class pentagon.
Therefore the decision made by $k=4$ may be more accurate.
This problem can be solved by applying other weighting schemes than uniform weights to kNN.
A typical approach, which was described in this section, is distance-based weights.
This approach assigns higher weights to closer neighbors and could therefore solve the previously mentioned inaccuracy.

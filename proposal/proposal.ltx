% !TEX TS-program = pdflatex
% !TEX encoding = UTF-8 Unicode

\documentclass[11pt]{scrartcl} % use larger type; default would be 10pt

\usepackage[utf8]{inputenc} % set input encoding (not needed with XeLaTeX)

%%% PAGE DIMENSIONS
\usepackage{geometry} % to change the page dimensions
\geometry{a4paper} % or letterpaper (US) or a5paper or....

\usepackage{graphicx} % support the \includegraphics command and options

 %\usepackage[parfill]{parskip} % Activate to begin paragraphs with an empty line rather than an indent

%%% PACKAGES
\usepackage{booktabs} % for much better looking tables
\usepackage{array} % for better arrays (eg matrices) in maths
\usepackage{paralist} % very flexible & customisable lists (eg. enumerate/itemize, etc.)
\usepackage{verbatim} % adds environment for commenting out blocks of text & for better verbatim
\usepackage{subfig} % make it possible to include more than one captioned figure/table in a single float
\usepackage{color}
\usepackage{ulem}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{tabularx}
\usepackage[numbers]{natbib} % for citeauthor
\usepackage{pgfgantt} % ganttchart

\newcounter{ToDoId}
\newcommand{\TODO}[1]{\stepcounter{ToDoId}\textcolor{red}{\textbf{TODO \theToDoId: #1}}}

\newcommand{\subclassof}{
\ensuremath{\vartriangleleft_{subclass}}
}

\theoremstyle{definition}
\newtheorem{definition}{Definition}
\newtheorem*{problem}{Problem}


\title{Refinement of the Wikidata taxonomy with neural networks}
\subtitle{Proposal for Bachelor thesis}
\author{Alex Baier \\ abaier@uni-koblenz.de}


\begin{document}
\maketitle

\section{Motivation}
Wikidata is an open, free, multilingual and collaborative knowledge base. It is as a structured knowledge source for other Wikimedia projects. It tries to model the real world, meaning every concept, object, animal, person,etc.. We
call these entities notorious entities. Wikidata is mostly edited and extended by humans, which in general
improves the quality of entries compared to fully-automated systems, because different editors
can validate and correct occurring errors.\\
Most entities in Wikidata are items. Items consist of labels, aliases and descriptions in different languages. 
Sitelinks connect items to their corresponding Wiki articles. Most importantly items are described by statements.
Statements are in their simplest form a pair of property and value. They can be annotated with references and
qualifiers. See figure~\ref{fig:item} for an example. \\
\begin{figure}
\centering
\includegraphics[width=\textwidth]{images/item_example.png}
\caption{photographic film (Q6239)}
\label{fig:item}
\end{figure}
An important property used to describe a Wikidata item is \textit{subclass of (P279)}. Items, which contain
statements with this property, are classes, and the statement also points to a superclass, which is a generalization
of the subclass. For example in Figure~\ref{fig:item} \textit{photographic film (Q6239)} is a subclass of
\textit{data storage device (Q193395)}, \textit{Photo equipment (Q1439598)}, and \textit{ribbon (Q857421)}.
With \textit{subclass of (P279)} a taxonomy can be created in Wikidata.
Figure~\ref{fig:taxonomy} shows a fragment of Wikidata's taxonomy with a focus on the class 
\textit{photographic film (Q6239)}.
\begin{figure}
\centering
\includegraphics[width=\textwidth]{images/example_taxonomy.pdf}
\caption{Fragment of Wikidata taxonomy with suggested improvement.}
\label{fig:taxonomy}
\end{figure}
Taxonomies like this can be used for different tasks. 
\citeauthor{pekar2002tl} \cite{pekar2002tl} for example develop a method of word classification in thesauri, which 
exploits the structure of
taxonomies. Other uses may be found in information retrieval and reasoning. \\
As of the 7th November 2016 over a million classes are present in this taxonomy. 
A root class in a taxonomy is a class, which has no more generalizations.
Root classes should therefore describe the most basic concepts. 
According to this view, we would assume that a good taxonomy has only very few, possibly only one root class.
The last remaining root class in Wikidata should be \textit{entity (Q35120)}.\\
At the current state (2016-11-07) Wikidata contains 7142 root classes, of which 5332 have an English label.
There are many root classes for which we easily can find generalizations.
Consider for example \textit{reversal film (Q166816)} in the taxonomy fragment of Figure~\ref{fig:taxonomy}.
We can see that the class only has one subclass and is otherwise isolated in the taxonomy. 
Based on an expert opinion or the associated Wikipedia article, we can easily identify 
\textit{photographic film (Q6239)} as a possible superclass of \textit{reversal film (Q166816)}, as indicated by
the red arrow.
A superclass should be considered appropriate, if it is a generalization of the child class and
also the most similar respectively nearest class to the child class.
Even though it is possible to solve this task by hand, which is the current process in Wikidata, multiple
obstacles prevent this process to be efficient. First the number of root classes is high, and identifying
them is not directly supported by Wikidata. Additionally finding an appropriate superclass
for a given class is a difficult task, because the number of potential solutions is very high.
Tools, which solve this task, may help the Wikidata community in improving the existing taxonomy.
Similar tasks in the field of ontology learning are already well researched. We propose the use of neural
networks for solving this task, because the application of them in ontology learning is sparse in existing work,
and as shown in Section~\ref{section:relatedwork} neural networks seem to be appropriate for the task.

\section{Problem statement}\label{section:problemstatement}
To define the problem following definitions are needed:
\\ \TODO{Redefine statement and class.}
\begin{definition}[Statement]
A statement is tuple $(pid, value)$:
	\begin{itemize}
	\item $pid \in \mathbb{N}$, which is a numerical Wikidata property ID;
	\item $value$ depends on the data type corresponding with $pid$, in most cases it will be natural number,
	representing a Wikidata item id. 
	\end{itemize}
\end{definition}
\begin{definition}[Class]
\label{def:class}
A class is a tuple $(id, label, Statements, Instances, wiki)$:
	\begin{itemize}
	\item $id \in \mathbb{N}$, which is a numerical Wikidata item ID;
	\item $label$, which is the, to $id$ corresponding,  English label in Wikidata;
	\item $Statements$ is a set of statements about the class;
	\item $Instances \in \mathcal{P}(\mathbb{N})$ is the set of numerical Wikidata item IDs, which are instances of 
	the class;
	\item $wiki$ is the, to the class corresponding, English Wikipedia article text.
	\end{itemize}
\end{definition}
\begin{definition}[Taxonomy]
A taxonomy $T=(C, S)$ is a acyclic graph, where $C$ is a set of classes, and $S$ is a set of subclass-of relations
between these classes.
\end{definition}
Because a class can have multiple superclasses, a tree structure is insufficient for modeling the taxonomy.
\begin{definition}[Subclass Relation]
Let $T=(C, S)$ be a taxonomy.\\
The transitive, ordered relation $\subclassof$ is defined.\\
Let $c_1, c_2 \in C$. $c_1 \subclassof c_2$, if there is a path $P=(c_1, \ldots, c_2)$ from $c_1$ to $c_2$ in $T$.
\end{definition}
\begin{definition}[Root class]
Let $out(r)$ be the set of all outgoing edges of $r$.
Let $T=(C, S)$ be a taxonomy.\\
$r \in C$ is called root class of $T$, if $|succ(r)| = 0$.\\
$root(T) = \{r \in C \mid  |out(r)| = 0\}$ is the set of all root classes in $T$.
\end{definition}
\begin{definition}
Define a function $sim: Class \times Class \mapsto (0,1)$ as the similarity between two classes.
Two classes have high similarity if the output of the function is close to $1$. 
\end{definition}
Finally we can define our problem as the following task:
\begin{problem}
We simplify the taxonomy refinement task to the problem of finding the closest superclass for a given root class. \\
\\
Given the input $W=(C, S)$ a taxonomy and $r \in C$ a root class in $W$,\\
find a function $f: Taxonomy \times Class \mapsto Class$, so that it produces\\
an output $s = f(W, r)$, which fulfills $\neg (s \subclassof r)$ and $s = \underset{c \in C}{\max}(sim(c, r))$.


\end{problem}

\section{Related work}\label{section:relatedwork}
The related work for this thesis can be divided in two categories. First are papers, which try to solve similar
tasks, and second is the topic of neural networks, which may be used to solve the defined problem.

\subsection{Similar tasks}

 \citeauthor{Maedche2001} \cite{Maedche2001} define and analyze the topic of ontology learning.
Additionally a tool called \textit{OntoEdit} was developed in the process. \cite{Maedche2001} considers 
a semi-automatic approach and divides the process of ontology learning into the following steps:\\
\textbf{import/reuse} existing ontologies, \textbf{extract}
major parts of target ontology, \textbf{prune} to adjust the ontology for its primary purpose, \textbf {refine}
ontology to complete it at a fine granularity, and \textbf{apply} it on target application to validate 
the results.\\
The problem solved by this thesis belongs to the step \textbf{refine}.\\

\citeauthor{pekar2002tl} \cite{pekar2002tl} define algorithms for classification, which exploit the structure of
taxonomies. Distributional and taxonomic similarity measures on nearest neighbors are used to make a 
classification decision. These algorithms are applied on the classification of new words (instances) into thesauri. In 
comparison the target of this thesis will be to improve the existing taxonomy. For this the closest generalizations
of root classes have to be found. The algorithms used by \citeauthor{pekar2002tl} \cite{pekar2002tl} may prove 
useful for the defined problem.\\

\citeauthor{PetrucciGR16} \cite{PetrucciGR16} describe a recurrent neural network model for ontology learning. 
Using encyclopedic text as input OWL formulas are created. The authors argue that their
model should be effective, because neural networks have shown success in natural language processing tasks.
At this time the described model is under evaluation, so it is not shown that the model will generate good
results. Different subtasks of ontology learning are solved by the paper and by
the proposed thesis. Additionally the thesis will contain an evaluation of the created model and it can be shown
if neural networks are a sensible method for ontology learning. \\

\subsection{Neural networks}

\citeauthor{CaoLX16} \cite{CaoLX16} and \citeauthor{Sperduti1997} \cite{Sperduti1997} develop neural networks, 
deep neural network and recursive neuron
network, which are able to encode graphs as vectors. It is proposed by both papers to use the generated
vectors as input for classification methods. Because the networks are defined in such a way that
semantic information of the graph is preserved to some degree, the vectors could be used for
other task like measuring the similarity of classes based on their position in the taxonomy using for example
cosine similarity.\\

\citeauthor{mikolov2013} \cite{mikolov2013} define two neural network models, Continuous Bag-of-Words (CBOW) 
and
Continuous Skip-Gram, which are able to create word vector representations. They capture the semantics
of words very well and  preserve linear regularities between words. \citeauthor{pekar2002tl} \cite{pekar2002tl} use 
word representations
based on counts with context words. It is possible that the use of newer word representation model like CBOW
will also improve their classification method.\\

\TODO{add \cite{Doan2002} \cite{Rodriguez2003}}

\section{Preliminary data analysis}
Wikidata does not inherently differ between entities and classes. Therefore it is necessary to define, how
classes and root classes can be identified in Wikidata. In Wikidata an entity is a class, if it has instances or has subclasses or is a subclass.
A root class is a class, which is not the subclass of any other class.
It has to be noted that the results of this definition may not be completely accurate,
because Wikidata does not enforce how the $instance of (P31)$ and $subclass of (P279)$ are to be used.
However Wikidata is constantly curated by editors and the number of misused properties should be low, therefore 
we can assume that the percentage of misidentified classes is also low.\\
The taxonomy of Wikidata, containing $1299276$ classes, was analyzed. The following statistics about root
classes were acquired:
\begin{itemize}
\item $16148$ root classes
\item $13624$ root classes with English label
\item $11438$ root classes with an English or Simple English Wikipedia article
\item $\sim 4.8$ statement groups (properties) per root class on average (see Figure~\ref{fig:propertysum})
\item $\sim 4.69$ instances per root class on average (see Figure~\ref{fig:instancesum})
\item $\sim 0.86$ subclasses per root class on average (see Figure~\ref{fig:subclasssum})
\end{itemize}
The 5 most frequent properties in root classes are the following (see Figure~\ref{fig:property_frequency}):
\begin{itemize}
\item \textit{instance of (P31)} with $8687$ occurrences
\item \textit{Freebase ID (P646)} with $7221$ occurrences
\item \textit{topic's main category (P910)} with $6243$ occurrences
\item \textit{Commons category (P373)} with $6183$ occurrences
\item \textit{image (P18)} with $2367$ occurrences
\end{itemize}

\begin{figure}
\centering
\includegraphics[width=0.9\textwidth]{images/property_sum.png}
\caption{number of classes with a specific amount of properties}
\label{fig:propertysum}

\includegraphics[width=0.9\textwidth]{images/instance_sum.png}
\caption{number of classes with a specific amount of instances}
\label{fig:instancesum}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=0.9\textwidth]{images/subclass_sum.png}
\caption{number of classes with a specific amount of subclasses}
\label{fig:subclasssum}

\includegraphics[width=0.9\textwidth]{images/property_frequency.png}
\caption{frequency of properties}
\label{fig:property_frequency}
\end{figure}

It can be seen that classes in Wikidata are used by editors mainly  for the purpose of grouping instances  to a 
concept, because the average root class has $\sim 5$ instances and $\sim 70\%$ have instances.
The taxonomy itself is underdeveloped, as most root classes have less than 1 subclass, and the number
of root classes is high.\\
This means that taxonomy-based approaches \cite{pekar2002tl} may not work well. But approaches from ontology 
mapping using semantic similarity \cite{Doan2002} \cite{Rodriguez2003} could be used,
because they exploit the instances of a class as a mean to find similar concepts.\\
Because most classes have labels and also corresponding Wikipedia articles, another possible approach could be the 
use of word vector models \cite{Mikolov2013}. The Wikipedia articles would ensure that each label will
at least occur once in some context.\\
Based on this observations it is proposed that the set of  root classes is reduced to
the set of labeled root classes with at least one instance, 
so that all observed classes fulfill basic requirements, which can be exploited by the new algorithms.
Therefore the analysis needs to be repeated on this reduced set.

\section{Methodology}
For solving the defined problem with a neural network, the following methodology is proposed:\\

First the current taxonomy of Wikidata needs to be analyzed. It should be answered, how many
classes and especially root classes are available and what their characteristics are, e.g. number of instances and subclasses, how many and what kind of statements. This will allow a more focused search and analysis
in the following step.\\

The literature about neural networks needs be researched. Different neural network models will
be analyzed and compared, regarding input, output, task and performance. Furthermore
the neural networks should be compared to other solutions for the same tasks, so
the decision to use neural networks can be motivated.\\

This leads to the development of a new neural network architecture based on the researched networks,
which is specialized to solve the defined problem. The decision made in the development will be justified based on
the results of the previous steps.\\

\TODO{More specific}
After the neural network is developed, the system needs to be  implemented and training data needs to be 
collected.
The implemented network will be trained, and then tested. Reconfiguration of the network 
and modification of training data will be repeated, until the test results are satisfactory.\\

\TODO{Compare 3 different evaluation approaches. Expert-curated ground truth, automatically generated
ground truth, community agreement}
In the last step the develop  base-line method and modified versions will be evaluated.
One version will ignore the taxonomic structure, and the other methods will use different similarity measures.
The evaluation will be based on a modified version of the taxonomy. A high \TODO{set a number} number of
subclasses will be chosen, and their connection to their superclasses removed, effectively turning them
into root classes. The methods will be applied on this subclasses and the correct result will be the
original superclass. 
The methods will be compared regarding precision, recall, $F_1$-score, direct-hit, and near-hit ratio, where near-hit 
means that the result is the subclass or superclass of the correct class.
This will identify the most suitable similarity measure for the given problem, 
and show how relevant the taxonomic structure
is for the decision making. The near-hit ratio will tell if the developed methods are able to identify the correct
section of the taxonomy, and if this corresponds with the consideration of the taxonomic structure.\\
The best performing method of the previous evaluation will be applied on all current
root classes. In this evaluation participants
will be asked to rate the results.  The evaluation
results will be analyzed and possible improvements for the network discussed.

\section{Expected results}
The bachelor thesis will generate the following products:
\begin{itemize}
\item Statistics about Wikidata's taxonomy with focus on root classes
\item Neural-network based algorithm for taxonomy refinement with multiple variations
\item Training and test data sets for developed algorithm
\item Evaluation regarding precision, recall, $F_1$-score, and near-hits of base-line algorithm and variations
\item Survey with Wikidata community regarding relevance of developed algorithm
\end{itemize}
At the current time I expect the algorithm to consist of two stages. In the first stage (mapping),
the class will be represented as one, or multiple, vectors using neural networks. In the second stage (classification),
a similarity-based classification method like k-nearest-neighbors or the, by Staab and Pekar \cite{pekar2002tl},  proposed algorithms can be used for finding the most similar superclass.\\
The first approach (see Figure~\ref{fig:approach1}) consists of two neural networks, which will represent the class
with two vectors.
The first network being the
Continuous Bag-of-Words (CBOW) by Mikolov et al. \cite{mikolov2013} , and the second being graph
representation neural network (GRNN), like the deep neural network by Cao et al. \cite{CaoLX16}.
CBOW will be trained on the Wikipedia articles of all classes, and GRNN will be trained on the Wikidata taxonomy.
To find a superclass for a given class, the closest classes in the word and in the graph vector space
will be identified and used in a classification method like k-nearest neighbors.\\
The second approach (see Figure~\ref{fig:approach2}) will only use CBOW to map the classes to word vectors.
But the taxonomic structure will still be exploited by using for example tree-ascending+kNN \cite{pekar2002tl}
in the classification stage.\\
For both approaches it will be necessary to calculate the representation vectors for all classes in the taxonomy
ahead of time, so the suggested classification methods can be used.\\
\\
\begin{figure}
\centering
\includegraphics[width=1.1\textwidth]{images/architecture_approach1.pdf}
\caption{Solution using two neural networks \cite{mikolov2013} \cite{CaoLX16} and k-nearest neighbors for 
classification.}
\label{fig:approach1}
\includegraphics[width=1.1\textwidth]{images/architecture_approach2.pdf}
\caption{Solution using CBOW \cite{mikolov2013} for word representation and tree-ascending \cite{pekar2002tl} 
for classification.}
\label{fig:approach2}
\end{figure}
If the evaluation shows that ignoring the taxonomic structure only slightly influences the quality
of results, it may be a good decision to just remove the according part of the algorithm, which
could greatly affect its runtime depending on the used network. 
I assume that the choice of similarity measure could greatly influence the evaluation results.

\section{Time plan}

The following outline is proposed for the thesis:
\begin{enumerate}[1.]
\item Foundations: definitions; problem statement; types of neural networks
\item Taxonomy analysis: statistics about classes and root classes in Wikidata
\item Related work: ontology learning; similarity measures; neural network models
\item Comparison of neural networks: compare input, output, task, type and suitability of different models for the 
given task
\item Development of new algorithms: design new baseline-algorithm and variations; justify design decisions based 
on previous sections
\item Evaluation: explain evaluation method; evaluate/compare baseline-algorithm and variations
\end{enumerate}
The thesis will be written in parallel to the design and implementation of the solution. The first phase consists
of research of related work and an analysis of the Wikidata taxonomy. In the second phase a baseline-algorithm
and variations of it are designed and developed using the results of the previous phase. In the next phase
test data is collected and the evaluation of the algorithms executed. The evaluation results will be 
visualized and interpreted. Finally the summary, introduction, and conclusion of the thesis will be written
and a presentation prepared.

See the following Gantt chart for the time plan \ref{fig:timeplan}:
\begin{figure}
\begin{ganttchart}[vgrid={*1{draw=black!20, line width=.75pt}}, x unit=0.05\textwidth]{1}{12}
\gantttitle{Time plan for bachelor thesis (in weeks)}{12}\\
\gantttitlelist{1,...,12}{1}\\
\ganttgroup{Foundations, Sections 1-4}{1}{5}\\
\ganttbar[name=RelWork]{Research related work}{1}{3}\\
\ganttbar[name=NN]{Compare neural networks}{1}{3}\\
\ganttbar[name=TaxAn]{Analyze taxonomy}{1}{1}\\
\ganttgroup{Algorithms, Sections 5}{4}{8}\\
\ganttbar[name=Design]{Design algorithms}{4}{5}\\
\ganttbar[name=Impl]{Implement algorithms}{6}{8}\\
\ganttbar[name=Tensor]{Learn Tensorflow}{5}{5}\\
\ganttgroup{Evaluation, Section 6}{8}{10}\\
\ganttbar[name=Eval]{Evaluate algorithms}{9}{9}\\
\ganttbar[name=Data]{Create test data}{8}{8}\\
\ganttgroup{Summary, Introduction, Conclusion}{10}{11}\\
\ganttbar[name=SIC]{Write missing parts}{10}{11}\\
\ganttbar[name=Pres]{Prepare presentation}{11}{12}

\ganttlink{TaxAn}{Design}
\ganttlink{RelWork}{Design}
\ganttlink{NN}{Design}
\ganttlink{Design}{Impl}
\ganttlink{Tensor}{Impl}
\ganttlink{Impl}{Eval}
\ganttlink{Data}{Eval}
\ganttlink{Eval}{SIC}
\ganttlink{Eval}{Pres}
\end{ganttchart}
\caption{time plan}
\label{fig:timeplan}
\end{figure}

\listoffigures

\nocite{*}

\bibliographystyle{plainnat}
\bibliography{bibliography}


\end{document}

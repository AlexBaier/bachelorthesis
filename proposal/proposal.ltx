% !TEX TS-program = pdflatex
% !TEX encoding = UTF-8 Unicode

\documentclass[11pt]{scrartcl} % use larger type; default would be 10pt

\usepackage[utf8]{inputenc} % set input encoding (not needed with XeLaTeX)

%%% PAGE DIMENSIONS
\usepackage{geometry} % to change the page dimensions
\geometry{a4paper} % or letterpaper (US) or a5paper or....

\usepackage{graphicx} % support the \includegraphics command and options

 %\usepackage[parfill]{parskip} % Activate to begin paragraphs with an empty line rather than an indent

%%% PACKAGES
\usepackage{booktabs} % for much better looking tables
\usepackage{array} % for better arrays (eg matrices) in maths
\usepackage{paralist} % very flexible & customisable lists (eg. enumerate/itemize, etc.)
\usepackage{verbatim} % adds environment for commenting out blocks of text & for better verbatim
\usepackage{subfig} % make it possible to include more than one captioned figure/table in a single float
\usepackage{color}
\usepackage{ulem}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{tabularx}
\usepackage[numbers]{natbib} % for citeauthor
\usepackage{pgfgantt} % ganttchart

\newcounter{ToDoId}
\newcommand{\TODO}[1]{\stepcounter{ToDoId}\textcolor{red}{\textbf{TODO \theToDoId: #1}}}

\newcommand{\subclassof}{
\ensuremath{\vartriangleleft_{subclass}}
}

\theoremstyle{definition}
\newtheorem{definition}{Definition}
\newtheorem*{problem}{Problem}


\title{Proposal: Enrichment of ontological taxonomies using a neural network approach}
\subtitle{Case study: Wikidata }
\author{Alex Baier \\ abaier@uni-koblenz.de}


\begin{document}
\maketitle

\section{Motivation}
Wikidata is an open, free, multilingual and collaborative knowledge base. It is as a structured knowledge source for other Wikimedia projects. It tries to model the real world, meaning every concept, object, animal, person,etc.. We
call these entities notorious entities. Wikidata is mostly edited and extended by humans, which in general
improves the quality of entries compared to fully-automated systems, because different editors
can validate and correct occurring errors.\\
Most entities in Wikidata are items. Items consist of labels, aliases and descriptions in different languages. 
Sitelinks connect items to their corresponding Wiki articles. Most importantly items are described by statements.
Statements are in their simplest form a pair of property and value. They can be annotated with references and
qualifiers. See figure~\ref{fig:item} for an example. \\
\begin{figure}
\centering
\includegraphics[width=\textwidth]{images/item_example.png}
\caption{photographic film (Q6239)}
\label{fig:item}
\end{figure}
An important property used to describe a Wikidata item is \textit{subclass of (P279)}. Items, which contain
statements with this property, are classes, and the statement also points to a superclass, which is a generalization
of the subclass. For example in Figure~\ref{fig:item} \textit{photographic film (Q6239)} is a subclass of
\textit{data storage device (Q193395)}, \textit{Photo equipment (Q1439598)}, and \textit{ribbon (Q857421)}.
With \textit{subclass of (P279)} a taxonomy can be created in Wikidata.
Figure~\ref{fig:taxonomy} shows a fragment of Wikidata's taxonomy with a focus on the class 
\textit{photographic film (Q6239)}.
\begin{figure}
\centering
\includegraphics[width=\textwidth]{images/example_taxonomy.pdf}
\caption{Fragment of Wikidata taxonomy with suggested improvement.}
\label{fig:taxonomy}
\end{figure}
Taxonomies like this can be used for different tasks. 
\citeauthor{pekar2002tl} \cite{pekar2002tl} for example develop a method of word classification in thesauri, which 
exploits the structure of
taxonomies. Other uses may be found in information retrieval and reasoning. \\
As of the 7th November 2016 over a million classes are present in this taxonomy. 
A root class in a taxonomy is a class, which has no more generalizations.
Root classes should therefore describe the most basic concepts. 
According to this view, we would assume that a good taxonomy has only very few, possibly only one root class.
The last remaining root class in Wikidata should be \textit{entity (Q35120)}.\\
At the current state (2016-11-07) Wikidata contains 7142 root classes, of which 5332 have an English label.
There are many root classes for which we easily can find generalizations.
Consider for example \textit{reversal film (Q166816)} in the taxonomy fragment of Figure~\ref{fig:taxonomy}.
We can see that the class only has one subclass and is otherwise isolated in the taxonomy. 
Based on an expert opinion or the associated Wikipedia article, we can easily identify 
\textit{photographic film (Q6239)} as a possible superclass of \textit{reversal film (Q166816)}, as indicated by
the red arrow.
A superclass should be considered appropriate, if it is a generalization of the child class and
also the most similar respectively nearest class to the child class.
Even though it is possible to solve this task by hand, which is the current process in Wikidata, multiple
obstacles prevent this process to be efficient. First the number of root classes is high, and identifying
them is not directly supported by Wikidata. Additionally finding an appropriate superclass
for a given class is a difficult task, because the number of potential solutions is very high.
Tools, which solve this task, may help the Wikidata community in improving the existing taxonomy.
Similar tasks in the field of ontology learning are already well researched. We propose the use of neural
networks for solving this task, because the application of them in ontology learning is sparse in existing work,
and as shown in Section~\ref{section:relatedwork} neural networks seem to be appropriate for the task.

\section{Problem statement}\label{section:problemstatement}
To define the problem following definitions are needed:
\begin{definition}[Item]
An entity is a tuple $(id, description)$:
	\begin{itemize}
	\item $id \in \mathbb{N}$ is the numerical item ID;
	\item $description=(label, aliases, summary, statements, wiki)$
		\begin{itemize}
		\item $label \in String$ is the English label of the item;
		\item $aliases \in \mathcal{P}(String)$ is the set of English synonyms for the label;
		\item $summary \in String$ is a short sentence describing the item;
		\item $statements \in \mathcal{P}(Statement)$ is the set of statements of the item.
		\end{itemize}
	\end{itemize}
\end{definition}
\begin{definition}[Statement]
A statement is a tuple (itemid, pid, value, refs, qualifiers):
\begin{itemize}
\item $itemid \in \mathbb{N}$ is a numerical item ID, to which the statement belongs;
\item $pid \in \mathbb{N}$ is a numerical property ID;
\item $value$ is either a constant value like string, int, etc., or another item;
\item $refs$ is a set of references, storing the source of information for the statement;
\item $qualifiers$ is a set of qualifiers, which further specifies the statement.
\end{itemize}
If the qualifiers and references are not needed, a statement can be shortened to a triple
$(itemid, pid, value)$, similar to semantic triples in RDF.
\end{definition}
Because references and qualifiers are not used in the definition or solution of the problem, they will not
be defined.
\begin{definition}[Class]
\label{def:class}
A class in Wikidata is technically an entity. But a class is also described by its instances, therefore a class
can be represented as the tuple $(id, description, instances)$:
	\begin{itemize}
	\item $instances \in \mathcal{P}(\mathbb{N})$ is the set of numerical item IDs, which are instances of 
	the class.
	\end{itemize}
\end{definition}
\begin{definition}[Taxonomy]
A taxonomy $T=(C, S)$ is an acyclic graph, where $C$ is a set of classes, and $S$ is a set of subclass-of relations
between these classes.
\end{definition}
Because a class can have multiple superclasses, a tree structure is insufficient for modeling the taxonomy.
\begin{definition}[Subclass Relation]
Let $T=(C, S)$ be a taxonomy.\\
The transitive, ordered relation $\subclassof$ is defined.\\
Let $c_1, c_2 \in C$. $c_1 \subclassof c_2$, if there is a path $P=(c_1, \ldots, c_2)$ from $c_1$ to $c_2$ in $T$.
\end{definition}
\begin{definition}[Root class]
Let $out(r)$ be the set of all outgoing edges of $r$.
Let $T=(C, S)$ be a taxonomy.\\
$r \in C$ is called root class of $T$, if $|succ(r)| = 0$.\\
$root(T) = \{r \in C \mid  |out(r)| = 0\}$ is the set of all root classes in $T$.
\end{definition}
\begin{definition}
Define a function $sim: Class \times Class \mapsto [0,1]$ as the similarity between two classes.
Two classes have high similarity if the output of the function is close to $1$. 
$sim(c_1, c_2) = 1$ is only the case, if $c_1$ and $c_2$ are identical.
\end{definition}
Finally we can define our problem as the following task:
\begin{problem}
We simplify the taxonomy refinement task to the problem of finding the closest superclass for a given root class. \\
\\
Given the input $W=(C, S)$ a taxonomy and $r \in C$ a root class in $W$,\\
find a function $f: Taxonomy \times Class \mapsto Class$, so that it produces\\
an output $s = f(W, r)$, which fulfills $\neg (s \subclassof r)$ and $s = \underset{c \in C}{\max}(sim(c, r))$.


\end{problem}

\section{Related work}\label{section:relatedwork}
The related work for this thesis can be divided in two categories. First are papers, which try to solve similar
tasks, and second is the topic of neural networks, which may be used to solve the defined problem.

\subsection{Similar tasks}

 \citeauthor{Maedche2001} \cite{Maedche2001} define and analyze the topic of ontology learning.
Additionally a tool called \textit{OntoEdit} was developed in the process. \cite{Maedche2001} considers 
a semi-automatic approach and divides the process of ontology learning into the following steps:\\
\textbf{import/reuse} existing ontologies, \textbf{extract}
major parts of target ontology, \textbf{prune} to adjust the ontology for its primary purpose, \textbf {refine}
ontology to complete it at a fine granularity, and \textbf{apply} it on target application to validate 
the results.\\
The problem solved by this thesis belongs to the step \textbf{refine}.\\

\citeauthor{pekar2002tl} \cite{pekar2002tl} define algorithms for classification, which exploit the structure of
taxonomies. Distributional and taxonomic similarity measures on nearest neighbors are used to make a 
classification decision. These algorithms are applied on the classification of new words (instances) into thesauri. In 
comparison the target of this thesis will be to improve the existing taxonomy. For this the closest generalizations
of root classes have to be found. The algorithms used by \citeauthor{pekar2002tl} may prove 
useful for the defined problem, if the root classes of Wikidata are parts of the bigger components in 
the taxonomy graph, so that a taxonomic structure can be exploited.\\

\citeauthor{PetrucciGR16} \cite{PetrucciGR16} describe a recurrent neural network model for ontology learning. 
Using encyclopedic text as input OWL formulas are created. The authors argue that their
model should be effective, because neural networks have shown success in natural language processing tasks.
At this time the described model is under evaluation, so it is not shown that the model will generate good
results. Different subtasks of ontology learning are solved by the paper and by
the proposed thesis. Additionally the thesis will contain an evaluation of the created model and it can be shown
if neural networks are a sensible method for ontology learning. \\

\citeauthor{Doan2002} \cite{Doan2002} describe a system for ontology mapping called GLUE, which uses
machine learning techniques.  Three probabilistic distribution-based similarity measures are defined.
The measures are used to compare concepts of the two ontologies on a semantic level.
The GLUE system takes two taxonomies as input and produces mappings for both taxonomies.
Different similarity measures can be used interchangeably in the system. GLUE is evaluated with the Jacard 
coefficient as similarity function. The system shows to have a high matching accuracy.\\
The defined measures are
the Jaccard coefficient, which describes an exact similarity, the ''most-specific-parent'' similarity, 
and the ''most-general-child'' similarity. Especially the ''most-specific-parent'' similarity for concepts seems to
fit the thesis' problem well, as the optimal solution for the defined problem is to the most similar, therefore most
specific, superclass/parent class for a given class. \\

\citeauthor{Rodriguez2003} \cite{Rodriguez2003} describes different matching using semantic similarity
between entity classes from different ontologies. The following three measures are shown.
Word matching compares the number of common and different words in the synonym sets (aliases in Wikidata) of 
entity classes.
Feature matching uses common and different characteristics between the entities of the classes.
Information content defines the similarity as degree of informativeness of the immediate superconcept
that subsumes the two concepts being compared.\\
For the task of the thesis, word matching and feature matching are relevant, as Wikidata offers aliases for
word matching, and it will be shown in Section~\ref{section:data} that classes in Wikidata have on average about 
$5$ entities, which is relevant for feature matching.

\subsection{Neural networks}

\citeauthor{CaoLX16} \cite{CaoLX16} and \citeauthor{Sperduti1997} \cite{Sperduti1997} develop neural networks, 
deep neural network and recursive neuron
network, which are able to encode graphs as vectors. It is proposed by both papers to use the generated
vectors as input for classification methods. Because the networks are defined in such a way that
semantic information of the graph is preserved to some degree, the vectors could be used for
other task like measuring the similarity of classes based on their position in the taxonomy using for example
cosine similarity.\\

\citeauthor{mikolov2013} \cite{mikolov2013} define two neural network models, Continuous Bag-of-Words (CBOW) 
and
Continuous Skip-Gram, which are able to create word vector representations. They capture the semantics
of words very well and  preserve linear regularities between words. \citeauthor{pekar2002tl} \cite{pekar2002tl} use 
word representations
based on counts with context words. It is possible that the use of newer word representation model like CBOW
will also improve their classification method.\\

\section{Preliminary data analysis} \label{section:data}
Wikidata does not inherently differ between entities and classes. Therefore it is necessary to define, how
classes and root classes can be identified in Wikidata. In Wikidata an entity is a class, if it has instances or has subclasses or is a subclass.
A root class is a class, which is not the subclass of any other class.
It has to be noted that the results of this definition may not be completely accurate,
because Wikidata does not enforce how the $instance of (P31)$ and $subclass of (P279)$ are to be used.
However Wikidata is constantly curated by editors and the number of misused properties should be low, therefore 
we can assume that the percentage of misidentified classes is also low.\\
The taxonomy of Wikidata, containing $1299276$ classes, was analyzed. The following statistics about root
classes were acquired:
\begin{itemize}
\item $16148$ root classes
\item $13624$ root classes with English label
\item $11438$ root classes with an English or Simple English Wikipedia article
\item $\sim 4.8$ statement groups (properties) per root class on average (see Figure~\ref{fig:propertysum})
\item $\sim 4.69$ instances per root class on average (see Figure~\ref{fig:instancesum})
\item $\sim 0.86$ subclasses per root class on average (see Figure~\ref{fig:subclasssum})
\end{itemize}
The 5 most frequent properties in root classes are the following (see Figure~\ref{fig:property_frequency}):
\begin{itemize}
\item \textit{instance of (P31)} with $8687$ occurrences
\item \textit{Freebase ID (P646)} with $7221$ occurrences
\item \textit{topic's main category (P910)} with $6243$ occurrences
\item \textit{Commons category (P373)} with $6183$ occurrences
\item \textit{image (P18)} with $2367$ occurrences
\end{itemize}

\begin{figure}
\centering
\includegraphics[width=0.9\textwidth]{images/property_sum.png}
\caption{number of classes with a specific amount of properties}
\label{fig:propertysum}

\includegraphics[width=0.9\textwidth]{images/instance_sum.png}
\caption{number of classes with a specific amount of instances}
\label{fig:instancesum}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=0.9\textwidth]{images/subclass_sum.png}
\caption{number of classes with a specific amount of subclasses}
\label{fig:subclasssum}

\includegraphics[width=0.9\textwidth]{images/property_frequency.png}
\caption{frequency of properties}
\label{fig:property_frequency}
\end{figure}

It can be seen that classes in Wikidata are used by editors mainly  for the purpose of grouping instances  to a 
concept, because the average root class has $\sim 5$ instances and $\sim 70\%$ have instances.
The taxonomy itself is underdeveloped, as most root classes have less than 1 subclass, and the number
of root classes is high.\\
This means that taxonomy-based approaches \cite{pekar2002tl} may not work well. But approaches from ontology 
mapping using semantic similarity \cite{Doan2002} \cite{Rodriguez2003} could be used,
because they exploit the instances of a class as a mean to find similar concepts.\\
Because most classes have labels and also corresponding Wikipedia articles, another possible approach could be the 
use of word vector models \cite{mikolov2013}. The Wikipedia articles would ensure that each label will
at least occur once in some context.\\
Based on this observations it is proposed that the set of  root classes is reduced to
the set of labeled root classes with at least one instance, 
so that all observed classes fulfill basic requirements, which can be exploited by the new algorithms.
Therefore the analysis needs to be repeated on this reduced set.

\section{Methodology}
For solving the defined problem with a neural network, the following methodology is proposed:\\

\subsection{Taxonomy analysis}
First the current taxonomy of Wikidata needs to be analyzed. It should be answered, how many
classes and especially root classes are available and what their characteristics are, e.g. number of instances and subclasses, how many and what kind of statements. This will allow a more focused search and analysis
in the following step.

\subsection{Comparison of neural networks}
The literature about neural networks needs be researched. Different neural network models will
be analyzed and compared, regarding input, output, task and performance. Furthermore
the neural networks should be compared to other solutions for the same tasks, so
the decision to use neural networks can be motivated.

\subsection{Development of a neural network-based algorithm}
This leads to the development of a new algorithm using neural networks, which solves the defined problem.
The algorithm should try to exploit the different aspects of the knowledge provided by Wikidata.
After the baseline-algorithm is developed at least one variation should also be implemented,
which ignores a certain aspect of the provided knowledge, or uses a different similarity measure.

\subsection{Evaluation}
In the last step the baseline algorithm and its variations will be evaluated.\\
Variations of the baseline-algorithm should only use information provided directly by Wikidata, instead
of also exploiting the encyclopedic text provided by Wikidata.
Other variations should use different measures of similarity for finding the most similar superclass.\\
There are two purposes to the evaluation. Firstly, the evaluation will show if the developed algorithm
is able to solve the problem. And secondly, the evaluation of the variations will show, which measures
are better suited for the task, and if it is necessary to include additional data, which is not provided by Wikidata
to get good results.\\
Two different approaches for evaluation are proposed.
For both approaches a ground truth for testing needs to be created, 
which consists of pairs of root classes and their correct superclasses. \\
In the first approach, the ground truth
will be created by group of experts via for example crowdsourcing. 
In this case the subset of used root classes will be taken from the set of root classes in the current taxonomy.\\
 In the second case, the ground truth will be automatically generated. It can be assumed,
that the subclass relations in the current taxonomy are valid. Therefore for a set of non-root classes in
the taxonomy the subclass relation to their superclasses will be removed, and the removed relation
used as ground truth. The modified taxonomy will be used in the algorithms. \\
For these evaluations, the percision, recall, $F_1$-score, direct-hit ratio, and near-hit ratio
will be calculated for all versions of the algorithm.\\ 
The first approach should generate a better ground truth, and the application of the algorithms
on the unmodified taxonomy is a more realistic testing scenario than the second approach using
a modified taxonomy. But the first approach also requires the participation of voluntary experts,
which would require a high time investment, because the experts need to be found and motivated to participate
in the task.\\
In comparison the second approach can be executed by a single person, because the ground truth already exists.
For this approach the quality of the ground truth varies depending on the number of editors for
the chosen classes. The number of edits of a class correlates directly to its quality,
because a high number of edits implies that many different users reached an agreement about
the values of a class. Therefore it is possible to create a good ground truth with this approach
by choosing only classes, which have a certain threshold of edits.\\
For the evaluation in the thesis, the second evaluation approach will be used, because it requires less time
and should provide accurate results. 

\section{Expected results}
The bachelor thesis will generate the following products:
\begin{itemize}
\item Statistics about Wikidata's taxonomy with focus on root classes
\item Neural-network based algorithm for taxonomy refinement with multiple variations
\item Training and test data sets for developed algorithm
\item Evaluation regarding precision, recall, $F_1$-score, and near-hits of base-line algorithm and variations
\end{itemize}
At the current time I expect the algorithm to consist of two stages. In the first stage (mapping),
the class will be represented as one, or multiple, vectors using neural networks. In the second stage (classification),
a similarity-based classification method like k-nearest-neighbors or the, by Staab and Pekar \cite{pekar2002tl},  proposed algorithms can be used for finding the most similar superclass.\\
The first approach (see Figure~\ref{fig:approach1}) consists of two neural networks, which will represent the class
with two vectors.
The first network being the
Continuous Bag-of-Words (CBOW) by Mikolov et al. \cite{mikolov2013} , and the second being graph
representation neural network (GRNN), like the deep neural network by Cao et al. \cite{CaoLX16}.
CBOW will be trained on the Wikipedia articles of all classes, and GRNN will be trained on the Wikidata taxonomy.
To find a superclass for a given class, the closest classes in the word and in the graph vector space
will be identified and used in a classification method like k-nearest neighbors.\\
The second approach (see Figure~\ref{fig:approach2}) will only use CBOW to map the classes to word vectors.
But the taxonomic structure will still be exploited by using for example tree-ascending+kNN \cite{pekar2002tl}
in the classification stage.\\
For both approaches it will be necessary to calculate the representation vectors for all classes in the taxonomy
ahead of time, so the suggested classification methods can be used.\\
\\
\begin{figure}
\centering
\includegraphics[width=1.1\textwidth]{images/architecture_approach1.pdf}
\caption{Solution using two neural networks \cite{mikolov2013} \cite{CaoLX16} and k-nearest neighbors for 
classification.}
\label{fig:approach1}
\includegraphics[width=1.1\textwidth]{images/architecture_approach2.pdf}
\caption{Solution using CBOW \cite{mikolov2013} for word representation and tree-ascending \cite{pekar2002tl} 
for classification.}
\label{fig:approach2}
\end{figure}
If the evaluation shows that ignoring the taxonomic structure only slightly influences the quality
of results, it may be a good decision to just remove the according part of the algorithm, which
could greatly affect its runtime depending on the used network. 
I assume that the choice of similarity measure could greatly influence the evaluation results.

\section{Time plan}

The following outline is proposed for the thesis:
\begin{enumerate}[1.]
\item Foundations: definitions; problem statement; types of neural networks
\item Taxonomy analysis: statistics about classes and root classes in Wikidata
\item Related work: ontology learning; similarity measures; neural network models
\item Comparison of neural networks: compare input, output, task, type and suitability of different models for the 
given task
\item Development of new algorithms: design new baseline-algorithm and variations; justify design decisions based 
on previous sections
\item Evaluation: explain evaluation method; evaluate/compare baseline-algorithm and variations
\end{enumerate}
The thesis will be written in parallel to the design and implementation of the solution. The first phase consists
of research of related work and an analysis of the Wikidata taxonomy. In the second phase a baseline-algorithm
and variations of it are designed and developed using the results of the previous phase. In the next phase
test data is collected and the evaluation of the algorithms executed. The evaluation results will be 
visualized and interpreted. Finally the summary, introduction, and conclusion of the thesis will be written
and a presentation prepared.

See the following Gantt chart for the time plan \ref{fig:timeplan}:
\begin{figure}
\begin{ganttchart}[vgrid={*1{draw=black!20, line width=.75pt}}, x unit=0.05\textwidth]{1}{12}
\gantttitle{Time plan for bachelor thesis (in weeks)}{12}\\
\gantttitlelist{1,...,12}{1}\\
\ganttgroup{Foundations, Sections 1-4}{1}{5}\\
\ganttbar[name=RelWork]{Research related work}{1}{3}\\
\ganttbar[name=NN]{Compare neural networks}{1}{3}\\
\ganttbar[name=TaxAn]{Analyze taxonomy}{1}{1}\\
\ganttgroup{Algorithms, Sections 5}{4}{8}\\
\ganttbar[name=Design]{Design algorithms}{4}{5}\\
\ganttbar[name=Impl]{Implement algorithms}{6}{8}\\
\ganttbar[name=Tensor]{Learn Tensorflow}{5}{5}\\
\ganttgroup{Evaluation, Section 6}{8}{10}\\
\ganttbar[name=Eval]{Evaluate algorithms}{9}{9}\\
\ganttbar[name=Data]{Create test data}{8}{8}\\
\ganttgroup{Summary, Introduction, Conclusion}{10}{11}\\
\ganttbar[name=SIC]{Write missing parts}{10}{11}\\
\ganttbar[name=Pres]{Prepare presentation}{11}{12}

\ganttlink{TaxAn}{Design}
\ganttlink{RelWork}{Design}
\ganttlink{NN}{Design}
\ganttlink{Design}{Impl}
\ganttlink{Tensor}{Impl}
\ganttlink{Impl}{Eval}
\ganttlink{Data}{Eval}
\ganttlink{Eval}{SIC}
\ganttlink{Eval}{Pres}
\end{ganttchart}
\caption{time plan}
\label{fig:timeplan}
\end{figure}

\listoffigures

\nocite{*}

\bibliographystyle{plainnat}
\bibliography{bibliography}


\end{document}

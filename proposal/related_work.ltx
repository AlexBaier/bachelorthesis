% !TEX TS-program = pdflatex
% !TEX encoding = UTF-8 Unicode

\documentclass[11pt]{scrartcl} % use larger type; default would be 10pt

\usepackage[utf8]{inputenc} % set input encoding (not needed with XeLaTeX)

%%% PAGE DIMENSIONS
\usepackage{geometry} % to change the page dimensions
\geometry{a4paper} % or letterpaper (US) or a5paper or....

\usepackage{graphicx} % support the \includegraphics command and options

 \usepackage[parfill]{parskip} % Activate to begin paragraphs with an empty line rather than an indent

%%% PACKAGES
\usepackage{booktabs} % for much better looking tables
\usepackage{array} % for better arrays (eg matrices) in maths
\usepackage{paralist} % very flexible & customisable lists (eg. enumerate/itemize, etc.)
\usepackage{verbatim} % adds environment for commenting out blocks of text & for better verbatim
\usepackage{subfig} % make it possible to include more than one captioned figure/table in a single float
\usepackage{color}
\usepackage{ulem}

\newcounter{ToDoId}
\newcommand{\TODO}[1]{\stepcounter{ToDoId}\textcolor{red}{\textbf{TODO \theToDoId: #1}}}

\title{Summaries of Related work}
\author{Alex Baier \\ abaier@uni-koblenz.de}


\begin{document}
\maketitle

\tableofcontents

\section{Ontology learning}


\subsection{Taxonomy learning â€“ factoring the structure of a taxonomy into a semantic classification decision}
\cite{pekar2002tl} defines a tree-ascending algorithm, which calculates the taxonomic similarity between nearest 
neighbors. Evaluation shows that the algorithm is good at choosing the correct class for a new word. A combination of tree-ascending and k-nearest-neighbors was developed, which slightly improved the results in finding the correct class.  Another algorithm, tree-descending, was shown to be less efficient than standard classifiers. \\
The paper solves a similar problem to the problem of my work. The exploitation of the taxonomy for classification
proves to be effective and should therefore be incorporated into the solution of the thesis' problem.

\subsection{Ontology Enrichment by Discovering Multi-Relational Association Rules from Ontological Knowledge Bases}
The algorithm presented in \cite{dAmatoSTMG16} discovers relational association rules expressed SWRL, which
allows the discovered rules to be directly integrated into the ontology and increase its expressiveness. This is done
by detecting hidden patterns in a populated ontological knowledge base, and then obtaining relational association 
rules from these pattern.\\
Not relevant for the thesis, because the work is concerned with finding new rules in ontological KBs, while the task 
of the thesis is to find new connections in the taxonomy of a KB. And other papers solve problems with a higher
similarity to the thesis' problem.

\subsection{Ontology Learning for the Semantic Web}
The task of ontology learning is analyzed by \cite{Maedche2001}. A semi-automated approach is considered, which
consists of 5 steps: import/reuse existing ontologies, extract major parts of target ontology, prune to adjust 
ontology for primary purpose, refine ontology to complete ontology at fine granularity, and apply ontology
on target applications, which validates the results. These steps may be repeated as often as necessary.\\
The problem of the thesis belongs to the task of ontology refinement, because an ontology is already 
provided. 

\section{Neural networks}

\subsection{Using Recurrent Neural Network for Learning Expressive Ontologies}
\TODO{Some parts of this summary should be moved to the comparison of neural networks.}
\cite{PetrucciGR16} describes a recurrent neural network with short-term memory capabilities through Gated Recursive Units for usage in ontology learning. It is argued that recurrent neural networks can handle this task,
because they proved to do well in handling natural languages, and therefore should also be able to ''handle the typical syntactic structures of encyclopedic text''. \\
The ontology learning process is described as transduction task. The typical structure of encyclopedic text is exploited by translating the text into a logical formula using a pipeline, which creates a formula template and a tagged sentence. These are combined to create a formula. \\
 This pipeline is assembled by a combination of neural networks.
 \begin{itemize}
 \item Sentence tagging: single Recurrent Neural Network with Gated Recursive Units.
 	\begin{itemize}
 	\item Input: sentence in natural language, represented as n+1 symbol sequence. First n symbols are words.
            	The last symbol is $<$EOS$>$, which indicates end of sequence. Each word has a mapping to an integer,
            	which represents its position in the vocabulary. The input is then transformed into a sequence of 				context windows.
	\item Output: Estimation of each tag to be the right one for k-th word. Predicted tag for k-th word is the one 	
		with the highest probability.
	\end{itemize}
\item Sentence transduction: Recurrent Encoder-Decoder with Gated Recursive Units.
	\begin{itemize}
	\item Input: The same as in sentence tagging. But the context windowing is not executed.
	\item Output: Set of all possible formula terms. Input and output sequences don't have the same length.
	\end{itemize}
\item Gated Recursive Unit provides both networks with a short-term memory effect.
 \end{itemize}

\subsection{Recurrent neural network based language model}
\cite{MikolovKBCK10} describes a simple recurrent network language based model and tests its performance for 
the task of speech recognition. It outperforms the typical n-gram model, but has a higher computational performance. \\
Because speech recognition has only very few similarities to the concerned data of my work, 
this paper has no further relevance for problem of this work. But it should be noted, that this paper shows the 
power of recurrent neural networks in predicting contextual data and not having a limited size of context.


\subsection{Efficient Estimation of Word Representations in Vector Space}
\cite{abs-1301-3781} proposes two new neural network architectures, which compute continuous vector representations of words. It is possible to use algebraic operations on such word vectors. An example, which is mentioned in the paper $vector(''King'') - vector(''Man'') + vector(''Woman'')$ results into a vector that is very close
to the representation of the word \textit{Queen}. The paper tries to maximize the accuracy of such operations by developing their architectures to ''preserve the linear regularities among words''. The Continuous Bag-of-Words Model (CBOW) and the Continuous Skip-gram Model were developed. CBOW is similar to the Feedforward Neural Net Language Model. It predicts the current word based on its context. Continuous Skip-Gram in contrast predicts the words before and after the current word.\\
The results of different word vectors versions were compared by applying them to a similarity task. The proposed
models are able to create word vectors with a higher syntactic and semantic accuracies than other existing models.

\subsection{Deep Neural Networks for Learning Graph Representations}
\cite{CaoLX16} describes a deep neural network, which creates deep graph representation model, in which a low 
dimensional vector is created for each vertex in the graph. The method was tested on real datasets and 
outperformed some state-of-the-art systems. It is able to handle noisy data, which may be important for the task of 
this work.  The output of this network, the graph representations, can be used as input for other methods, e.g. 
supervised classification. Therefore this network could be used for transforming the taxonomy of Wikidata into a 
small-dimensional input format.

\subsection{Supervised neural networks for the classification of structures}
\cite{Sperduti1997} describes neural networks using generalized recursive neurons for the classification of structured patterns (e.g. concept graphs). The a-priori encoding of structures for use in the neural network has drawbacks. The authors propose the use a second neural network for encoding the structure, which is trained alongside the classification network. To solve this problem the generalized recursive neuron is introduced. An encoding network is constructed using this neurons. It creates a vector, which encodes the graph, and is finally fed into a feed-forward neural network for classification. \TODO{Read this paper again, I think misunderstood, what the network actually does.}

\subsection{Neural Networks for Classification: A Survey}
\cite{Zhang00neuralnetworks} reviews the topic neural networks for classification.
Key issues and features of neural networks are discussed.
Relevant for the thesis, the topic of feature variable selection is discussed.
It is important to ''find the smallest set of features that can result in satisfactory predictive performance''.
Different approaches are discussed. It is noted that linear limitations of popular methods like
principle component analysis can be overcome by using neural networks to perform networks, because they
are able to perform nonlinear PCA.

\subsection{A Convolutional Neural Network for Modelling Sentences}
\cite{KalchbrennerGB14} defines a convolutional neural network architecture called Dynamic
Convolutional Neural Network. The network models sentences using combinations of pooling and convolution
layers, which internally induce something akin to parse tree on the input sentences. The specific output
of the network is based on its supervised training. For example classification of sentiments was tested
on movie reviews and tweets. The network proved to be effective at these tasks.

\subsection{DRAW: A Recurrent Neural Network For Image Generation}
Deep Recurrent Attentive Writer (DRAW) is introduced by \cite{GregorDGW15}. It is an neural network
architecture, which is able to generate images, by iteratively drawing and refining an image. 
The architecture consists of an encoder network, used for compressing images during learning,
and a decoder network that recreates the image after receiving a code. This type of architecture is
called variational auto-encoders. IN DRAW both of these networks are recurrent, which means a sequence of code samples is exchanged between the networks. The image generation works in multiple steps, in which different
parts of the image are generated. The decision on where to write and what code samples to read
is made by the neural network. After a number a step an image is produced, which cannot be distinguished
from real images. 

\bibliographystyle{plain}
\bibliography{bibliography}

\end{document}
